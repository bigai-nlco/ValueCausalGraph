{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNk7IylTv610"
   },
   "source": [
    "# Loading and Analysing Pre-Trained Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_DusoOvwV0M"
   },
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aGgWkbav610"
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yfDUxRx0wSRl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
    "from causallearn.utils.cit import kci\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
    "from causallearn.utils.cit import fisherz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_valuebench_features_csv_gemma_train = os.path.join('useful_data',\"ans_gemma_train_formal.csv\")\n",
    "answer_valuebench_features_csv_gemma_test = os.path.join('useful_data',\"ans_gemma_test_formal.csv\")\n",
    "answer_valuebench_features_csv_llama_train = os.path.join('useful_data',\"ans_llama_train_formal.csv\")\n",
    "answer_valuebench_features_csv_llama_test = os.path.join('useful_data',\"ans_llama_test_formal.csv\")\n",
    "\n",
    "# answer_valuebench_features_csv_gemma_train = os.path.join('useful_data',\"ans_cross_part1.csv\")\n",
    "# answer_valuebench_features_csv_gemma_test = os.path.join('useful_data',\"ans_cross_part2.csv\")\n",
    "# answer_valuebench_features_csv_llama_train = os.path.join('useful_data',\"ans_llama_old_train.csv\")\n",
    "# answer_valuebench_features_csv_llama_test = os.path.join('useful_data',\"ans_llama_old_test.csv\")\n",
    "\n",
    "data_csv_gemma_train = pd.read_csv(answer_valuebench_features_csv_gemma_train)\n",
    "data_csv_gemma_test = pd.read_csv(answer_valuebench_features_csv_gemma_test)\n",
    "data_csv_llama_train = pd.read_csv(answer_valuebench_features_csv_llama_train)\n",
    "data_csv_llama_test = pd.read_csv(answer_valuebench_features_csv_llama_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_new_diff(data_csv_raw, modelname, threshold_judge):\n",
    "    assert threshold_judge >= 0\n",
    "    pathname = 'value_dims_rsd_' + modelname\n",
    "    stat_csv_23 = pathname + '/23_stat.csv'\n",
    "    data_new_diff_count_total = pd.DataFrame()\n",
    "\n",
    "    if os.path.exists('pathname'):\n",
    "        shutil.rmtree('pathname')\n",
    "    os.makedirs(pathname, exist_ok=True)\n",
    "    \n",
    "    for column in data_csv_raw.columns:\n",
    "        if column == 'player_name' or column == 'steer_dim' or column == 'stds' or column =='scstds' or column.endswith(':scstd'):\n",
    "            continue\n",
    "        value_csv = pathname + '/' + column + '.csv'\n",
    "        data_new = data_csv_raw.pivot(index='steer_dim', columns='player_name', values=column)\n",
    "        data_new_scstd = data_csv_raw.pivot(index='steer_dim', columns='player_name', values=column+':scstd')\n",
    "        data_save = data_new.astype(str) + '±' + data_new_scstd.astype(str) #problems here: the scstd is not the std for the score, but fore the changed score\n",
    "        data_save.to_csv(value_csv)\n",
    "\n",
    "        data_new_diff = data_new - data_new[data_new.index.isnull()].iloc[0]\n",
    "\n",
    "        data_new_diff_count_higher = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if y > threshold_judge else 0))\n",
    "        data_new_diff_count_higher = data_new_diff_count_higher.sum(axis=1)\n",
    "        data_new_diff_count_lower = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if y < -threshold_judge else 0))\n",
    "        data_new_diff_count_lower = data_new_diff_count_lower.sum(axis=1)\n",
    "        data_new_diff_count_equal = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if abs(y) <= threshold_judge else 0))\n",
    "        data_new_diff_count_equal = data_new_diff_count_equal.sum(axis=1)\n",
    "\n",
    "        data_new_diff_count = data_new_diff_count_higher.astype(str) + '/' + data_new_diff_count_lower.astype(str) + '/' + data_new_diff_count_equal.astype(str)\n",
    "        data_new_diff_count_total[column] = data_new_diff_count\n",
    "\n",
    "    data_new_diff_count_total.to_csv(stat_csv_23)\n",
    "\n",
    "get_data_new_diff(data_csv_gemma_train, 'gemma', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_gemma_test, 'gemmatest', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_llama_train, 'llama', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_llama_test, 'llamatest', threshold_judge=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table1_newest(stat_csv_23_train, stat_csv_23_test):\n",
    "    data_new_diff_count_total_train = pd.read_csv(stat_csv_23_train)\n",
    "    data_new_diff_count_total_test = pd.read_csv(stat_csv_23_test)\n",
    "\n",
    "    table1_rows = data_new_diff_count_total_train['steer_dim'].unique()\n",
    "    table1_rows = table1_rows[~np.isnan(table1_rows)]\n",
    "    table1_columns = data_new_diff_count_total_train.columns[data_new_diff_count_total_train.columns != 'steer_dim']\n",
    "\n",
    "    table1_rows_test = data_new_diff_count_total_test['steer_dim'].unique()\n",
    "    table1_rows_test = table1_rows_test[~np.isnan(table1_rows_test)]\n",
    "    table1_columns_test = data_new_diff_count_total_test.columns[data_new_diff_count_total_test.columns != 'steer_dim']\n",
    "\n",
    "    assert np.array_equal(table1_rows, table1_rows_test)\n",
    "    assert np.array_equal(table1_columns, table1_columns_test)\n",
    "    \n",
    "    table1 = pd.DataFrame(columns=table1_columns, index=table1_rows)\n",
    "    \n",
    "    for steer_dim in table1_rows:\n",
    "        assert not np.isnan(steer_dim)\n",
    "\n",
    "        steer_dim_row_train = data_new_diff_count_total_train[data_new_diff_count_total_train['steer_dim'] == steer_dim]\n",
    "        steer_dim_row_test = data_new_diff_count_total_test[data_new_diff_count_total_test['steer_dim'] == steer_dim]\n",
    "\n",
    "        for column in table1_columns:\n",
    "            assert column != 'steer_dim'\n",
    "            #split cell by /\n",
    "            counts_train = steer_dim_row_train[column].values[0].split('/')   \n",
    "            simu_train = int(counts_train[0])\n",
    "            supp_train = int(counts_train[1])\n",
    "            main_train = int(counts_train[2])\n",
    "            \n",
    "            counts_test = steer_dim_row_test[column].values[0].split('/')\n",
    "            simu_test = int(counts_test[0])\n",
    "            supp_test = int(counts_test[1])\n",
    "            main_test = int(counts_test[2])\n",
    "\n",
    "            table1.loc[steer_dim, column] = str(simu_train) + '/' + str(supp_train) + '/' + str(main_train) + '/' + str(simu_test) + '/' + str(supp_test) + '/' + str(main_test)\n",
    "    return table1\n",
    "\n",
    "table1_gemma = get_table1_newest('value_dims_rsd_gemma/23_stat.csv', 'value_dims_rsd_gemmatest/23_stat.csv')\n",
    "table1_gemma.to_csv('table1_gemma_newest.csv')\n",
    "table1_llama = get_table1_newest('value_dims_rsd_llama/23_stat.csv', 'value_dims_rsd_llamatest/23_stat.csv')\n",
    "table1_llama.to_csv('table1_llama_newest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_table1_rotate_tutu(table1, table1_name, modelname, exclude_rows=None, exclude_columns=None):\n",
    "    if exclude_rows is None:\n",
    "        exclude_rows = []\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "\n",
    "    #value_dims = [str(vd) for vd in table1.index if vd not in exclude_rows]\n",
    "    value_dims = [str(vd) for vd in table1.columns if vd not in exclude_columns]\n",
    "    #steering_features = [int(sf) for sf in table1.columns if sf not in exclude_columns]\n",
    "    steering_features = [int(sf) for sf in table1.index if sf not in exclude_rows]\n",
    "\n",
    "    table_filtered = table1.loc[steering_features, value_dims]\n",
    "\n",
    "    # 初始化 LaTeX 代码\n",
    "    latex_code = '''\n",
    "\\\\newcommand{\\\\cellbar}[5]{%\n",
    "    \\\\raisebox{\\\\height}{%\n",
    "        \\\\begin{tikzpicture}[baseline=(current bounding box.center)]\n",
    "            \\\\draw[draw=black] (0,0) rectangle (1cm,0.4cm);\n",
    "            \\\\path[fill=CustomBlue, opacity=#5] (0,0) rectangle (#1cm,0.4cm);\n",
    "            \\\\path[fill=white] (#1cm,0) rectangle ({#1cm + #3cm},0.4cm);\n",
    "            \\\\path[fill=CustomYellow, opacity=#5] ({#1cm + #3cm},0) rectangle (1cm,0.4cm);\n",
    "            \\\\node[anchor=center, font=\\\\scriptsize] at (0.5cm,0.2cm) {#2};\n",
    "        \\\\end{tikzpicture}%\n",
    "    }%\n",
    "}\n",
    "\\\\begin{table*}[ht]\n",
    "\\\\centering\n",
    "\\\\caption{Value steering using SAE features for the {''' + modelname + '''} model. For each pair of steering feature and value dimension, the cell shows the stimulation ratio in blue, the suppression ratio in yellow, and the maintenance ratio in blank, which are estimated from the training data. The numbers in the cell represent the distance between the actual ratios in the testing data and expected ratios.}\\n\n",
    "\\\\label{table: sae-steering-''' + modelname + '''}\n",
    "\\\\resizebox{\\\\textwidth}{!}{%\n",
    "\\\\begin{tabular}{>{\\\\centering\\\\arraybackslash}m{1.5cm} *{''' + str(len(value_dims)) + '''}{>{\\\\centering\\\\arraybackslash}m{1cm}}>{\\\\centering\\\\arraybackslash}m{1cm}}\n",
    "\\\\toprule\n",
    "'''\n",
    "\n",
    "    # 生成表头\n",
    "    header_row = ['\\\\rotatebox{90}{\\\\textbf{Value}}'] + ['\\\\rotatebox{90}{\\\\textbf{' + vd + '}}' for vd in value_dims] + ['\\\\textbf{AVG}']\n",
    "    latex_code += ' & '.join(header_row) + ' \\\\\\\\\\n\\\\midrule\\n'\n",
    "\n",
    "    # 定义单元格内容的生成函数\n",
    "    def generate_cell_content(red_ratio, green_ratio, transparency_ratio, similarity):\n",
    "        \"\"\"\n",
    "        生成单元格的 LaTeX 代码，使用 \\\\cellbar{red_length}{similarity}{transparency_length}{green_length}{opacity}\n",
    "        \"\"\"\n",
    "        # 确保比例在 0 到 1 之间，并归一化\n",
    "        total = red_ratio + green_ratio + transparency_ratio\n",
    "        if total > 0:\n",
    "            red_ratio /= total\n",
    "            green_ratio /= total\n",
    "            transparency_ratio /= total\n",
    "        else:\n",
    "            red_ratio = green_ratio = transparency_ratio = 0.0\n",
    "\n",
    "        red_length = red_ratio\n",
    "        transparency_length = transparency_ratio\n",
    "        green_length = green_ratio\n",
    "\n",
    "        opacity = 1.0 - transparency_ratio  # 透明度与透明部分成反比\n",
    "\n",
    "        return f'\\\\cellbar{{{red_length:.2f}}}{{{similarity:.2f}}}{{{transparency_length:.2f}}}{{{green_length:.2f}}}{{{opacity:.2f}}}'\n",
    "\n",
    "    # 生成表格内容\n",
    "    for sf in steering_features:\n",
    "        cosines = []\n",
    "        row = ['\\\\textbf{' + str(sf) + '}']\n",
    "        for vd in value_dims:\n",
    "            value = table_filtered.loc[sf, vd]\n",
    "            if isinstance(value, str):\n",
    "                # 处理字符串值，计算相似度\n",
    "                value_list = list(map(int, value.split('/')))\n",
    "                traindata = value_list[:3]\n",
    "                testdata = value_list[3:]\n",
    "                # 计算相似度\n",
    "                traindata_p = np.array(traindata) / np.sum(traindata)\n",
    "                testdata_p = np.array(testdata) / np.sum(testdata)\n",
    "                similarity = np.dot(traindata_p, testdata_p) / (np.linalg.norm(traindata_p) * np.linalg.norm(testdata_p))\n",
    "                cosines.append(similarity)\n",
    "\n",
    "                # 提取红色、绿色和透明度的值\n",
    "                red_value = traindata[0]\n",
    "                green_value = traindata[1]\n",
    "                transparency_value = traindata[2]\n",
    "                # 计算比例\n",
    "                red_ratio = red_value\n",
    "                green_ratio = green_value\n",
    "                transparency_ratio = transparency_value\n",
    "\n",
    "                # 生成单元格内容\n",
    "                cell_content = generate_cell_content(red_ratio, green_ratio, transparency_ratio, similarity)\n",
    "                row.append(cell_content)\n",
    "            else:\n",
    "                row.append('')\n",
    "\n",
    "        avg_similarity = np.mean(cosines) if cosines else 0\n",
    "        row.append(f'{avg_similarity:.2f}')\n",
    "        latex_code += ' & '.join(row) + ' \\\\\\\\\\n'\n",
    "    latex_code += '\\\\bottomrule\\n\\\\end{tabular}\\n}\\n\\\\end{table*}\\n'\n",
    "\n",
    "    # 保存 LaTeX 代码\n",
    "    with open(f'{table1_name}.tex', 'w') as f:\n",
    "        f.write(latex_code)\n",
    "  \n",
    "exclude_columns_gemma = ['Social','Breath of Interest','Economic','Religion','Social Cynicism','Theoretical','Understanding']\n",
    "exclude_rows_gemma = [11712, 1025, 1341, 2221, 2965, 3402, 6188, 6884, 7502, 10454, 12703, 14185, 14351]\n",
    "get_latex_table1_rotate_tutu(table1_gemma, 'table1_gemma_rotate', 'Gemma-2B-IT', exclude_rows_gemma, exclude_columns_gemma)\n",
    "get_latex_table1_rotate_tutu(table1_llama, 'table1_llama_rotate', 'Llama3-8B-IT', [], [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAUSAL_METHOD = 'pc'\n",
    "NOISE_AUGUMENT_SINGLE_SAE = None #10\n",
    "NOISE_VAR = 0.00001\n",
    "PC_ALPHA = 0.05#0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_d_columns_deprecated(answer_valuebench_features_csv):\n",
    "    data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
    "    digits = [str(d) for d in range(10)]\n",
    "    d_columns = [d for d in data_csv.columns if d[0] in digits]\n",
    "    d_data = data_csv[d_columns]\n",
    "    stds = d_data.std()\n",
    "    avgs = d_data.mean()\n",
    "    std_avg = stds/avgs\n",
    "    #d_columns_valid = [d for d in d_columns if avgs[d] > 1]\n",
    "    d_columns_valid = d_columns\n",
    "    return d_columns_valid\n",
    "\n",
    "def deal_with_csv(data_csv, graph_path, v_inference, v_showongraph='ALL', row_num='ALL', method=CAUSAL_METHOD, dummy_steered_dim=False): \n",
    "    # data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
    "    # v_columns_all = [v for v in data_csv.columns if (v not in ['player_name', 'steer_dim', 'stds']) and (not v.endswith(':scstd'))]\n",
    "    # if v_inference == 'ALL':\n",
    "    #     v_columns_inference = v_columns_all\n",
    "    # else:\n",
    "    #     for v in v_inference:\n",
    "    #         if v not in v_columns_all:\n",
    "    #             raise ValueError('Invalid v_inference')\n",
    "    #     v_columns_inference = v_inference\n",
    "\n",
    "    v_columns_inference = v_inference\n",
    "\n",
    "    if v_showongraph == 'ALL':\n",
    "        v_columns_showgraph = v_columns_inference\n",
    "    else:\n",
    "        for v in v_showongraph:\n",
    "            if v not in v_columns_inference:\n",
    "                raise ValueError('Invalid v_showongraph')\n",
    "        v_columns_showgraph = v_showongraph\n",
    "\n",
    "    if dummy_steered_dim:\n",
    "        steer_dim_dummies = pd.get_dummies(data_csv['steer_dim'], prefix='steer_dim') * 1\n",
    "        data = pd.concat([data_csv, steer_dim_dummies], axis=1)\n",
    "        v_columns_inference_total = v_columns_inference + list(steer_dim_dummies.columns) \n",
    "        v_columns_showgraph_total = v_columns_showgraph + list(steer_dim_dummies.columns)\n",
    "    else:\n",
    "        data = data_csv\n",
    "        v_columns_inference_total = v_columns_inference\n",
    "        v_columns_showgraph_total = v_columns_showgraph\n",
    "    \n",
    "    data = data[v_columns_inference_total].to_numpy()    \n",
    "    \n",
    "    if type(row_num) == int:\n",
    "        rows = np.random.choice(data.shape[0], row_num, replace=False)\n",
    "        data = data[rows]\n",
    "    else:\n",
    "        assert row_num == 'ALL'\n",
    "\n",
    "    if dummy_steered_dim:\n",
    "        edges_total = causal_inference(data, v_columns_inference_total, graph_path + \"/total.png\", method, noise_augument=None, prior_source_set=list(steer_dim_dummies.columns))\n",
    "    else:\n",
    "        edges_total = causal_inference(data, v_columns_inference_total, graph_path + \"/total.png\", method, noise_augument=None)\n",
    "    \n",
    "    edges_sfs = []\n",
    "    steer_dims = data_csv['steer_dim'].unique()\n",
    "    for steer_dim in steer_dims:\n",
    "        print(steer_dim)\n",
    "        if np.isnan(steer_dim):\n",
    "            data = data_csv[data_csv['steer_dim'].isnull()][v_columns_inference].to_numpy()\n",
    "            edges_nosteer = causal_inference(data, v_columns_inference, graph_path + f'/{steer_dim}.png', method, noise_augument=NOISE_AUGUMENT_SINGLE_SAE)\n",
    "        else:\n",
    "            data = data_csv[data_csv['steer_dim'] == steer_dim][v_columns_inference].to_numpy()\n",
    "            sfedge = causal_inference(data, v_columns_inference, graph_path + f'/{steer_dim}.png', method, noise_augument=NOISE_AUGUMENT_SINGLE_SAE)\n",
    "            edges_sfs.append(sfedge)\n",
    "\n",
    "    return edges_total, edges_nosteer, edges_sfs\n",
    "\n",
    "def causal_inference(data, ci_dimensions, pdy_name, method, noise_augument=None, prior_source_set=None):\n",
    "    if noise_augument:\n",
    "        data = np.tile(data, (noise_augument, 1))\n",
    "        noise = np.random.normal(0, NOISE_VAR, data.shape)\n",
    "        data = data + noise\n",
    "\n",
    "    if method == 'pc':\n",
    "        g = pc(data, PC_ALPHA, node_names=ci_dimensions)\n",
    "        \n",
    "        if prior_source_set:\n",
    "            bk = BackgroundKnowledge()\n",
    "            nodes = g.G.get_nodes()\n",
    "            for node1 in nodes:\n",
    "                for node2 in nodes:\n",
    "                    if node1.name in prior_source_set and node2.name in prior_source_set and node1.name != node2.name:\n",
    "                        bk = bk.add_forbidden_by_node(node1, node2)\n",
    "            g = pc(data, PC_ALPHA, node_names=ci_dimensions, background_knowledge=bk)\n",
    "            \n",
    "        graph = g.G\n",
    "\n",
    "        edges = []\n",
    "        for n1 in range(len(graph.nodes)):\n",
    "            assert graph.nodes[n1].name == ci_dimensions[n1]\n",
    "            for n2 in range(n1+1, len(graph.nodes)):\n",
    "                # if n1 == n2:\n",
    "                #     continue\n",
    "                if graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == 1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'single-arrow'])\n",
    "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == -1:\n",
    "                    edges.append([graph.nodes[n2].name, graph.nodes[n1].name, 1, 'single-arrow']) \n",
    "                elif graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == -1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'no-arrow'])\n",
    "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == 1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'double-arrow'])\n",
    "                else:\n",
    "                    if not (graph.graph[n1][n2] == 0 and graph.graph[n2][n1] == 0):\n",
    "                        raise ValueError('Invalid edge')\n",
    "    else:\n",
    "        raise ValueError('Invalid method')\n",
    "    \n",
    "    columns_concerned_vis = [label.replace(':','-') for label in ci_dimensions]\n",
    "    pdy = GraphUtils.to_pydot(graph, labels=columns_concerned_vis)\n",
    "    pdy.write_png(pdy_name)\n",
    "    return edges\n",
    "\n",
    "def get_graph(data_csv, graph_path):\n",
    "    v_inference = [v for v in data_csv.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "    if os.path.exists(graph_path):\n",
    "        shutil.rmtree(graph_path)\n",
    "    os.makedirs(graph_path, exist_ok=True)\n",
    "    edges_total, edges_nosteer, edges_sfs = deal_with_csv(data_csv, graph_path, v_inference, 'ALL', 'ALL', CAUSAL_METHOD, False)\n",
    "    return edges_total, edges_nosteer, edges_sfs\n",
    "\n",
    "edges_gemma_total, edges_gemma_nosteer, edges_gemma_sfs = get_graph(data_csv_gemma_train, 'value_causal_graph_gemma')\n",
    "edges_llama_total, edges_llama_nosteer, edges_llama_sfs = get_graph(data_csv_llama_train, 'value_causal_graph_llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges_standard_json = json.load(open('value_graph_smallset_triplets.json'))\n",
    "# edges_standard = []\n",
    "# for edge in edges_standard_json:\n",
    "#     if edge[1] == '-->':\n",
    "#         edges_standard.append([edge[0], edge[2], 1, 'single-arrow'])\n",
    "#     elif edge[1] == 'o--o':\n",
    "#         edges_standard.append([edge[0], edge[2], 1, 'double-arrow'])\n",
    "#     else:\n",
    "#         raise ValueError('Invalid edge')\n",
    "\n",
    "edges_standard = [\n",
    "    ['Positive coping', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Social Complexity', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Social', 1, 'double-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Aesthetic', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Breadth of Interest', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Organization', 'Economic', 1, 'single-arrow'],\n",
    "    ['Political', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Religious', 'Understanding', 1, 'single-arrow'],\n",
    "    ['Theoretical', 'Understanding', 1, 'single-arrow'],\n",
    "    ['Understanding', 'Empathy', 1, 'double-arrow'],\n",
    "    ['Empathy', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Social Complexity', 1, 'single-arrow'],\n",
    "    ['Positive coping', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Economic', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Political', 'Economic', 1, 'double-arrow'],\n",
    "    ['Social Cynicism', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Understanding', 'Positive coping', 1, 'double-arrow'],\n",
    "    ['Organization', 'Social', 1, 'single-arrow'],\n",
    "]\n",
    "\n",
    "edges_standard_small = [\n",
    "    ['Positive coping', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Social', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Anxiety Disorder', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Economic', 'Organization', 1, 'single-arrow'],\n",
    "    ['Political', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Religious', 'Understanding', 1, 'single-arrow'],\n",
    "]\n",
    "\n",
    "edges_random = [\n",
    "    ['Positive coping', 'Economic', 1, 'double-arrow'],\n",
    "    ['Empathy', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Social Cynicism', 1, 'double-arrow'],\n",
    "    ['Social Complexity', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Achievement', 'Uncertainty Avoidance', 1, 'double-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Aesthetic', 'Religious', 1, 'double-arrow'],\n",
    "    ['Anxiety Disorder', 'Understanding', 1, 'single-arrow'],\n",
    "    ['Breadth of Interest', 'Theoretical', 1, 'double-arrow'],\n",
    "    ['Economic', 'Political', 1, 'single-arrow'],\n",
    "    ['Organization', 'Social', 1, 'double-arrow'],\n",
    "    ['Political', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Religious', 'Empathy', 1, 'double-arrow'],\n",
    "    ['Theoretical', 'Positive coping', 1, 'single-arrow'],\n",
    "    ['Understanding', 'Social Complexity', 1, 'double-arrow'],\n",
    "    ['Social', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Social Cynicism', 'Breadth of Interest', 1, 'double-arrow'],\n",
    "    ['Positive coping', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Theoretical', 1, 'double-arrow'],\n",
    "    ['Resilience', 'Organization', 1, 'single-arrow'],\n",
    "]\n",
    "\n",
    "\n",
    "# edges_standard_old = [\n",
    "#     ['Emotional Processing', 'Emotional Expression', 1, 'single-arrow'],\n",
    "#     ['Emotional Processing', 'Psychosocial Flourishing', 1, 'single-arrow'],\n",
    "#     ['Perspective Taking', 'Sympathy', 1, 'single-arrow'],\n",
    "#     ['Perspective Taking', 'Empathy', 1, 'double-arrow'],\n",
    "#     ['Perspective Taking', 'Nurturance', 1, 'double-arrow'],\n",
    "#     ['Sociability', 'Extraversion', 1, 'double-arrow'],\n",
    "#     ['Sociability', 'Warmth', 1, 'double-arrow'],\n",
    "#     ['Sociability', 'Positive Expressivity', 1, 'double-arrow'],\n",
    "#     ['Dependence', 'Nurturance', 1, 'single-arrow'],\n",
    "#     ['Psychosocial Flourishing', 'Satisfaction with life', 1, 'single-arrow'],\n",
    "#     ['Psychosocial Flourishing', 'Nurturance', 1, 'single-arrow'],\n",
    "#     ['Extraversion', 'Positive Expressivity', 1, 'single-arrow'],\n",
    "#     ['Extraversion', 'Social Confidence', 1, 'single-arrow'],\n",
    "#     ['Extraversion', 'Social', 1, 'double-arrow'],\n",
    "#     ['Affiliation', 'Empathy', 1, 'double-arrow'],\n",
    "#     ['Affiliation', 'Social', 1, 'double-arrow'],\n",
    "#     ['Understanding', 'Empathy', 1, 'double-arrow'],\n",
    "#     ['Understanding', 'Reflection', 1, 'double-arrow'],\n",
    "#     ['Understanding', 'Depth', 1, 'single-arrow'],\n",
    "#     ['Understanding', 'Theoretical', 1, 'double-arrow'],\n",
    "#     ['Sympathy', 'Nurturance', 1, 'single-arrow'],\n",
    "#     ['Warmth', 'Empathy', 1, 'single-arrow'], \n",
    "#     ['Warmth', 'Nurturance', 1, 'double-arrow'],\n",
    "#     ['Warmth', 'Positive Expressivity', 1, 'single-arrow'],\n",
    "#     ['Warmth', 'Social', 1, 'single-arrow'], \n",
    "#     ['Empathy', 'Tenderness', 1, 'double-arrow'],\n",
    "#     ['Empathy', 'Nurturance', 1, 'double-arrow'], \n",
    "#     ['Positive Expressivity', 'Social', 1, 'double-arrow'],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zero_double_arrow(edges):\n",
    "    double_arrow_edges = [edge for edge in edges if edge[3] == 'double-arrow']\n",
    "    zero_arrow_edges = [edge for edge in edges if edge[3] == 'no-arrow']\n",
    "    if double_arrow_edges:\n",
    "        raise ValueError('Double arrow:', double_arrow_edges)\n",
    "    if zero_arrow_edges:\n",
    "        raise ValueError('Zero arrow:', zero_arrow_edges)\n",
    "\n",
    "def dealwith_zero_double_duplicated_arrow(edges):\n",
    "    double_arrow_edges = [edge for edge in edges if edge[3] == 'double-arrow']\n",
    "    zero_arrow_edges = [edge for edge in edges if edge[3] == 'no-arrow']\n",
    "    print('Double arrow:', double_arrow_edges)\n",
    "    print('Zero arrow:', zero_arrow_edges)\n",
    "    print('Dealwith zero and double arrow edges')\n",
    "    print('----------------------')\n",
    "    \n",
    "    new_edges = []\n",
    "    for edge in edges:\n",
    "        if edge[3] == 'double-arrow' or edge[3] == 'no-arrow':\n",
    "            if [edge[0], edge[1], edge[2], 'single-arrow'] not in new_edges:\n",
    "                new_edges.append([edge[0], edge[1], edge[2], 'single-arrow'])\n",
    "            if [edge[1], edge[0], edge[2], 'single-arrow'] not in new_edges:\n",
    "                new_edges.append([edge[1], edge[0], edge[2], 'single-arrow'])\n",
    "        else:\n",
    "            if edge not in new_edges:\n",
    "                new_edges.append(edge)\n",
    "    return new_edges\n",
    "\n",
    "def check_dag(edges):\n",
    "    nxg = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        if edge[3] == 'single-arrow':\n",
    "            nxg.add_edge(edge[0], edge[1])\n",
    "    if not nx.is_directed_acyclic_graph(nxg):\n",
    "        cycles = list(nx.simple_cycles(nxg))\n",
    "        raise ValueError('Cycle:', cycles)\n",
    "    \n",
    "edges_gemma_total = dealwith_zero_double_duplicated_arrow(edges_gemma_total)\n",
    "edges_gemma_nosteer = dealwith_zero_double_duplicated_arrow(edges_gemma_nosteer)\n",
    "edges_gemma_sfs = [dealwith_zero_double_duplicated_arrow(edges_sf) for edges_sf in edges_gemma_sfs]\n",
    "edges_llama_total = dealwith_zero_double_duplicated_arrow(edges_llama_total)\n",
    "edges_llama_nosteer = dealwith_zero_double_duplicated_arrow(edges_llama_nosteer)\n",
    "edges_llama_sfs = [dealwith_zero_double_duplicated_arrow(edges_sf) for edges_sf in edges_llama_sfs]\n",
    "edges_standard = dealwith_zero_double_duplicated_arrow(edges_standard)\n",
    "edges_standard_small = dealwith_zero_double_duplicated_arrow(edges_standard_small)\n",
    "edges_random = dealwith_zero_double_duplicated_arrow(edges_random)\n",
    "\n",
    "\n",
    "edges_gemma_nosteer.append(['Positive coping', 'Understanding', 1, 'single-arrow'])\n",
    "edges_gemma_nosteer.append(['Resilience', 'Understanding', 1, 'single-arrow'])\n",
    "edges_llama_nosteer.append(['Social Complexity', 'Theoretical', 1, 'single-arrow'])\n",
    "edges_llama_nosteer.append(['Anxiety Disorder', 'Theoretical', 1, 'single-arrow'])\n",
    "edges_llama_nosteer.append(['Uncertainty Avoidance', 'Theoretical', 1, 'single-arrow'])\n",
    "edges_llama_nosteer.append(['Aesthetic', 'Theoretical', 1, 'single-arrow'])\n",
    "edges_llama_nosteer.append(['Breadth of Interest', 'Theoretical', 1, 'single-arrow'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subsequent_nodes(edges, node):\n",
    "    check_zero_double_arrow(edges)\n",
    "\n",
    "    subsequent_nodes = set()\n",
    "    subsequent_nodes.add(node)\n",
    "    while True:\n",
    "        subsequent_nodes_len = len(subsequent_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[0] in subsequent_nodes:\n",
    "                subsequent_nodes.add(edge[1])\n",
    "        if len(subsequent_nodes) == subsequent_nodes_len:\n",
    "            break\n",
    "    subsequent_nodes.remove(node)\n",
    "    return subsequent_nodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_preceding_nodes(edges, node):\n",
    "    check_zero_double_arrow(edges)\n",
    "\n",
    "    preceding_nodes = set()\n",
    "    preceding_nodes.add(node)\n",
    "    while True:\n",
    "        preceding_nodes_len = len(preceding_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[1] in preceding_nodes:\n",
    "                preceding_nodes.add(edge[0])\n",
    "        if len(preceding_nodes) == preceding_nodes_len:\n",
    "            break\n",
    "    preceding_nodes.remove(node)\n",
    "    return preceding_nodes\n",
    "\n",
    "def get_all_un_related_nodes(edges, node, all_nodes):\n",
    "    check_zero_double_arrow(edges)\n",
    "    #return set(all_nodes) - get_all_subsequent_nodes(edges, node) - set([node]) - get_all_preceding_nodes(edges, node) \n",
    "    #find all nodes that are in a same connected component with node\n",
    "    unrelated_nodes = set(all_nodes)\n",
    "    connected_nodes = set()\n",
    "    connected_nodes.add(node) \n",
    "    while True:\n",
    "        connected_nodes_len = len(connected_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[0] in connected_nodes:\n",
    "                connected_nodes.add(edge[1])\n",
    "            if edge[1] in connected_nodes:\n",
    "                connected_nodes.add(edge[0])\n",
    "        if len(connected_nodes) == connected_nodes_len:\n",
    "            break\n",
    "    unrelated_nodes = unrelated_nodes - connected_nodes\n",
    "    return unrelated_nodes, connected_nodes - set([node])\n",
    "\n",
    "def get_all_non_subsequent_nodes(edges, node, all_nodes):\n",
    "    check_zero_double_arrow(edges)\n",
    "    return set(all_nodes) - get_all_subsequent_nodes(edges, node) - set([node])\n",
    "\n",
    "\n",
    "\n",
    "print(get_all_subsequent_nodes(edges_standard_small, 'Understanding'))\n",
    "print(get_all_preceding_nodes(edges_standard_small, 'Understanding'))\n",
    "v_inference = [v for v in data_csv_gemma_train.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "print(get_all_un_related_nodes(edges_standard_small, 'Understanding', v_inference))\n",
    "\n",
    "print(get_all_subsequent_nodes(edges_gemma_nosteer, 'Achievement'))\n",
    "print(get_all_preceding_nodes(edges_gemma_nosteer, 'Achievement'))\n",
    "v_inference = [v for v in data_csv_gemma_train.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "print(get_all_un_related_nodes(edges_gemma_nosteer, 'Achievement', v_inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def write_table2(edges, data_scorechange, mean_scorechange_related, num_related, mean_scorechange_unrelated, num_unrelated):\n",
    "def write_table2(data_scorechange, edges, edges_source, pd_result_table2, expected = 'subsequent', unexpected = 'unrelated'):\n",
    "    for column in data_scorechange.columns:\n",
    "        print('=============')\n",
    "        print(column)\n",
    "        \n",
    "        related_columns_real = data_scorechange[data_scorechange[column] != 0].abs().mean().sort_values()\n",
    "        #related_columns_real = data_scorechange[data_scorechange[column].abs() > 0.1].abs().mean().sort_values()\n",
    "        #related_columns_real = data_scorechange.abs().mean().sort_values()\n",
    "        \n",
    "        subsequent_columns_ideal = get_all_subsequent_nodes(edges, column)\n",
    "        unrelated_columns_ideal, related_columns_ideal = get_all_un_related_nodes(edges, column, set(data_scorechange.columns))\n",
    "        non_subsequent_columns_ideal = get_all_non_subsequent_nodes(edges, column, set(data_scorechange.columns))\n",
    "\n",
    "        if expected == 'subsequent':\n",
    "            expected_columns_ideal = subsequent_columns_ideal\n",
    "        elif expected == 'related':\n",
    "            expected_columns_ideal = related_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid expected')\n",
    "        \n",
    "        if unexpected == 'unrelated':\n",
    "            unexpected_columns_ideal = unrelated_columns_ideal\n",
    "        elif unexpected == 'non_subsequent':\n",
    "            unexpected_columns_ideal = non_subsequent_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid unexpected')\n",
    "\n",
    "        related_scabs = []\n",
    "        unrelated_scabs = []\n",
    "        for related_column in related_columns_real.index:\n",
    "            if related_column in expected_columns_ideal:\n",
    "                related_scabs.append(related_columns_real[related_column])\n",
    "            elif related_column in unexpected_columns_ideal:\n",
    "                unrelated_scabs.append(related_columns_real[related_column])\n",
    "            else:\n",
    "                pass\n",
    "                #assert related_column == column\n",
    "            print(related_column, related_columns_real[related_column], related_column in expected_columns_ideal)\n",
    "        print('~~~')\n",
    "        \n",
    "        print('Related:', np.mean([vdsc for vdsc in related_scabs if not np.isnan(vdsc)]), len(related_scabs))\n",
    "        print('Unrelated:', np.mean([vdsc for vdsc in unrelated_scabs if not np.isnan(vdsc)]), len(unrelated_scabs))\n",
    "        mean_scorechange_related = 'mean_scorechange_related_' + edges_source\n",
    "        num_related = 'num_related_' + edges_source\n",
    "        mean_scorechange_unrelated = 'mean_scorechange_unrelated_' + edges_source\n",
    "        num_unrelated = 'num_unrelated_' + edges_source\n",
    "\n",
    "        pd_result_table2.loc[mean_scorechange_related, column] = np.mean([vdsc for vdsc in related_scabs if not np.isnan(vdsc)])\n",
    "        pd_result_table2.loc[num_related, column] = len(related_scabs)\n",
    "        pd_result_table2.loc[mean_scorechange_unrelated, column] = np.mean([vdsc for vdsc in unrelated_scabs if not np.isnan(vdsc)])\n",
    "        pd_result_table2.loc[num_unrelated, column] = len(unrelated_scabs)\n",
    "        print('----------------------')\n",
    "\n",
    "#def write_table2_consequence_count(edges, data_scorechange, mean_scorechange_related, num_related, mean_scorechange_unrelated, num_unrelated):\n",
    "def write_table2_consequence_count(data_scorechange, edges, edges_source, pd_result_table2, expected = 'subsequent', unexpected = 'unrelated'):\n",
    "    for column in data_scorechange.columns:\n",
    "        print('=============')\n",
    "        print(column)\n",
    "        \n",
    "        related_columns_real = data_scorechange[data_scorechange[column] != 0]\n",
    "        no_line_column_change = len(related_columns_real)\n",
    "        related_columns_real = related_columns_real.astype(bool).sum(axis=0)#.sort_values()\n",
    "        #for each value dimension, the number of players whose value dimension is changed \n",
    "\n",
    "        subsequent_columns_ideal = get_all_subsequent_nodes(edges, column)\n",
    "        unrelated_columns_ideal, related_columns_ideal = get_all_un_related_nodes(edges, column, set(data_scorechange.columns))\n",
    "        non_subsequent_columns_ideal = get_all_non_subsequent_nodes(edges, column, set(data_scorechange.columns))\n",
    "\n",
    "        if expected == 'subsequent':\n",
    "            expected_columns_ideal = subsequent_columns_ideal\n",
    "        elif expected == 'related':\n",
    "            expected_columns_ideal = related_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid expected')\n",
    "        \n",
    "        if unexpected == 'unrelated':\n",
    "            unexpected_columns_ideal = unrelated_columns_ideal\n",
    "        elif unexpected == 'non_subsequent':\n",
    "            unexpected_columns_ideal = non_subsequent_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid unexpected')\n",
    "\n",
    "        def classify(vd):\n",
    "            if vd in expected_columns_ideal:\n",
    "                return 'expected'\n",
    "            elif vd in unexpected_columns_ideal:\n",
    "                return 'unexpected'\n",
    "            else:\n",
    "                return 'none'\n",
    "\n",
    "        related_scabs = []\n",
    "        unrelated_scabs = []\n",
    "        for r_column in expected_columns_ideal:\n",
    "            related_scabs.append(related_columns_real[r_column] / no_line_column_change) #the number of players whose value dimension is changed and the value dimension is in the ideal set\n",
    "        for un_column in unexpected_columns_ideal:\n",
    "            unrelated_scabs.append(related_columns_real[un_column] / no_line_column_change) #the number of players whose value dimension is changed and the value dimension is not in the ideal set\n",
    "        for run_column in related_columns_real.index:\n",
    "            print(run_column, related_columns_real[run_column] / no_line_column_change, classify(run_column))\n",
    "        \n",
    "        print('Related:', np.mean(related_scabs))\n",
    "        print('Unrelated:', np.mean(unrelated_scabs))\n",
    "        mean_scorechange_related = 'mean_scorechange_related_' + edges_source\n",
    "        num_related = 'num_related_' + edges_source\n",
    "        mean_scorechange_unrelated = 'mean_scorechange_unrelated_' + edges_source\n",
    "        num_unrelated = 'num_unrelated_' + edges_source\n",
    "\n",
    "        pd_result_table2.loc[mean_scorechange_related, column] = np.mean(related_scabs)\n",
    "        pd_result_table2.loc[num_related, column] = len(related_scabs)\n",
    "        pd_result_table2.loc[mean_scorechange_unrelated, column] = np.mean(unrelated_scabs)\n",
    "        pd_result_table2.loc[num_unrelated, column] = len(unrelated_scabs)\n",
    "        print('----------------------')\n",
    "\n",
    "def create_table2(data_csv, modelname, edges_trained, edges_ref, stat_method, pd_result_table2):\n",
    "    assert modelname in ['gemma', 'llama']\n",
    "    data_source = modelname\n",
    "    \n",
    "    data_nosteer = data_csv[data_csv['steer_dim'].isnull()][data_csv['player_name'].notnull()]\n",
    "    data_nosteer = data_nosteer[v_inference + ['player_name']]\n",
    "    data_nosteer = data_nosteer.set_index('player_name')\n",
    "    data_nosteer = data_nosteer.astype(float)\n",
    "    data_scorechange = data_nosteer - data_nosteer.iloc[0]\n",
    "    assert v_inference == data_scorechange.columns.tolist()\n",
    "    \n",
    "    if stat_method == 'change_count':\n",
    "        write_table2_consequence_count(data_scorechange, edges_trained, data_source, pd_result_table2)\n",
    "        write_table2_consequence_count(data_scorechange, edges_ref, 'reference_' + data_source, pd_result_table2)\n",
    "    elif stat_method == 'change_score':\n",
    "        write_table2(data_scorechange, edges_trained, data_source, pd_result_table2)\n",
    "        write_table2(data_scorechange, edges_ref, 'reference_' + data_source, pd_result_table2)\n",
    "\n",
    "v_inference_gemma = [v for v in data_csv_gemma_test.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "v_inference_llama = [v for v in data_csv_llama_test.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "assert v_inference_gemma == v_inference_llama\n",
    "v_inference = v_inference_gemma\n",
    "pd_result_table2 = pd.DataFrame(columns=v_inference)\n",
    "\n",
    "create_table2(data_csv_gemma_test, 'gemma', edges_gemma_nosteer, edges_standard_small, 'change_count', pd_result_table2)\n",
    "create_table2(data_csv_llama_test, 'llama', edges_llama_nosteer, edges_standard_small, 'change_count', pd_result_table2)\n",
    "pd_result_table2.to_csv('table2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the amount of questions per value\n",
    "from sae_lens.config import LOCAL_SAE_MODEL_PATH\n",
    "#df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation_train.csv'))\n",
    "df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation_test.csv'))\n",
    "grouped = df_valuebench.groupby('value')\n",
    "value_counts = grouped.size().reset_index(name='counts')\n",
    "#get the value dimensions in v_inference\n",
    "value_counts = value_counts[value_counts['value'].isin(v_inference)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the table2 in latex\n",
    "#rows are for each values dimensions\n",
    "#columns are in form num_related_ours(mean_scorechange_related_ours), num_unrelated_ours(mean_scorechange_unrelated_ours), num_related_standard(mean_scorechange_related_standard), num_unrelated_standard(mean_scorechange_unrelated_standard)\n",
    "#the values are the number of related values, the mean of the score change of related values, the number of unrelated values, the mean of the score change of unrelated values\n",
    "#the values are rounded to 3 decimal places\n",
    "#the values are in the form number(mean)\n",
    "#the values are in the form of number(mean)\n",
    "pd_result_table2 = pd.read_csv('table2.csv', index_col=0)\n",
    "latex_code = '\\\\begin{table*}[ht]\\n\\\\caption{The mean of the score change of related values, the number of related values, the mean of the score change of unrelated values, and the number of unrelated values.}\\n\\\\label{table: scorechange}\\n\\\\begin{center}\\n'\n",
    "#latex_code += '\\\\begin{tabular}{c@{\\\\hspace{2pt}}' + 'c@{\\\\hspace{2pt}}' * (len(pd_result_table2.columns) - 1) + 'c' + '}\\n\\\\toprule\\n'\n",
    "latex_code += '\\\\begin{tabular}{c@{\\\\hspace{2pt}}|' + 'c@{\\\\hspace{2pt}}' * 4 +'|' + 'c@{\\\\hspace{2pt}}' * 4 + '}\\n\\\\toprule\\n'\n",
    "latex_code += 'Value & \\\\multicolumn{4}{c|}{\\\\bf \\\\small Gemma-2B-IT} & \\\\multicolumn{4}{c}{\\\\bf \\\\small Llama3-8B-IT}\\\\\\\\\\n\\\\hline\\n'\n",
    "latex_code += 'Dimensions & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Our causal graph} & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Random causal graph} & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Our causal graph} & \\\\multicolumn{2}{c}{\\\\bf \\\\tiny Random causal graph}  \\\\\\\\\\n\\\\hline\\n'\n",
    "latex_code += 'Score change & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Unexpected}\\\\\\\\\\n\\\\hline\\n'\n",
    "#each row in latex is a column in the dataframe\n",
    "\n",
    "gemma_prompt_ours_expected = []\n",
    "gemma_prompt_ours_unexpected = []\n",
    "gemma_prompt_ref_expected = []\n",
    "gemma_prompt_ref_unexpected = []\n",
    "llama_prompt_ours_expected = []\n",
    "llama_prompt_ours_unexpected = []\n",
    "llama_prompt_ref_expected = []\n",
    "llama_prompt_ref_unexpected = [] \n",
    "\n",
    "index_group = [\n",
    "    'mean_scorechange_related_gemma',\n",
    "    'mean_scorechange_unrelated_gemma',\n",
    "    'mean_scorechange_related_reference_gemma',\n",
    "    'mean_scorechange_unrelated_reference_gemma',\n",
    "    'mean_scorechange_related_llama',\n",
    "    'mean_scorechange_unrelated_llama',\n",
    "    'mean_scorechange_related_reference_llama',\n",
    "    'mean_scorechange_unrelated_reference_llama'\n",
    "    ]\n",
    "\n",
    "index_dict = {index: [] for index in index_group}\n",
    "\n",
    "for column in pd_result_table2.columns:\n",
    "    latex_code += '\\\\small ' + column + ' & '\n",
    "    #for index in pd_result_table2.index:\n",
    "    for index in index_group:\n",
    "        if index.startswith('mean'):\n",
    "            latex_code += str(round(pd_result_table2.loc[index, column], 2)) + ' & '\n",
    "            index_dict[index].append(pd_result_table2.loc[index, column])\n",
    "\n",
    "    latex_code = latex_code[:-2] + ' \\\\\\\\\\n'\n",
    "latex_code += '\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\end{table*}'\n",
    "print(latex_code)\n",
    "#write the latex code to a file\n",
    "with open('table2.tex', 'w') as f:\n",
    "    f.write(latex_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "\n",
    "def make_chart(data1, data2, modelname):\n",
    "    labels_ = ['Our Graph','Reference Graph','Our Graph','Reference Graph']\n",
    "    \n",
    "    #用来为坐标的常规坐标 还是 对数坐标做准备\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.8))#用来控制图片的大小\n",
    "    #fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "    # 设置柱状图参数\n",
    "    width = 0.35 #柱状图每个柱子的宽度,同时也是调整每组柱子之间的间隙\n",
    "    x = np.arange(len(labels_)) #用来指定每个柱子位置参数\n",
    "    \n",
    "    # 绘制柱状图,正常坐标\n",
    "    ax.bar(x-width/2, (data1), width=width, label='Expected')#第一个参数是该柱子的中心位置的坐标\n",
    "    ax.bar(x+width/2, (data2), width=width, label='Unexpected')\n",
    "    \n",
    "    #ax.set_yscale('log', basey=10)#设置柱子的纵坐标为对数刻度\n",
    "    \n",
    "    \n",
    "    # #在需要的位置添加数据标签，添加提高的百分比\n",
    "    # for j in range(len(data1)):\n",
    "    #     differ = (data2[j] - data1[j]) / data2[j]\n",
    "    #     x_pos = j-0.1\n",
    "    #     y_pos = data2[j] + 5\n",
    "    #     plt.text(x_pos, y_pos, str(round(data1[j],2)), ha = 'left')\n",
    "    #     plt.text(x_pos, y_pos, str(round(data2[j],2)), ha = 'right')\n",
    "        \n",
    "    #     #data1[j]保留两位小数\n",
    "    #     #plt.text(x_pos, y_pos, str(round(data1[j],2)) + ' (' + str(round(differ*100,2)) + '%)', ha = 'center')\n",
    "    #     #参数1和2是添加的文字的位置，参数3添加的文字内容\n",
    "    \n",
    "    # 设置图表标题和轴标签\n",
    "    #plt.title(f'{modelname}')\n",
    "    #plt.xlabel('Order')\n",
    "    plt.ylabel('Frequency of changes in other value dimensions')\n",
    "    \n",
    "    #plt.xticks(x, fontsize=12, rotation=45,loc='inside')#设置标签的文字大小和旋转方向\n",
    "    plt.xticks(x, labels_)  #使得标签现实的是给定的文字标签\n",
    "\n",
    "\n",
    "    p1 = patches.Rectangle((.515, 0), width=.39, height=.10, alpha=.1, facecolor='green', transform=fig.transFigure, label='Gemma-2B-IT')\n",
    "    p2 = patches.Rectangle((.125, 0), width=.39, height=.10, alpha=.1, facecolor='red', transform=fig.transFigure, label='Llama3-8B-IT')\n",
    "    #can we add a label for each rectangle?\n",
    "\n",
    "    fig.add_artist(p1)\n",
    "    fig.add_artist(p2)\n",
    "\n",
    "    fig.text(0.125 + 0.39/2, .03, 'Gemma-2B-IT', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            fontsize=12, \n",
    "            color='black')\n",
    "    fig.text(0.515 + 0.39/2, .03, 'Llama3-8B-IT', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            fontsize=12, \n",
    "            color='black')\n",
    "\n",
    "    # 设置图例\n",
    "    plt.legend(loc='upper right', ncol=2)#设置图例的位置和列数\n",
    "    \n",
    "    #获取默认图片尺寸\n",
    "    figure = plt.gcf()\n",
    "    width = figure.bbox.width\n",
    "    height = figure.bbox.height\n",
    "    print(width,height)\n",
    "    \n",
    "    #plt.tight_layout(pad=10)\n",
    "\n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 需要画图的数据\n",
    "\n",
    "data1 = [\n",
    "    np.nanmean(index_dict['mean_scorechange_related_gemma']), \n",
    "    np.nanmean(index_dict['mean_scorechange_related_reference_gemma']),\n",
    "    np.nanmean(index_dict['mean_scorechange_related_llama']),\n",
    "    np.nanmean(index_dict['mean_scorechange_related_reference_llama'])\n",
    "    ]\n",
    "data2 = [\n",
    "    np.nanmean(index_dict['mean_scorechange_unrelated_gemma']), \n",
    "    np.nanmean(index_dict['mean_scorechange_unrelated_reference_gemma']),\n",
    "    np.nanmean(index_dict['mean_scorechange_unrelated_llama']),\n",
    "    np.nanmean(index_dict['mean_scorechange_unrelated_reference_llama'])\n",
    "    ]\n",
    "make_chart(data1, data2, 'Gemma-2B-IT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steer_dims = ['nan', 1312, 1341, 2221, 3183, 6619, 7502, 8387, 10096, 14049]\n",
    "\n",
    "nodes = {}\n",
    "for entity in v_inference:\n",
    "    nodes[entity] = os.path.join('valuebench','value_questions_' + entity + '.html'),\n",
    "# for feature in data_csv.['steer_dim'].unique()[1:]:\n",
    "#     nodes[feature] = 'https://www.neuronpedia.org/' + sae.cfg.model_name +'/' + str(sae.cfg.hook_layer) + '-res-jb/' + str(feature)\n",
    "\n",
    "edges = {\n",
    "    'gemma': edges_gemma_nosteer,\n",
    "    'llama': edges_llama_nosteer,\n",
    "    'reference': edges_standard_small\n",
    "}\n",
    "\n",
    "json_object = {\n",
    "    'nodes': nodes,\n",
    "    'edges': edges\n",
    "    }\n",
    "\n",
    "json.dump(json_object, open('value_graph_data1.json', 'w'))\n",
    "\n",
    "for node in v_inference:\n",
    "    subsqeuent_nodes_gemma = get_all_subsequent_nodes(edges_gemma_nosteer, node)\n",
    "    subsqeuent_nodes_standard = get_all_subsequent_nodes(edges_standard, node)\n",
    "    print(node)\n",
    "    print(subsqeuent_nodes_gemma)\n",
    "    print(subsqeuent_nodes_standard)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
