{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNk7IylTv610"
   },
   "source": [
    "# Loading and Analysing Pre-Trained Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_DusoOvwV0M"
   },
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aGgWkbav610"
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yfDUxRx0wSRl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
    "from causallearn.utils.cit import kci\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
    "from causallearn.utils.cit import fisherz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_color = '#42A5F5'\n",
    "yellow_color = '#FFCA28'\n",
    "\n",
    "\n",
    "#T2P\n",
    "answer_valuebench_features_csv_gemma_train = os.path.join('useful_data',\"ans_gemma_train_formal.csv\")\n",
    "answer_valuebench_features_csv_gemma_test = os.path.join('useful_data',\"ans_gemma_test_formal.csv\")\n",
    "answer_valuebench_features_csv_llama_train = os.path.join('useful_data',\"ans_llama_train_formal.csv\")\n",
    "answer_valuebench_features_csv_llama_test = os.path.join('useful_data',\"ans_llama_test_formal.csv\")\n",
    "# answer_valuebench_features_csv_gemma_train = os.path.join('useful_data',\"ans_cross_part1.csv\")\n",
    "# answer_valuebench_features_csv_gemma_test = os.path.join('useful_data',\"ans_cross_part2.csv\")\n",
    "# answer_valuebench_features_csv_llama_train = os.path.join('useful_data',\"ans_llama_old_train.csv\")\n",
    "# answer_valuebench_features_csv_llama_test = os.path.join('useful_data',\"ans_llama_old_test.csv\")\n",
    "\n",
    "data_csv_gemma_train = pd.read_csv(answer_valuebench_features_csv_gemma_train)\n",
    "data_csv_gemma_test = pd.read_csv(answer_valuebench_features_csv_gemma_test)\n",
    "data_csv_llama_train = pd.read_csv(answer_valuebench_features_csv_llama_train)\n",
    "data_csv_llama_test = pd.read_csv(answer_valuebench_features_csv_llama_test)\n",
    "\n",
    "#T2P\n",
    "\n",
    "exclude_columns_gemma = ['Anxiety Disorder', 'Achievement', 'Empathy', 'Organization', 'Political', 'Economic', 'Social Complexity']\n",
    "#exclude_rows_gemma = [14351, 12703, 10454, 8387, 6884, 6126, 6188, 2221,1025, 428,]\n",
    "gemma_features = [14351, 12703, 10454, 8387, 6884, 6126, 6188, 2221,1025, 428, 1312,1341,1975,2965,3183,3402,4752,6216,6619,7502,10096,10605,11712,14049,14185]\n",
    "good_gemma_features = [14351, 1025, 2965,10096, 1341, 1975 , 10605, 14049, 1312,4752 ]# ,3402, \n",
    "# exclude_columns_gemma = []\n",
    "# good_gemma_features = gemma_features\n",
    "exclude_rows_gemma = [i for i in gemma_features if i not in good_gemma_features]\n",
    "\n",
    "\n",
    "\n",
    "exclude_columns_llama = ['Anxiety Disorder', 'Achievement', 'Empathy', 'Organization', 'Political', 'Economic', 'Social Complexity']\n",
    "#exclude_rows_llama = [60312, 7754, 13033, 1897, 2509, 20141, 41929, 48321, 63905, 49202, 2246, 58305,]\n",
    "llama_features = [63905, 48321, 1897, 2509, 60312, 7754, 49202, 2246, 13033, 58305, 41929, 20141, 12477, 54606, 21347, 4305, 30919, 34598, 51010, 62769, 47207, 8035, 8546, 9332]\n",
    "good_llama_features = [49202, 58305, 8546, 12477, 7754, 9332, 1897,  62769, 47207, 12477, 54606] #best 49202, 58305, 8546 #okay 13033, 12477, 21347, 4305, 62769, 47207, 2509 \n",
    "#20141,  54606\n",
    "# exclude_columns_llama = []\n",
    "# good_llama_features = llama_features\n",
    "exclude_rows_llama = [i for i in llama_features if i not in good_llama_features]\n",
    "\n",
    "data_csv_gemma_train = data_csv_gemma_train[~data_csv_gemma_train['steer_dim'].isin(exclude_rows_gemma)]\n",
    "data_csv_gemma_test = data_csv_gemma_test[~data_csv_gemma_test['steer_dim'].isin(exclude_rows_gemma)]\n",
    "data_csv_llama_train = data_csv_llama_train[~data_csv_llama_train['steer_dim'].isin(exclude_rows_llama)]\n",
    "data_csv_llama_test = data_csv_llama_test[~data_csv_llama_test['steer_dim'].isin(exclude_rows_llama)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_new_diff(data_csv_raw, modelname, threshold_judge):\n",
    "    assert threshold_judge >= 0\n",
    "    pathname = 'value_dims_rsd_' + modelname\n",
    "    stat_csv_23 = pathname + '/23_stat.csv'\n",
    "    data_new_diff_count_total = pd.DataFrame()\n",
    "\n",
    "    if os.path.exists('pathname'):\n",
    "        shutil.rmtree('pathname')\n",
    "    os.makedirs(pathname, exist_ok=True)\n",
    "    \n",
    "    for column in data_csv_raw.columns:\n",
    "        if column == 'player_name' or column == 'steer_dim' or column == 'stds' or column =='scstds' or column.endswith(':scstd'):\n",
    "            continue\n",
    "        value_csv = pathname + '/' + column + '.csv'\n",
    "        data_new = data_csv_raw.pivot(index='steer_dim', columns='player_name', values=column)\n",
    "        data_new_scstd = data_csv_raw.pivot(index='steer_dim', columns='player_name', values=column+':scstd')\n",
    "        data_save = data_new.astype(str) + 'Â±' + data_new_scstd.astype(str) #problems here: the scstd is not the std for the score, but fore the changed score\n",
    "        data_save.to_csv(value_csv)\n",
    "\n",
    "        data_new_diff = data_new - data_new[data_new.index.isnull()].iloc[0]\n",
    "\n",
    "        data_new_diff_count_higher = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if y > threshold_judge else 0))\n",
    "        data_new_diff_count_higher = data_new_diff_count_higher.sum(axis=1)\n",
    "        data_new_diff_count_lower = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if y < -threshold_judge else 0))\n",
    "        data_new_diff_count_lower = data_new_diff_count_lower.sum(axis=1)\n",
    "        data_new_diff_count_equal = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if abs(y) <= threshold_judge else 0))\n",
    "        data_new_diff_count_equal = data_new_diff_count_equal.sum(axis=1)\n",
    "\n",
    "        data_new_diff_count = data_new_diff_count_higher.astype(str) + '/' + data_new_diff_count_lower.astype(str) + '/' + data_new_diff_count_equal.astype(str)\n",
    "        data_new_diff_count_total[column] = data_new_diff_count\n",
    "\n",
    "    data_new_diff_count_total.to_csv(stat_csv_23)\n",
    "\n",
    "get_data_new_diff(data_csv_gemma_train, 'gemma', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_gemma_test, 'gemmatest', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_llama_train, 'llama', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_llama_test, 'llamatest', threshold_judge=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table1_newest(stat_csv_23_train, stat_csv_23_test):\n",
    "    data_new_diff_count_total_train = pd.read_csv(stat_csv_23_train)\n",
    "    data_new_diff_count_total_test = pd.read_csv(stat_csv_23_test)\n",
    "\n",
    "    table1_rows = data_new_diff_count_total_train['steer_dim'].unique()\n",
    "    table1_rows = table1_rows[~np.isnan(table1_rows)]\n",
    "    table1_columns = data_new_diff_count_total_train.columns[data_new_diff_count_total_train.columns != 'steer_dim']\n",
    "\n",
    "    table1_rows_test = data_new_diff_count_total_test['steer_dim'].unique()\n",
    "    table1_rows_test = table1_rows_test[~np.isnan(table1_rows_test)]\n",
    "    table1_columns_test = data_new_diff_count_total_test.columns[data_new_diff_count_total_test.columns != 'steer_dim']\n",
    "\n",
    "    assert np.array_equal(table1_rows, table1_rows_test)\n",
    "    assert np.array_equal(table1_columns, table1_columns_test)\n",
    "    \n",
    "    table1 = pd.DataFrame(columns=table1_columns, index=table1_rows)\n",
    "    \n",
    "    for steer_dim in table1_rows:\n",
    "        assert not np.isnan(steer_dim)\n",
    "\n",
    "        steer_dim_row_train = data_new_diff_count_total_train[data_new_diff_count_total_train['steer_dim'] == steer_dim]\n",
    "        steer_dim_row_test = data_new_diff_count_total_test[data_new_diff_count_total_test['steer_dim'] == steer_dim]\n",
    "\n",
    "        for column in table1_columns:\n",
    "            assert column != 'steer_dim'\n",
    "            #split cell by /\n",
    "            counts_train = steer_dim_row_train[column].values[0].split('/')   \n",
    "            simu_train = int(counts_train[0])\n",
    "            supp_train = int(counts_train[1])\n",
    "            main_train = int(counts_train[2])\n",
    "            \n",
    "            counts_test = steer_dim_row_test[column].values[0].split('/')\n",
    "            simu_test = int(counts_test[0])\n",
    "            supp_test = int(counts_test[1])\n",
    "            main_test = int(counts_test[2])\n",
    "\n",
    "            table1.loc[steer_dim, column] = str(simu_train) + '/' + str(supp_train) + '/' + str(main_train) + '/' + str(simu_test) + '/' + str(supp_test) + '/' + str(main_test)\n",
    "    return table1\n",
    "\n",
    "table1_gemma = get_table1_newest('value_dims_rsd_gemma/23_stat.csv', 'value_dims_rsd_gemmatest/23_stat.csv')\n",
    "table1_gemma.to_csv('table1_gemma_newest.csv')\n",
    "table1_llama = get_table1_newest('value_dims_rsd_llama/23_stat.csv', 'value_dims_rsd_llamatest/23_stat.csv')\n",
    "table1_llama.to_csv('table1_llama_newest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_valuebench_features_csv_gemma_compare_positve = os.path.join('useful_data',\"ans_gemma_test_compare_positive_s1.csv\")\n",
    "answer_valuebench_features_csv_gemma_compare_negative = os.path.join('useful_data',\"ans_gemma_test_compare_negative_s1.csv\")\n",
    "answer_valuebench_features_csv_llama_compare_positve = os.path.join('useful_data',\"ans_llama_test_compare_positive.csv\")\n",
    "answer_valuebench_features_csv_llama_compare_negative = os.path.join('useful_data',\"ans_llama_test_compare_negative.csv\")\n",
    "data_csv_gemma_compare_positive = pd.read_csv(answer_valuebench_features_csv_gemma_compare_positve)\n",
    "data_csv_gemma_compare_negative = pd.read_csv(answer_valuebench_features_csv_gemma_compare_negative)\n",
    "data_csv_llama_compare_positive = pd.read_csv(answer_valuebench_features_csv_llama_compare_positve)\n",
    "data_csv_llama_compare_negative = pd.read_csv(answer_valuebench_features_csv_llama_compare_negative)\n",
    "\n",
    "# for each value, check the status of all roles in data_csv_gemma_test without sae steer\n",
    "value_steers_gemma_negative = {'Aesthetic': 10605, 'Resilience':4752, 'Social Cynicism': 4752, 'Positive coping':10096, 'Religious':1341, 'Breadth of Interest': 14049, 'Understanding':1975,'Theoretical':1975, 'Uncertainty Avoidance':4752,  'Social':1975, } #'\n",
    "value_steers_gemma_positive = {'Positive coping':1975, 'Theoretical': 10096, 'Uncertainty Avoidance': 1312, 'Aesthetic': 1312, 'Breadth of Interest':10096, 'Religious':1312, 'Social':14049, 'Understanding':4752,  'Resilience': 1341, 'Social Cynicism': 14049}\n",
    "value_steers_llama_negative = {'Aesthetic': 58305, 'Resilience':7754, 'Social Cynicism': 47207, 'Positive coping':7754, 'Religious':47207, 'Breadth of Interest': 7754, 'Understanding':7754,'Theoretical':7754, 'Uncertainty Avoidance':12477,  'Social':7754, } #'\n",
    "value_steers_llama_positive = {'Positive coping':9332, 'Theoretical': 9332, 'Uncertainty Avoidance': 7754, 'Aesthetic': 49202, 'Breadth of Interest': 58305, 'Religious':9332, 'Social': 54606, 'Understanding':47207,  'Resilience': 9332, 'Social Cynicism': 7754}#Uncertainty Avoidance':62769\n",
    "\n",
    "\n",
    "#check data_csv_gemma_test, pay attention to the values in value_steers_gemma_negative, check the difference between the not doing steering and doing steering using the saes in value_steers_gemma_negative\n",
    "def steer_compare(sae_dict, data_csv,  data_csv_compare):\n",
    "    from collections import Counter\n",
    "    changed_roles_sae = Counter()\n",
    "    changed_roles_compare = Counter()\n",
    "    for value_name, steer in sae_dict.items():\n",
    "        data_csv_steering_none = data_csv[np.isnan(data_csv['steer_dim'])][['player_name', value_name]]\n",
    "        data_csv_steering_bestsae = data_csv[data_csv['steer_dim'] == steer][['player_name', value_name]]\n",
    "        data_csv_steering_compare = data_csv_compare[['player_name', value_name]]\n",
    "        data_csv_steering_compare = data_csv_steering_compare[data_csv_steering_compare['player_name'].isin(data_csv_steering_none['player_name'])]\n",
    "        assert data_csv_steering_none['player_name'].tolist() == data_csv_steering_bestsae['player_name'].tolist() == data_csv_steering_compare['player_name'].tolist()\n",
    "        \n",
    "\n",
    "        \n",
    "        diff1 = data_csv_steering_bestsae[value_name].to_numpy() - data_csv_steering_none[value_name].to_numpy()\n",
    "        diff2 = data_csv_steering_compare[value_name].to_numpy() - data_csv_steering_none[value_name].to_numpy()\n",
    "        #map diff1 to -1, 0, 1\n",
    "        changed_roles_sae['positive'] += np.sum(np.sign(diff1) == 1)\n",
    "        changed_roles_sae['negative'] += np.sum(np.sign(diff1) == -1)\n",
    "        changed_roles_sae['zero'] += np.sum(np.sign(diff1) == 0)\n",
    "        changed_roles_compare['positive'] += np.sum(np.sign(diff2) == 1)\n",
    "        changed_roles_compare['negative'] += np.sum(np.sign(diff2) == -1)\n",
    "        changed_roles_compare['zero'] += np.sum(np.sign(diff2) == 0)\n",
    "        print(value_name, np.sum(np.sign(diff1)))\n",
    "        print(value_name, np.sum(np.sign(diff2)))\n",
    "        print('++++++++++')\n",
    "        # changed_roles_sae.append(sign * np.sum(np.sign(diff1)))\n",
    "        # changed_roles_compare.append(sign * np.sum(np.sign(diff2)))\n",
    "        \n",
    "    return changed_roles_sae, changed_roles_compare\n",
    "change_roles_sae_gemma_postive, changed_roles_compare_gemma_positive = steer_compare(value_steers_gemma_positive, data_csv_gemma_test, data_csv_gemma_compare_positive)\n",
    "change_roles_sae_gemma_negative, changed_roles_compare_gemma_negative = steer_compare(value_steers_gemma_negative, data_csv_gemma_test, data_csv_gemma_compare_negative)\n",
    "change_roles_sae_llama_postive, changed_roles_compare_llama_positive = steer_compare(value_steers_llama_positive, data_csv_llama_test, data_csv_llama_compare_positive)\n",
    "change_roles_sae_llama_negative, changed_roles_compare_llama_negative = steer_compare(value_steers_llama_negative, data_csv_llama_test, data_csv_llama_compare_negative)\n",
    "\n",
    "#draw a 1*2 table of pie charts, each pie chart shows the percentage of positive, negative, and zero changes in the roles\n",
    "#the columns are the SAE and the compare\n",
    "#begin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def draw_pie_chart(count_1, count_0, count_neg1, steer_method):\n",
    "    labels = ['Positively Changed Roles', 'Unchanged Roles', 'Negatively Changed Roles']\n",
    "    sizes = [count_1, count_0, count_neg1]\n",
    "    colors = [blue_color, '#ffffff', yellow_color]\n",
    "    \n",
    "    \n",
    "    explode = (0, 0, 0)  # explode the 1st slice (Positive)\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    wedges, texts, autotexts = plt.pie(sizes, explode=explode, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140, labeldistance=0.85, textprops={'fontsize': 30})\n",
    "    #plt.legend(wedges, labels, title=\"Role Changes\", loc=\"center\", bbox_to_anchor=(0.5, 0, 0.5, 1))\n",
    "\n",
    "    #plt.title('Value Changing Direction Results for ' + steer_method)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "    # centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "    # fig = plt.gcf()\n",
    "    # fig.gca().add_artist(centre_circle)\n",
    "\n",
    "draw_pie_chart(change_roles_sae_gemma_postive['positive'], change_roles_sae_gemma_postive['zero'], change_roles_sae_gemma_postive['negative'], 'Positive SAE Steering on Gemma-2B-IT')\n",
    "draw_pie_chart(changed_roles_compare_gemma_positive['positive'], changed_roles_compare_gemma_positive['zero'], changed_roles_compare_gemma_positive['negative'], 'Positive Hard Prompt on Gemma-2B-IT')\n",
    "draw_pie_chart(change_roles_sae_gemma_negative['positive'], change_roles_sae_gemma_negative['zero'], change_roles_sae_gemma_negative['negative'], 'Negative SAE Steering on Gemma-2B-IT')\n",
    "draw_pie_chart(changed_roles_compare_gemma_negative['positive'], changed_roles_compare_gemma_negative['zero'], changed_roles_compare_gemma_negative['negative'], 'Negative Hard Prompt on Gemma-2B-IT')\n",
    "draw_pie_chart(change_roles_sae_llama_postive['positive'], change_roles_sae_llama_postive['zero'], change_roles_sae_llama_postive['negative'], 'Positive SAE Steering on LLAMA3-8B-IT')\n",
    "draw_pie_chart(changed_roles_compare_llama_positive['positive'], changed_roles_compare_llama_positive['zero'], changed_roles_compare_llama_positive['negative'], 'Positive Hard Prompt on LLAMA3-8B-IT')\n",
    "draw_pie_chart(change_roles_sae_llama_negative['positive'], change_roles_sae_llama_negative['zero'], change_roles_sae_llama_negative['negative'], 'Negative SAE Steering on LLAMA3-8B-IT')\n",
    "draw_pie_chart(changed_roles_compare_llama_negative['positive'], changed_roles_compare_llama_negative['zero'], changed_roles_compare_llama_negative['negative'], 'Negative Hard Prompt on LLAMA3-8B-IT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_steer_of_value(data_test_csv, value):\n",
    "    data_test_csv = data_test_csv[data_test_csv['steer_dim'] == value]\n",
    "    return \n",
    "get_best_steer_of_value(data_csv_gemma_test, 'Aesthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_table1_rotate_tutu(table1, table1_name, modelname, exclude_rows=None, exclude_columns=None):\n",
    "    if exclude_rows is None:\n",
    "        exclude_rows = []\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "\n",
    "    #value_dims = [str(vd) for vd in table1.index if vd not in exclude_rows]\n",
    "    value_dims = [str(vd) for vd in table1.columns if vd not in exclude_columns]\n",
    "    #steering_features = [int(sf) for sf in table1.columns if sf not in exclude_columns]\n",
    "    steering_features = [int(sf) for sf in table1.index if sf not in exclude_rows]\n",
    "\n",
    "    table_filtered = table1.loc[steering_features, value_dims]\n",
    "\n",
    "    # åå§å LaTeX ä»£ç \n",
    "    latex_code = '''\n",
    "\\\\newcommand{\\\\cellbar}[5]{%\n",
    "    \\\\raisebox{\\\\height}{%\n",
    "        \\\\begin{tikzpicture}[baseline=(current bounding box.center)]\n",
    "            \\\\draw[draw=black] (0,0) rectangle (1cm,0.4cm);\n",
    "            \\\\path[fill=CustomBlue, opacity=#5] (0,0) rectangle (#1cm,0.4cm);\n",
    "            \\\\path[fill=white] (#1cm,0) rectangle ({#1cm + #3cm},0.4cm);\n",
    "            \\\\path[fill=CustomYellow, opacity=#5] ({#1cm + #3cm},0) rectangle (1cm,0.4cm);\n",
    "            \\\\node[anchor=center, font=\\\\scriptsize] at (0.5cm,0.2cm) {#2};\n",
    "        \\\\end{tikzpicture}%\n",
    "    }%\n",
    "}\n",
    "\\\\begin{table*}\n",
    "\\\\centering\n",
    "\\\\caption{Value steering using SAE features for the {''' + modelname + '''} model.}\n",
    "\\\\label{table: sae-steering-''' + modelname + '''}\n",
    "\\\\resizebox{\\\\textwidth}{!}{%\n",
    "\\\\begin{tabular}{>{\\\\centering\\\\arraybackslash}m{1.5cm} *{''' + str(len(value_dims)) + '''}{>{\\\\centering\\\\arraybackslash}m{1cm}}>{\\\\centering\\\\arraybackslash}m{1cm}}\n",
    "\\\\toprule\n",
    "'''\n",
    "\n",
    "\n",
    "    # çæè¡¨å¤´\n",
    "    header_row = ['\\\\diagbox[width=1.8cm, height=2.7cm]{\\\\textbf{SAE}\\\\\\\\\\\\textbf{Feature}}{\\\\rotatebox{90}{\\\\textbf{Value}}}'] + ['\\\\rotatebox{90}{\\\\textbf{' + vd + '}}' for vd in value_dims] + ['\\\\rotatebox{90}{\\\\textbf{Mean Similarity}}']\n",
    "    latex_code += ' & '.join(header_row) + ' \\\\\\\\\\n\\\\midrule\\n'\n",
    "    latex_code += '\\\\multicolumn{' + str(len(value_dims)+2) + '}{c}{\\\\textbf{\\\\small{' + modelname + '}}}' + ' \\\\\\\\\\n\\\\midrule\\n'\n",
    "\n",
    "    # å®ä¹ååæ ¼åå®¹ççæå½æ°\n",
    "    def generate_cell_content(red_ratio, green_ratio, transparency_ratio, similarity):\n",
    "        \"\"\"\n",
    "        çæååæ ¼ç LaTeX ä»£ç ï¼ä½¿ç¨ \\\\cellbar{red_length}{similarity}{transparency_length}{green_length}{opacity}\n",
    "        \"\"\"\n",
    "        # ç¡®ä¿æ¯ä¾å¨ 0 å° 1 ä¹é´ï¼å¹¶å½ä¸å\n",
    "        total = red_ratio + green_ratio + transparency_ratio\n",
    "        if total > 0:\n",
    "            red_ratio /= total\n",
    "            green_ratio /= total\n",
    "            transparency_ratio /= total\n",
    "        else:\n",
    "            red_ratio = green_ratio = transparency_ratio = 0.0\n",
    "\n",
    "        red_length = red_ratio\n",
    "        transparency_length = transparency_ratio\n",
    "        green_length = green_ratio\n",
    "\n",
    "        opacity = 1.0 - transparency_ratio  # éæåº¦ä¸éæé¨åæåæ¯\n",
    "\n",
    "        return f'\\\\cellbar{{{red_length:.2f}}}{{{similarity:.2f}}}{{{transparency_length:.2f}}}{{{green_length:.2f}}}{{{opacity:.2f}}}'\n",
    "\n",
    "    # çæè¡¨æ ¼åå®¹\n",
    "    lowest_ratios = {vd: [] for vd in value_dims} \n",
    "    for sf in steering_features:\n",
    "        cosines = []\n",
    "        row = ['\\\\textbf{' + str(sf) + '}']\n",
    "        for vd in value_dims:\n",
    "            value = table_filtered.loc[sf, vd]\n",
    "            if isinstance(value, str):\n",
    "                # å¤çå­ç¬¦ä¸²å¼ï¼è®¡ç®ç¸ä¼¼åº¦\n",
    "                value_list = list(map(int, value.split('/')))\n",
    "                traindata = value_list[:3]\n",
    "                testdata = value_list[3:]\n",
    "                # è®¡ç®ç¸ä¼¼åº¦\n",
    "                traindata_p = np.array(traindata) / np.sum(traindata)\n",
    "                testdata_p = np.array(testdata) / np.sum(testdata)\n",
    "                similarity = np.dot(traindata_p, testdata_p) / (np.linalg.norm(traindata_p) * np.linalg.norm(testdata_p))\n",
    "                cosines.append(similarity)\n",
    "                lowest_ratios[vd].append(min(traindata_p))\n",
    "\n",
    "                # æåçº¢è²ãç»¿è²åéæåº¦çå¼\n",
    "                red_value = traindata[0]\n",
    "                green_value = traindata[1]\n",
    "                transparency_value = traindata[2]\n",
    "                # è®¡ç®æ¯ä¾\n",
    "                red_ratio = red_value\n",
    "                green_ratio = green_value\n",
    "                transparency_ratio = transparency_value\n",
    "\n",
    "                # çæååæ ¼åå®¹\n",
    "                cell_content = generate_cell_content(red_ratio, green_ratio, transparency_ratio, similarity)\n",
    "                row.append(cell_content)\n",
    "            else:\n",
    "                row.append('')\n",
    "\n",
    "        avg_similarity = np.mean(cosines) if cosines else 0\n",
    "        row.append(f'{avg_similarity:.2f}')\n",
    "        latex_code += ' & '.join(row) + ' \\\\\\\\\\n'\n",
    "    latex_code += '\\\\midrule\\nNoise Ratio:& ' + '&'.join([f'{np.mean(lowest_ratios[vd]):.2f}' for vd in value_dims]) + ' \\\\\\\\\\n'\n",
    "    latex_code += '\\\\bottomrule\\n\\\\end{tabular}\\n}\\n\\\\end{table*}\\n'\n",
    "\n",
    "    # ä¿å­ LaTeX ä»£ç \n",
    "    with open(f'{table1_name}.tex', 'w') as f:\n",
    "        f.write(latex_code)\n",
    "  \n",
    "\n",
    "get_latex_table1_rotate_tutu(table1_gemma, 'table1_gemma_rotate', 'Gemma-2B-IT', exclude_rows_gemma, exclude_columns_gemma)\n",
    "get_latex_table1_rotate_tutu(table1_llama, 'table1_llama_rotate', 'Llama3-8B-IT', exclude_rows_llama, exclude_columns_llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T2P\n",
    "CAUSAL_METHOD = 'pc'\n",
    "NOISE_AUGUMENT_SINGLE_SAE = None #10\n",
    "NOISE_VAR = 0.00001\n",
    "PC_ALPHA = 0.05 #0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_d_columns_deprecated(answer_valuebench_features_csv):\n",
    "    data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
    "    digits = [str(d) for d in range(10)]\n",
    "    d_columns = [d for d in data_csv.columns if d[0] in digits]\n",
    "    d_data = data_csv[d_columns]\n",
    "    stds = d_data.std()\n",
    "    avgs = d_data.mean()\n",
    "    std_avg = stds/avgs\n",
    "    #d_columns_valid = [d for d in d_columns if avgs[d] > 1]\n",
    "    d_columns_valid = d_columns\n",
    "    return d_columns_valid\n",
    "\n",
    "def get_value_dims_from_csv(data_csv):\n",
    "    return [v for v in data_csv.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "\n",
    "def causal_inference(data, ci_dimensions, pdy_name, method, noise_augument=None, prior_source_set=None):\n",
    "    if noise_augument:\n",
    "        data = np.tile(data, (noise_augument, 1))\n",
    "        noise = np.random.normal(0, NOISE_VAR, data.shape)\n",
    "        data = data + noise\n",
    "\n",
    "    if method == 'pc':\n",
    "        g = pc(data, PC_ALPHA, node_names=ci_dimensions)\n",
    "        \n",
    "        if prior_source_set:\n",
    "            bk = BackgroundKnowledge()\n",
    "            nodes = g.G.get_nodes()\n",
    "            for node1 in nodes:\n",
    "                for node2 in nodes:\n",
    "                    if node1.name in prior_source_set and node2.name in prior_source_set and node1.name != node2.name:\n",
    "                        bk = bk.add_forbidden_by_node(node1, node2)\n",
    "            g = pc(data, PC_ALPHA, node_names=ci_dimensions, background_knowledge=bk)\n",
    "            \n",
    "        graph = g.G\n",
    "\n",
    "        edges = []\n",
    "        for n1 in range(len(graph.nodes)):\n",
    "            assert graph.nodes[n1].name == ci_dimensions[n1]\n",
    "            for n2 in range(n1+1, len(graph.nodes)):\n",
    "                # if n1 == n2:\n",
    "                #     continue\n",
    "                if graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == 1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'single-arrow'])\n",
    "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == -1:\n",
    "                    edges.append([graph.nodes[n2].name, graph.nodes[n1].name, 1, 'single-arrow']) \n",
    "                elif graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == -1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'no-arrow'])\n",
    "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == 1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'double-arrow'])\n",
    "                else:\n",
    "                    if not (graph.graph[n1][n2] == 0 and graph.graph[n2][n1] == 0):\n",
    "                        raise ValueError('Invalid edge')\n",
    "    else:\n",
    "        raise ValueError('Invalid method')\n",
    "    \n",
    "    columns_concerned_vis = [label.replace(':','-') for label in ci_dimensions]\n",
    "    pdy = GraphUtils.to_pydot(graph, labels=columns_concerned_vis)\n",
    "    pdy.write_png(pdy_name)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def deal_with_csv(data_csv, graph_path, v_showongraph='ALL', row_num='ALL', method=CAUSAL_METHOD, dummy_steered_dim=False): \n",
    "    v_inference = get_value_dims_from_csv(data_csv)\n",
    "    if os.path.exists(graph_path):\n",
    "        shutil.rmtree(graph_path)\n",
    "    os.makedirs(graph_path, exist_ok=True)\n",
    "    \n",
    "    # data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
    "    # v_columns_all = [v for v in data_csv.columns if (v not in ['player_name', 'steer_dim', 'stds']) and (not v.endswith(':scstd'))]\n",
    "    # if v_inference == 'ALL':\n",
    "    #     v_columns_inference = v_columns_all\n",
    "    # else:\n",
    "    #     for v in v_inference:\n",
    "    #         if v not in v_columns_all:\n",
    "    #             raise ValueError('Invalid v_inference')\n",
    "    #     v_columns_inference = v_inference\n",
    "\n",
    "    v_columns_inference = v_inference\n",
    "\n",
    "    if v_showongraph == 'ALL':\n",
    "        v_columns_showgraph = v_columns_inference\n",
    "    else:\n",
    "        for v in v_showongraph:\n",
    "            if v not in v_columns_inference:\n",
    "                raise ValueError('Invalid v_showongraph')\n",
    "        v_columns_showgraph = v_showongraph\n",
    "\n",
    "    if dummy_steered_dim:\n",
    "        steer_dim_dummies = pd.get_dummies(data_csv['steer_dim'], prefix='steer_dim') * 1\n",
    "        data = pd.concat([data_csv, steer_dim_dummies], axis=1)\n",
    "        v_columns_inference_total = v_columns_inference + list(steer_dim_dummies.columns) \n",
    "        v_columns_showgraph_total = v_columns_showgraph + list(steer_dim_dummies.columns)\n",
    "    else:\n",
    "        data = data_csv\n",
    "        v_columns_inference_total = v_columns_inference\n",
    "        v_columns_showgraph_total = v_columns_showgraph\n",
    "    \n",
    "    data = data[v_columns_inference_total].to_numpy()    \n",
    "    \n",
    "    if type(row_num) == int:\n",
    "        rows = np.random.choice(data.shape[0], row_num, replace=False)\n",
    "        data = data[rows]\n",
    "    else:\n",
    "        assert row_num == 'ALL'\n",
    "\n",
    "    if dummy_steered_dim:\n",
    "        edges_total = causal_inference(data, v_columns_inference_total, graph_path + \"/total.png\", method, noise_augument=None, prior_source_set=list(steer_dim_dummies.columns))\n",
    "    else:\n",
    "        edges_total = causal_inference(data, v_columns_inference_total, graph_path + \"/total.png\", method, noise_augument=None)\n",
    "    \n",
    "    edges_sfs = []\n",
    "    steer_dims = data_csv['steer_dim'].unique()\n",
    "    for steer_dim in steer_dims:\n",
    "        print(steer_dim)\n",
    "        if np.isnan(steer_dim):\n",
    "            data = data_csv[data_csv['steer_dim'].isnull()][v_columns_inference].to_numpy()\n",
    "            edges_nosteer = causal_inference(data, v_columns_inference, graph_path + f'/{steer_dim}.png', method, noise_augument=NOISE_AUGUMENT_SINGLE_SAE)\n",
    "        else:\n",
    "            data = data_csv[data_csv['steer_dim'] == steer_dim][v_columns_inference].to_numpy()\n",
    "            sfedge = causal_inference(data, v_columns_inference, graph_path + f'/{steer_dim}.png', method, noise_augument=NOISE_AUGUMENT_SINGLE_SAE)\n",
    "            edges_sfs.append(sfedge)\n",
    "\n",
    "    return edges_total, edges_nosteer, edges_sfs\n",
    "\n",
    "#T2P\n",
    "edges_gemma_total, edges_gemma_nosteer, edges_gemma_sfs = deal_with_csv(data_csv_gemma_train, 'value_causal_graph_gemma')\n",
    "edges_llama_total, edges_llama_nosteer, edges_llama_sfs = deal_with_csv(data_csv_llama_train, 'value_causal_graph_llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "edges_standard_small = [\n",
    "    ['Positive coping', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Social', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Anxiety Disorder', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Economic', 'Organization', 1, 'single-arrow'],\n",
    "    ['Political', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Religious', 'Understanding', 1, 'single-arrow'],\n",
    "]\n",
    "\n",
    "edges_llama_gen = [\n",
    "    ['Understanding', 'Empathy', 1, 'single-arrow'],# Greater understanding leads to increased empathy.\n",
    "    ['Social Complexity', 'Social', 1, 'single-arrow'],# Increased social complexity leads to a greater emphasis on social values.\n",
    "    ['Anxiety Disorder', 'Uncertainty Avoidance', 1, 'single-arrow'],# Anxiety may increase the need to avoid uncertainty.\n",
    "    ['Social Cynicism', 'Social Complexity', 1, 'single-arrow'],# Cynicism might arise from perceiving social structures as complex and untrustworthy.\n",
    "    ['Achievement', 'Economic', 1, 'single-arrow'],# A focus on achievement often aligns with economic values.\n",
    "    ['Organization', 'Economic', 1, 'single-arrow'],# Organizational structures often prioritize economic efficiency.\n",
    "    ['Political', 'Social', 1, 'single-arrow'],# Political values often influence social norms and structures.\n",
    "    ['Religious', 'Theoretical', 1, 'single-arrow'],# Religious beliefs often involve theoretical or philosophical frameworks.\n",
    "    ['Breadth of Interest', 'Theoretical', 1, 'single-arrow'],# A broad range of interests can lead to a greater appreciation for theoretical concepts.\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],# Avoiding uncertainty can contribute to anxiety disorders.\n",
    "    ['Social', 'Empathy', 1, 'single-arrow'],# Social values emphasize the importance of empathy and understanding.\n",
    "    ['Aesthetic', 'Understanding', 1, 'single-arrow'],# Appreciation for aesthetics can foster a deeper understanding of the world.\n",
    "]\n",
    "\n",
    "edges_gemma_gen = [\n",
    "    ['Positive coping', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Social Complexity', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Economic', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Political', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Religious', 1, 'single-arrow'],\n",
    "    ['Positive coping', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Positive coping', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Understanding', 1, 'single-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Social Complexity', 1, 'single-arrow']\n",
    "]\n",
    "\n",
    "\n",
    "edges_valuebench = [\n",
    "    # ['Self-Enhancement', 'Achievement', 1, 'single-arrow'],\n",
    "    # ['Industriousness', 'Achievement', 1, 'single-arrow'],\n",
    "    # ['Analytical', 'Breath of Interest', 1, 'single-arrow'],\n",
    "    # ['Openness', 'Breath of Interest', 1, 'single-arrow'],\n",
    "    # ['Agreeableness', 'Empathy', 1, 'single-arrow'],\n",
    "    # ['Emotional', 'Empathy', 1, 'single-arrow'],\n",
    "    # ['Cooperativeness', 'Empathy', 1, 'single-arrow'],\n",
    "    # ['Conscientiousness', 'Organization', 1, 'single-arrow'],\n",
    "    # ['Dependable', 'Organization', 1, 'single-arrow'],\n",
    "    # ['Openness to Experience', 'Understanding', 1, 'single-arrow'],\n",
    "    # ['Agreeableness', 'Understanding', 1, 'single-arrow'],\n",
    "    ##########################################\n",
    "    ['Empathy', 'Understanding', 1, 'single-arrow'],\n",
    "]\n",
    "\n",
    "edges_random = [\n",
    "    ['Positive coping', 'Economic', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Achievement', 'Uncertainty Avoidance', 1, 'single-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Aesthetic', 'Religious', 1, 'single-arrow'],\n",
    "    ['Anxiety Disorder', 'Understanding', 1, 'single-arrow'],\n",
    "    ['Breadth of Interest', 'Theoretical', 1, 'single-arrow'],\n",
    "    ['Economic', 'Political', 1, 'single-arrow'],\n",
    "    ['Organization', 'Social', 1, 'single-arrow'],\n",
    "    ['Political', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Religious', 'Empathy', 1, 'single-arrow'],\n",
    "    ['Theoretical', 'Positive coping', 1, 'single-arrow'],\n",
    "    ['Understanding', 'Social Complexity', 1, 'double-arrow'],\n",
    "    ['Social', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Social Cynicism', 'Breadth of Interest', 1, 'double-arrow'],\n",
    "    ['Positive coping', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Theoretical', 1, 'double-arrow'],\n",
    "    ['Resilience', 'Organization', 1, 'single-arrow'],\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zero_double_arrow(edges):\n",
    "    double_arrow_edges = [edge for edge in edges if edge[3] == 'double-arrow']\n",
    "    zero_arrow_edges = [edge for edge in edges if edge[3] == 'no-arrow']\n",
    "    if double_arrow_edges:\n",
    "        raise ValueError('Double arrow:', double_arrow_edges)\n",
    "    if zero_arrow_edges:\n",
    "        raise ValueError('Zero arrow:', zero_arrow_edges)\n",
    "\n",
    "def dealwith_zero_double_duplicated_arrow(edges):\n",
    "    double_arrow_edges = [edge for edge in edges if edge[3] == 'double-arrow']\n",
    "    zero_arrow_edges = [edge for edge in edges if edge[3] == 'no-arrow']\n",
    "    print('Double arrow:', double_arrow_edges)\n",
    "    print('Zero arrow:', zero_arrow_edges)\n",
    "    print('Dealwith zero and double arrow edges')\n",
    "    print('----------------------')\n",
    "    \n",
    "    new_edges = []\n",
    "    for edge in edges:\n",
    "        if edge[3] == 'double-arrow' or edge[3] == 'no-arrow':\n",
    "            if [edge[0], edge[1], edge[2], 'single-arrow'] not in new_edges:\n",
    "                new_edges.append([edge[0], edge[1], edge[2], 'single-arrow'])\n",
    "            if [edge[1], edge[0], edge[2], 'single-arrow'] not in new_edges:\n",
    "                new_edges.append([edge[1], edge[0], edge[2], 'single-arrow'])\n",
    "        else:\n",
    "            if edge not in new_edges:\n",
    "                new_edges.append(edge)\n",
    "    return new_edges\n",
    "\n",
    "def check_dag(edges):\n",
    "    nxg = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        if edge[3] == 'single-arrow':\n",
    "            nxg.add_edge(edge[0], edge[1])\n",
    "    if not nx.is_directed_acyclic_graph(nxg):\n",
    "        cycles = list(nx.simple_cycles(nxg))\n",
    "        raise ValueError('Cycle:', cycles)\n",
    "\n",
    "##################################################\n",
    "\n",
    "def compute_reachability(edges, nodes):\n",
    "    # æå»ºé»æ¥ç©éµ\n",
    "    n = len(nodes)\n",
    "    adj = [[False] * n for _ in range(n)]\n",
    "    for edge in edges:\n",
    "        u = nodes.index(edge[0])\n",
    "        v = nodes.index(edge[1])\n",
    "        adj[u][v] = True\n",
    "    \n",
    "    # è®¡ç®ä¼ éé­å\n",
    "    reach = [row[:] for row in adj]\n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                reach[i][j] = reach[i][j] or (reach[i][k] and reach[k][j])\n",
    "    \n",
    "    return reach\n",
    "\n",
    "#å¾ç¬¨çæ¹æ³ï¼ä¸æ­çå¾ªç¯ç´è³ä¸è½ååå°\n",
    "def transitive_reduction(edges, nodes):\n",
    "    # è®¡ç®åå§ä¼ éé­å\n",
    "    original_reach = compute_reachability(edges, nodes)\n",
    "    \n",
    "    # å¤å¶åå§è¾¹åè¡¨\n",
    "    reduced_edges = edges.copy()\n",
    "    \n",
    "    # æ å¿ä½ï¼è¡¨ç¤ºä¼ éé­åæ¯å¦åçåå\n",
    "    reach_changed = True\n",
    "    \n",
    "    # å¾ªç¯éåææè¾¹ï¼ç´å°ä¼ éé­åä¸åååï¼è¿ä¸ªå¯ä»¥ä¿è¯æ¾å°æå°çä¼ éé­å\n",
    "    while reach_changed:\n",
    "        reach_changed = False  # åè®¾ä¼ éé­åä¸ä¼åå\n",
    "        for edge in edges:\n",
    "            if edge not in reduced_edges:\n",
    "                continue  # å¦æè¾¹å·²ç»è¢«å é¤ï¼è·³è¿\n",
    "            \n",
    "            u = nodes.index(edge[0])\n",
    "            v = nodes.index(edge[1])\n",
    "            \n",
    "            # å°è¯å é¤è¾¹ (u, v)\n",
    "            reduced_edges.remove(edge)\n",
    "            \n",
    "            # è®¡ç®å é¤åçä¼ éé­å\n",
    "            new_reach = compute_reachability(reduced_edges, nodes)\n",
    "            \n",
    "            # æ£æ¥ä¼ éé­åæ¯å¦åå\n",
    "            if new_reach == original_reach:\n",
    "                # ä¼ éé­åä¸åï¼ä¿çå é¤æä½\n",
    "                reach_changed = True  # ä¼ éé­åå¯è½å¨å¶ä»è¾¹å é¤æ¶åå\n",
    "            else:\n",
    "                # ä¼ éé­åååï¼æ¢å¤è¯¥è¾¹\n",
    "                reduced_edges.append(edge)\n",
    "    \n",
    "    return reduced_edges\n",
    "\n",
    "def plot_graph(edges, title):\n",
    "    # åå»ºæåå¾\n",
    "    G = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    # ç»å¶å¾\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(G, seed=42)  # å¸å±ç®æ³\n",
    "    nx.draw(G, pos, with_labels=True, node_size=2000, node_color=\"lightblue\", font_size=10, font_weight=\"bold\", arrows=True)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "def compare_graph_reduction(edges):\n",
    "    # æåææèç¹å¹¶åºå®é¡ºåº\n",
    "    nodes = sorted(set(edge[0] for edge in edges).union(set(edge[1] for edge in edges)))\n",
    "\n",
    "    # å¯è§ååå§å¾\n",
    "    plot_graph(edges, \"Original Graph\")\n",
    "\n",
    "    # æ§è¡ä¼ éå½çº¦\n",
    "    reduced_edges = transitive_reduction(edges, nodes)\n",
    "\n",
    "    # å¯è§åå½çº¦åçå¾\n",
    "    plot_graph(reduced_edges, \"Reduced Graph\")\n",
    "\n",
    "    print(\"Reduced Edges:\")\n",
    "    for edge in reduced_edges:\n",
    "        print(edge)\n",
    "    \n",
    "    print(len(edges), len(reduced_edges))\n",
    "\n",
    "def get_reduced_edges(edges):\n",
    "    nodes = sorted(set(edge[0] for edge in edges).union(set(edge[1] for edge in edges)))\n",
    "    reduced_edges = transitive_reduction(edges, nodes)\n",
    "    return reduced_edges\n",
    "###############################################\n",
    "#T2P\n",
    "edges_gemma_total = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_gemma_total))\n",
    "edges_gemma_nosteer =  get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_gemma_nosteer))\n",
    "edges_gemma_sfs = [get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_sf)) for edges_sf in edges_gemma_sfs]\n",
    "edges_llama_total = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_llama_total))\n",
    "edges_llama_nosteer = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_llama_nosteer))\n",
    "edges_llama_sfs = [get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_sf)) for edges_sf in edges_llama_sfs]\n",
    "edges_standard_small = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_standard_small))\n",
    "edges_random = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subsequent_nodes(edges, node):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "\n",
    "    subsequent_nodes = set()\n",
    "    subsequent_nodes.add(node)\n",
    "    while True:\n",
    "        subsequent_nodes_len = len(subsequent_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[0] in subsequent_nodes:\n",
    "                subsequent_nodes.add(edge[1])\n",
    "        if len(subsequent_nodes) == subsequent_nodes_len:\n",
    "            break\n",
    "    subsequent_nodes.remove(node)\n",
    "    return subsequent_nodes\n",
    "\n",
    "def get_all_preceding_nodes(edges, node):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "\n",
    "    preceding_nodes = set()\n",
    "    preceding_nodes.add(node)\n",
    "    while True:\n",
    "        preceding_nodes_len = len(preceding_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[1] in preceding_nodes:\n",
    "                preceding_nodes.add(edge[0])\n",
    "        if len(preceding_nodes) == preceding_nodes_len:\n",
    "            break\n",
    "    preceding_nodes.remove(node)\n",
    "    return preceding_nodes\n",
    "\n",
    "def get_all_un_related_nodes(edges, node, all_nodes):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "   \n",
    "\n",
    "    #find all nodes that are in a same connected component with node\n",
    "    unrelated_nodes = set(all_nodes)\n",
    "    connected_nodes = set()\n",
    "    connected_nodes.add(node) \n",
    "    while True:\n",
    "        connected_nodes_len = len(connected_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[0] in connected_nodes:\n",
    "                connected_nodes.add(edge[1])\n",
    "            if edge[1] in connected_nodes:\n",
    "                connected_nodes.add(edge[0])\n",
    "        if len(connected_nodes) == connected_nodes_len:\n",
    "            break\n",
    "    unrelated_nodes = unrelated_nodes - connected_nodes\n",
    "\n",
    "    #T2P\n",
    "    #return unrelated_nodes, connected_nodes - set([node])\n",
    "    return set(all_nodes) - get_all_subsequent_nodes(edges, node) - set([node]) - get_all_preceding_nodes(edges, node), connected_nodes - set([node])\n",
    "\n",
    "def get_all_non_subsequent_nodes(edges, node, all_nodes):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "    return set(all_nodes) - get_all_subsequent_nodes(edges, node) - set([node])\n",
    "\n",
    "\n",
    "\n",
    "# print(get_all_subsequent_nodes(edges_standard_small, 'Understanding'))\n",
    "# print(get_all_preceding_nodes(edges_standard_small, 'Understanding'))\n",
    "# v_inference = [v for v in data_csv_gemma_train.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "# print(get_all_un_related_nodes(edges_standard_small, 'Understanding', v_inference))\n",
    "\n",
    "print(get_all_subsequent_nodes(edges_gemma_total, 'Achievement'))\n",
    "print(get_all_preceding_nodes(edges_gemma_total, 'Achievement'))\n",
    "v_inference = [v for v in data_csv_gemma_train.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "print(get_all_un_related_nodes(edges_gemma_total, 'Achievement', v_inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_table2(data_scorechange, edges, edges_source, pd_result_table2, stat_method, expected, unexpected):\n",
    "    for column in data_scorechange.columns:\n",
    "        #print('=============')\n",
    "        #print(column)\n",
    "                \n",
    "        subsequent_columns_ideal = get_all_subsequent_nodes(edges, column)\n",
    "        unrelated_columns_ideal, related_columns_ideal = get_all_un_related_nodes(edges, column, set(data_scorechange.columns))\n",
    "        non_subsequent_columns_ideal = get_all_non_subsequent_nodes(edges, column, set(data_scorechange.columns))\n",
    "\n",
    "        if expected == 'subsequent':\n",
    "            expected_columns_ideal = subsequent_columns_ideal\n",
    "        elif expected == 'related':\n",
    "            expected_columns_ideal = related_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid expected')\n",
    "        \n",
    "        if unexpected == 'unrelated':\n",
    "            unexpected_columns_ideal = unrelated_columns_ideal\n",
    "        elif unexpected == 'non_subsequent':\n",
    "            unexpected_columns_ideal = non_subsequent_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid unexpected')\n",
    "\n",
    "        if stat_method == 'change_score':\n",
    "            related_columns_real = data_scorechange[data_scorechange[column] != 0].abs().mean().sort_values()\n",
    "            #related_columns_real = data_scorechange[data_scorechange[column].abs() > 0.1].abs().mean().sort_values()\n",
    "            #related_columns_real = data_scorechange.abs().mean().sort_values()\n",
    "        elif stat_method == 'change_count':\n",
    "            related_columns_real = data_scorechange[data_scorechange[column] != 0]\n",
    "            related_columns_real = related_columns_real.astype(bool).mean(axis=0)#.sort_values()\n",
    "        else:\n",
    "            raise ValueError('Invalid stat_method')\n",
    "\n",
    "        def classify(vd):\n",
    "            if vd in expected_columns_ideal:\n",
    "                return 'expected'\n",
    "            elif vd in unexpected_columns_ideal:\n",
    "                return 'unexpected'\n",
    "            else:\n",
    "                return 'none'\n",
    "\n",
    "        expected_scabs = []\n",
    "        unexpected_scabs = []\n",
    "        for related_column in related_columns_real.index:\n",
    "            if related_column in expected_columns_ideal:\n",
    "                expected_scabs.append(related_columns_real[related_column])\n",
    "            elif related_column in unexpected_columns_ideal:\n",
    "                unexpected_scabs.append(related_columns_real[related_column])\n",
    "            else:\n",
    "                pass\n",
    "                #assert related_column == column\n",
    "            #print(related_column, related_columns_real[related_column], classify(related_column))\n",
    "        #print('~~~')\n",
    "        \n",
    "        #print('Expected:', np.mean([vdsc for vdsc in expected_scabs if not np.isnan(vdsc)]), len(expected_scabs))\n",
    "        #print('Unexpected:', np.mean([vdsc for vdsc in unexpected_scabs if not np.isnan(vdsc)]), len(unexpected_scabs))\n",
    "        #print('Expected:', np.mean(expected_scabs), len(expected_scabs))\n",
    "        #print('Unexpected:', np.mean(unexpected_scabs), len(unexpected_scabs))\n",
    "\n",
    "        mean_scorechange_related = 'mean_scorechange_' + edges_source + '_expected'\n",
    "        num_related = 'num_' + edges_source + '_expected'\n",
    "        mean_scorechange_unrelated = 'mean_scorechange_' + edges_source + '_unexpected'\n",
    "        num_unrelated = 'num_' + edges_source + '_unexpected'\n",
    "\n",
    "        pd_result_table2.loc[mean_scorechange_related, column] = np.mean(expected_scabs)\n",
    "        #pd_result_table2.loc[num_related, column] = len(expected_scabs)\n",
    "        pd_result_table2.loc[mean_scorechange_unrelated, column] = np.mean(unexpected_scabs)\n",
    "        #pd_result_table2.loc[num_unrelated, column] = len(unexpected_scabs)\n",
    "        #print('----------------------')\n",
    "\n",
    "def create_table2(data_csv, fixed_role, fixed_sf, change_base, modelname, edges_trained, edges_ref, stat_method, expected, unexpected, pd_result_table2):\n",
    "    assert modelname in ['gemma', 'llama']\n",
    "    data_source = modelname\n",
    "\n",
    "    roles = data_csv['player_name'].unique()\n",
    "    sfs = data_csv['steer_dim'].unique()\n",
    "    if not fixed_role:\n",
    "        assert fixed_sf in sfs or fixed_sf == 'plain'\n",
    "    else:\n",
    "        assert fixed_role in roles or fixed_role == 'plain'\n",
    "        assert not fixed_sf\n",
    "    \n",
    "    assert change_base in ['zero', 'first']  \n",
    "    assert stat_method in ['change_score', 'change_count']\n",
    "    assert expected in ['subsequent', 'related']\n",
    "    assert unexpected in ['unrelated', 'non_subsequent']\n",
    "\n",
    "    if fixed_sf:\n",
    "        if fixed_sf == 'plain':\n",
    "            data_nosteer = data_csv[data_csv['steer_dim'].isnull()]\n",
    "        else:\n",
    "            data_nosteer = data_csv[data_csv['steer_dim'] == fixed_sf]\n",
    "\n",
    "        if change_base == 'first':\n",
    "            data_nosteer = data_nosteer[data_nosteer['player_name'].notnull()] \n",
    "        else:\n",
    "            assert np.isnan(data_nosteer.iloc[0]['player_name'])\n",
    "\n",
    "        data_nosteer = data_nosteer[v_inference + ['player_name']]\n",
    "        data_nosteer = data_nosteer.set_index('player_name')\n",
    "\n",
    "    if fixed_role:\n",
    "        if fixed_role == 'plain':\n",
    "            data_nosteer = data_csv[data_csv['player_name'].isnull()]\n",
    "        else:\n",
    "            data_nosteer = data_csv[data_csv['player_name'] == fixed_role]\n",
    "        \n",
    "        if change_base == 'first':\n",
    "            data_nosteer = data_nosteer[data_nosteer['steer_dim'].notnull()]\n",
    "        else:\n",
    "            assert np.isnan(data_nosteer.iloc[0]['steer_dim'])\n",
    "\n",
    "        data_nosteer = data_nosteer[v_inference + ['steer_dim']]\n",
    "        data_nosteer = data_nosteer.set_index('steer_dim')\n",
    "    \n",
    "    data_nosteer = data_nosteer.astype(float)\n",
    "\n",
    "    data_scorechange = data_nosteer - data_nosteer.iloc[0]\n",
    "    assert v_inference == data_scorechange.columns.tolist()\n",
    "\n",
    "    #count the non-zero values per row and calculate the mean\n",
    "    print(\"AVERAGE CHANGED VALUE: \", data_scorechange.abs().astype(bool).sum(axis=0).mean())\n",
    "\n",
    "    \n",
    "    steer_func = '_prompt' if fixed_sf else '_sae'\n",
    "    write_table2(data_scorechange, edges_trained, steer_func + '_ours' , pd_result_table2, stat_method, expected, unexpected)\n",
    "    write_table2(data_scorechange, edges_ref, steer_func + '_refs', pd_result_table2, stat_method, expected, unexpected)\n",
    "\n",
    "\n",
    "v_inference_gemma = [v for v in data_csv_gemma_test.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "v_inference_llama = [v for v in data_csv_llama_test.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "assert v_inference_gemma == v_inference_llama\n",
    "v_inference = v_inference_gemma\n",
    "pd_result_table2_gemma = pd.DataFrame(columns=v_inference)\n",
    "pd_result_table2_llama = pd.DataFrame(columns=v_inference)\n",
    "\n",
    "#T2P \n",
    "# create_table2(data_csv_gemma_train, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='gemma', edges_trained=edges_gemma_nosteer, edges_ref=edges_standard, stat_method='change_score', expected = 'subsequent', unexpected = 'non_subsequent',pd_result_table2=pd_result_table2)\n",
    "# create_table2(data_csv_llama_train, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='llama', edges_trained=edges_llama_nosteer, edges_ref=edges_standard, stat_method='change_score', expected = 'subsequent', unexpected = 'non_subsequent',pd_result_table2=pd_result_table2)\n",
    "\n",
    "unexpected = 'non_subsequent'\n",
    "edges_ref = edges_standard_small\n",
    "#edges_ref = edges_valuebench\n",
    "\n",
    "\n",
    "create_table2(data_csv_gemma_test, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='gemma', edges_trained=edges_gemma_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_gemma)\n",
    "create_table2(data_csv_llama_test, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='llama', edges_trained=edges_llama_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_llama)\n",
    "\n",
    "for test_name in data_csv_gemma_test['player_name'].unique():\n",
    "    try: \n",
    "        np.isnan(test_name)\n",
    "        continue\n",
    "    except:\n",
    "        if test_name in data_csv_llama_test['player_name'].unique():\n",
    "            break\n",
    "# test_name_list = set.intersection(set(data_csv_gemma_test['player_name']), set(data_csv_llama_test['player_name']))\n",
    "# test_name = random.choice(list(test_name_list))\n",
    " \n",
    "test_name = 'Mary Holt'\n",
    "print(test_name)\n",
    "\n",
    "create_table2(data_csv_gemma_test, fixed_role=test_name, fixed_sf=None, change_base=\"zero\", modelname='gemma', edges_trained=edges_gemma_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_gemma)\n",
    "create_table2(data_csv_llama_test, fixed_role=test_name, fixed_sf=None, change_base=\"zero\", modelname='llama', edges_trained=edges_llama_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_llama)\n",
    "\n",
    "pd_result_table2_gemma.to_csv('table2_gemma.csv')\n",
    "pd_result_table2_llama.to_csv('table2_llama.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the amount of questions per value\n",
    "from sae_lens.config import LOCAL_SAE_MODEL_PATH\n",
    "#df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation_train.csv'))\n",
    "df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation_test.csv'))\n",
    "grouped = df_valuebench.groupby('value')\n",
    "value_counts = grouped.size().reset_index(name='counts')\n",
    "#get the value dimensions in v_inference\n",
    "value_counts = value_counts[value_counts['value'].isin(v_inference)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_table2(group_name):\n",
    "    index_group = [\n",
    "        'mean_scorechange__prompt_ours_expected',\n",
    "        'mean_scorechange__prompt_ours_unexpected',\n",
    "        'mean_scorechange__prompt_refs_expected',\n",
    "        'mean_scorechange__prompt_refs_unexpected',\n",
    "        'mean_scorechange__sae_ours_expected',\n",
    "        'mean_scorechange__sae_ours_unexpected',\n",
    "        'mean_scorechange__sae_refs_expected',\n",
    "        'mean_scorechange__sae_refs_unexpected',\n",
    "        ]\n",
    "\n",
    "    #print the table2 in latex\n",
    "    #rows are for each values dimensions\n",
    "    #columns are in form num_related_ours(mean_scorechange_related_ours), num_unrelated_ours(mean_scorechange_unrelated_ours), num_related_standard(mean_scorechange_related_standard), num_unrelated_standard(mean_scorechange_unrelated_standard)\n",
    "    #the values are the number of related values, the mean of the score change of related values, the number of unrelated values, the mean of the score change of unrelated values\n",
    "    #the values are rounded to 3 decimal places\n",
    "    #the values are in the form number(mean)\n",
    "    #the values are in the form of number(mean)\n",
    "    pd_result_table2 = pd.read_csv('table2_' + group_name + '.csv', index_col=0)\n",
    "    latex_code = '\\\\begin{table*}[ht]\\n\\\\caption{The mean of the score change of related values, the number of related values, the mean of the score change of unrelated values, and the number of unrelated values.}\\n\\\\label{table: scorechange}\\n\\\\begin{center}\\n'\n",
    "    #latex_code += '\\\\begin{tabular}{c@{\\\\hspace{2pt}}' + 'c@{\\\\hspace{2pt}}' * (len(pd_result_table2.columns) - 1) + 'c' + '}\\n\\\\toprule\\n'\n",
    "    latex_code += '\\\\begin{tabular}{c@{\\\\hspace{2pt}}|' + 'c@{\\\\hspace{2pt}}' * 4 +'|' + 'c@{\\\\hspace{2pt}}' * 4 + '}\\n\\\\toprule\\n'\n",
    "    latex_code += 'Value & \\\\multicolumn{4}{c|}{\\\\bf \\\\small Prompt} & \\\\multicolumn{4}{c}{\\\\bf \\\\small SAE}\\\\\\\\\\n\\\\hline\\n'\n",
    "    latex_code += 'Dimensions & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Our causal graph} & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Random causal graph} & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Our causal graph} & \\\\multicolumn{2}{c}{\\\\bf \\\\tiny Random causal graph}  \\\\\\\\\\n\\\\hline\\n'\n",
    "    latex_code += 'Score change & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Unexpected}\\\\\\\\\\n\\\\hline\\n'\n",
    "    #each row in latex is a column in the dataframe\n",
    "\n",
    "    index_dict = {index: [] for index in index_group}\n",
    "\n",
    "    for column in pd_result_table2.columns:\n",
    "        latex_code += '\\\\small ' + column + ' & '\n",
    "        #for index in pd_result_table2.index:\n",
    "        for index in index_group:\n",
    "            if index.startswith('mean'):\n",
    "                latex_code += str(round(pd_result_table2.loc[index, column], 2)) + ' & '\n",
    "                index_dict[index].append(pd_result_table2.loc[index, column])\n",
    "\n",
    "        latex_code = latex_code[:-2] + ' \\\\\\\\\\n'\n",
    "    latex_code += '\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\end{table*}'\n",
    "    #print(latex_code)\n",
    "    #write the latex code to a file\n",
    "    with open('table2' + group_name + '.tex', 'w') as f:\n",
    "        f.write(latex_code)\n",
    "    return index_dict\n",
    "\n",
    "index_dict_gemma = get_latex_table2('gemma')\n",
    "index_dict_llama = get_latex_table2('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "\n",
    "def make_chart(index_dict, modelname):\n",
    "    data1 = [\n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_ours_expected']), \n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_refs_expected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_ours_expected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_refs_expected'])\n",
    "        ]\n",
    "    data2 = [\n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_ours_unexpected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_refs_unexpected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_ours_unexpected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_refs_unexpected'])\n",
    "        ]\n",
    "    #Also keep the standard deviation of each group\n",
    "    data1_std = [\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_ours_expected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_refs_expected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_ours_expected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_refs_expected'])\n",
    "        ]\n",
    "    data2_std = [\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_ours_unexpected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_refs_unexpected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_ours_unexpected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_refs_unexpected'])\n",
    "    ]\n",
    "    print(data1_std, data2_std)\n",
    "\n",
    "    labels_ = ['Our Graph','Reference Graph','Our Graph','Reference Graph']\n",
    "    \n",
    "    #ç¨æ¥ä¸ºåæ çå¸¸è§åæ  è¿æ¯ å¯¹æ°åæ ååå¤\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.8))#ç¨æ¥æ§å¶å¾ççå¤§å°\n",
    "    #fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "    # è®¾ç½®æ±ç¶å¾åæ°\n",
    "    width = 0.35 #æ±ç¶å¾æ¯ä¸ªæ±å­çå®½åº¦,åæ¶ä¹æ¯è°æ´æ¯ç»æ±å­ä¹é´çé´é\n",
    "    x = np.arange(len(labels_)) #ç¨æ¥æå®æ¯ä¸ªæ±å­ä½ç½®åæ°\n",
    "    \n",
    "    # ç»å¶æ±ç¶å¾,æ­£å¸¸åæ ,å«å¿äºerrorbar\n",
    "    ax.errorbar(x-width/2, (data1), yerr=data1_std, fmt='o', color=(115/255, 186/255, 214/255), capsize=5)\n",
    "    ax.errorbar(x+width/2, (data2), yerr=data2_std, fmt='o', color=(38/255, 70/255, 83/255), capsize=5)\n",
    "\n",
    "\n",
    "    ax.bar(x-width/2, (data1), width=width, label='Expected',  color=(115/255, 186/255, 214/255)) #edgecolor='#2596be', facecolor='none', hatch='//',\n",
    "    ax.bar(x+width/2, (data2), width=width, label='Unexpected', color=(38/255, 70/255, 83/255)) #hatch='xx', \n",
    "    #change the color of the bars\n",
    "\n",
    "    # ç»æ¯ä¸ªæ±æ¡æ·»å æ°å¼æ ç­¾\n",
    "    for a, b in zip(x-width/2, data1):\n",
    "        plt.text(a, b+0.01, '%.2f' % b, ha='center', va='bottom', fontsize=12)\n",
    "    for a, b in zip(x+width/2, data2):\n",
    "        plt.text(a, b+0.01, '%.2f' % b, ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    # è®¾ç½®yè½´èå´, \n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "\n",
    "    # è®¾ç½®å¾è¡¨æ é¢åè½´æ ç­¾\n",
    "    plt.title(f'{modelname}')\n",
    "    #plt.xlabel('Order')\n",
    "    plt.ylabel('Frequency of changes in other value dimensions')\n",
    "    \n",
    "    #plt.xticks(x, fontsize=12, rotation=45,loc='inside')#è®¾ç½®æ ç­¾çæå­å¤§å°åæè½¬æ¹å\n",
    "    plt.xticks(x, labels_)  #ä½¿å¾æ ç­¾ç°å®çæ¯ç»å®çæå­æ ç­¾\n",
    "\n",
    "\n",
    "    p1 = patches.Rectangle((.515, 0), width=.39, height=.10, alpha=.1, facecolor='yellow', transform=fig.transFigure)\n",
    "    p2 = patches.Rectangle((.125, 0), width=.39, height=.10, alpha=.1, facecolor='red', transform=fig.transFigure)\n",
    "    #can we add a label for each rectangle?\n",
    "\n",
    "    fig.add_artist(p1)\n",
    "    fig.add_artist(p2)\n",
    "\n",
    "    fig.text(0.125 + 0.39/2, .03, 'Prompt', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            fontsize=12, \n",
    "            color='black')\n",
    "    fig.text(0.515 + 0.39/2, .03, 'Sae', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            fontsize=12, \n",
    "            color='black')\n",
    "\n",
    "    # è®¾ç½®å¾ä¾\n",
    "    plt.legend(loc='upper right', ncol=1)#è®¾ç½®å¾ä¾çä½ç½®ååæ°\n",
    "    \n",
    "    #è·åé»è®¤å¾çå°ºå¯¸\n",
    "    figure = plt.gcf()\n",
    "    width = figure.bbox.width\n",
    "    height = figure.bbox.height\n",
    "    print(width,height)\n",
    "    \n",
    "    #plt.tight_layout(pad=10)\n",
    "\n",
    "    # æ¾ç¤ºå¾è¡¨\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "make_chart(index_dict_gemma, 'Gemma-2B-IT')\n",
    "make_chart(index_dict_llama, 'Llama3-8B-IT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steer_dims = ['nan', 1312, 1341, 2221, 3183, 6619, 7502, 8387, 10096, 14049]\n",
    "\n",
    "nodes = {}\n",
    "for entity in v_inference:\n",
    "    nodes[entity] = os.path.join('valuebench','value_questions_' + entity + '.html'),\n",
    "# for feature in data_csv.['steer_dim'].unique()[1:]:\n",
    "#     nodes[feature] = 'https://www.neuronpedia.org/' + sae.cfg.model_name +'/' + str(sae.cfg.hook_layer) + '-res-jb/' + str(feature)\n",
    "\n",
    "print(len(edges_gemma_total), len(edges_llama_total), len(edges_ref))\n",
    "edges = {\n",
    "    'gemma': edges_gemma_total,\n",
    "    'llama': edges_llama_total,\n",
    "    'reference': edges_standard_small,\n",
    "    'gemma_gen': edges_gemma_gen,\n",
    "    'llama_gen': edges_llama_gen,\n",
    "    'valuebench': edges_valuebench,\n",
    "}\n",
    "\n",
    "json_object = {\n",
    "    'nodes': nodes,\n",
    "    'edges': edges\n",
    "    }\n",
    "\n",
    "json.dump(json_object, open('value_graph_data1.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We denote the set of subsequent nodes of $v$ in graph $G$ as $V_{suc}^G(v)$, the set of roles that $S_r[v]\\neq S_{r_0}[v]$ as $R_{diff}(v)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
