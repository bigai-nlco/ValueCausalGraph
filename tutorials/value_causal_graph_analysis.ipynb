{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNk7IylTv610"
   },
   "source": [
    "# Loading and Analysing Pre-Trained Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_DusoOvwV0M"
   },
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aGgWkbav610"
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yfDUxRx0wSRl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
    "from causallearn.utils.cit import kci\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
    "from causallearn.utils.cit import fisherz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_color = '#42A5F5'\n",
    "yellow_color = '#FFCA28'\n",
    "\n",
    "\n",
    "#T2P\n",
    "answer_valuebench_features_csv_gemma_train = os.path.join('useful_data',\"ans_gemma_train_formal.csv\")\n",
    "answer_valuebench_features_csv_gemma_test = os.path.join('useful_data',\"ans_gemma_test_formal.csv\")\n",
    "answer_valuebench_features_csv_llama_train = os.path.join('useful_data',\"ans_llama_train_formal.csv\")\n",
    "answer_valuebench_features_csv_llama_test = os.path.join('useful_data',\"ans_llama_test_formal.csv\")\n",
    "# answer_valuebench_features_csv_gemma_train = os.path.join('useful_data',\"ans_cross_part1.csv\")\n",
    "# answer_valuebench_features_csv_gemma_test = os.path.join('useful_data',\"ans_cross_part2.csv\")\n",
    "# answer_valuebench_features_csv_llama_train = os.path.join('useful_data',\"ans_llama_old_train.csv\")\n",
    "# answer_valuebench_features_csv_llama_test = os.path.join('useful_data',\"ans_llama_old_test.csv\")\n",
    "\n",
    "data_csv_gemma_train = pd.read_csv(answer_valuebench_features_csv_gemma_train)\n",
    "data_csv_gemma_test = pd.read_csv(answer_valuebench_features_csv_gemma_test)\n",
    "data_csv_llama_train = pd.read_csv(answer_valuebench_features_csv_llama_train)\n",
    "data_csv_llama_test = pd.read_csv(answer_valuebench_features_csv_llama_test)\n",
    "\n",
    "#T2P\n",
    "\n",
    "exclude_columns_gemma = ['Anxiety Disorder', 'Achievement', 'Empathy', 'Organization', 'Political', 'Economic', 'Social Complexity']\n",
    "#exclude_rows_gemma = [14351, 12703, 10454, 8387, 6884, 6126, 6188, 2221,1025, 428,]\n",
    "gemma_features = [14351, 12703, 10454, 8387, 6884, 6126, 6188, 2221,1025, 428, 1312,1341,1975,2965,3183,3402,4752,6216,6619,7502,10096,10605,11712,14049,14185]\n",
    "good_gemma_features = [14351, 1025, 2965,10096, 1341, 1975 , 10605, 14049, 1312,4752 ]# ,3402, \n",
    "# exclude_columns_gemma = []\n",
    "# good_gemma_features = gemma_features\n",
    "exclude_rows_gemma = [i for i in gemma_features if i not in good_gemma_features]\n",
    "\n",
    "\n",
    "\n",
    "exclude_columns_llama = ['Anxiety Disorder', 'Achievement', 'Empathy', 'Organization', 'Political', 'Economic', 'Social Complexity']\n",
    "#exclude_rows_llama = [60312, 7754, 13033, 1897, 2509, 20141, 41929, 48321, 63905, 49202, 2246, 58305,]\n",
    "llama_features = [63905, 48321, 1897, 2509, 60312, 7754, 49202, 2246, 13033, 58305, 41929, 20141, 12477, 54606, 21347, 4305, 30919, 34598, 51010, 62769, 47207, 8035, 8546, 9332]\n",
    "good_llama_features = [49202, 58305, 8546, 12477, 7754, 9332, 1897,  62769, 47207, 12477, 54606] #best 49202, 58305, 8546 #okay 13033, 12477, 21347, 4305, 62769, 47207, 2509 \n",
    "#20141,  54606\n",
    "# exclude_columns_llama = []\n",
    "# good_llama_features = llama_features\n",
    "exclude_rows_llama = [i for i in llama_features if i not in good_llama_features]\n",
    "\n",
    "data_csv_gemma_train = data_csv_gemma_train[~data_csv_gemma_train['steer_dim'].isin(exclude_rows_gemma)]\n",
    "data_csv_gemma_test = data_csv_gemma_test[~data_csv_gemma_test['steer_dim'].isin(exclude_rows_gemma)]\n",
    "data_csv_llama_train = data_csv_llama_train[~data_csv_llama_train['steer_dim'].isin(exclude_rows_llama)]\n",
    "data_csv_llama_test = data_csv_llama_test[~data_csv_llama_test['steer_dim'].isin(exclude_rows_llama)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_new_diff(data_csv_raw, modelname, threshold_judge):\n",
    "    assert threshold_judge >= 0\n",
    "    pathname = 'value_dims_rsd_' + modelname\n",
    "    stat_csv_23 = pathname + '/23_stat.csv'\n",
    "    data_new_diff_count_total = pd.DataFrame()\n",
    "\n",
    "    if os.path.exists('pathname'):\n",
    "        shutil.rmtree('pathname')\n",
    "    os.makedirs(pathname, exist_ok=True)\n",
    "    \n",
    "    for column in data_csv_raw.columns:\n",
    "        if column == 'player_name' or column == 'steer_dim' or column == 'stds' or column =='scstds' or column.endswith(':scstd'):\n",
    "            continue\n",
    "        value_csv = pathname + '/' + column + '.csv'\n",
    "        data_new = data_csv_raw.pivot(index='steer_dim', columns='player_name', values=column)\n",
    "        data_new_scstd = data_csv_raw.pivot(index='steer_dim', columns='player_name', values=column+':scstd')\n",
    "        data_save = data_new.astype(str) + '±' + data_new_scstd.astype(str) #problems here: the scstd is not the std for the score, but fore the changed score\n",
    "        data_save.to_csv(value_csv)\n",
    "\n",
    "        data_new_diff = data_new - data_new[data_new.index.isnull()].iloc[0]\n",
    "\n",
    "        data_new_diff_count_higher = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if y > threshold_judge else 0))\n",
    "        data_new_diff_count_higher = data_new_diff_count_higher.sum(axis=1)\n",
    "        data_new_diff_count_lower = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if y < -threshold_judge else 0))\n",
    "        data_new_diff_count_lower = data_new_diff_count_lower.sum(axis=1)\n",
    "        data_new_diff_count_equal = data_new_diff.apply(lambda x: x.apply(lambda y: 1 if abs(y) <= threshold_judge else 0))\n",
    "        data_new_diff_count_equal = data_new_diff_count_equal.sum(axis=1)\n",
    "\n",
    "        data_new_diff_count = data_new_diff_count_higher.astype(str) + '/' + data_new_diff_count_lower.astype(str) + '/' + data_new_diff_count_equal.astype(str)\n",
    "        data_new_diff_count_total[column] = data_new_diff_count\n",
    "\n",
    "    data_new_diff_count_total.to_csv(stat_csv_23)\n",
    "\n",
    "get_data_new_diff(data_csv_gemma_train, 'gemma', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_gemma_test, 'gemmatest', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_llama_train, 'llama', threshold_judge=0)\n",
    "get_data_new_diff(data_csv_llama_test, 'llamatest', threshold_judge=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table1_newest(stat_csv_23_train, stat_csv_23_test):\n",
    "    data_new_diff_count_total_train = pd.read_csv(stat_csv_23_train)\n",
    "    data_new_diff_count_total_test = pd.read_csv(stat_csv_23_test)\n",
    "\n",
    "    table1_rows = data_new_diff_count_total_train['steer_dim'].unique()\n",
    "    table1_rows = table1_rows[~np.isnan(table1_rows)]\n",
    "    table1_columns = data_new_diff_count_total_train.columns[data_new_diff_count_total_train.columns != 'steer_dim']\n",
    "\n",
    "    table1_rows_test = data_new_diff_count_total_test['steer_dim'].unique()\n",
    "    table1_rows_test = table1_rows_test[~np.isnan(table1_rows_test)]\n",
    "    table1_columns_test = data_new_diff_count_total_test.columns[data_new_diff_count_total_test.columns != 'steer_dim']\n",
    "\n",
    "    assert np.array_equal(table1_rows, table1_rows_test)\n",
    "    assert np.array_equal(table1_columns, table1_columns_test)\n",
    "    \n",
    "    table1 = pd.DataFrame(columns=table1_columns, index=table1_rows)\n",
    "    \n",
    "    for steer_dim in table1_rows:\n",
    "        assert not np.isnan(steer_dim)\n",
    "\n",
    "        steer_dim_row_train = data_new_diff_count_total_train[data_new_diff_count_total_train['steer_dim'] == steer_dim]\n",
    "        steer_dim_row_test = data_new_diff_count_total_test[data_new_diff_count_total_test['steer_dim'] == steer_dim]\n",
    "\n",
    "        for column in table1_columns:\n",
    "            assert column != 'steer_dim'\n",
    "            #split cell by /\n",
    "            counts_train = steer_dim_row_train[column].values[0].split('/')   \n",
    "            simu_train = int(counts_train[0])\n",
    "            supp_train = int(counts_train[1])\n",
    "            main_train = int(counts_train[2])\n",
    "            \n",
    "            counts_test = steer_dim_row_test[column].values[0].split('/')\n",
    "            simu_test = int(counts_test[0])\n",
    "            supp_test = int(counts_test[1])\n",
    "            main_test = int(counts_test[2])\n",
    "\n",
    "            table1.loc[steer_dim, column] = str(simu_train) + '/' + str(supp_train) + '/' + str(main_train) + '/' + str(simu_test) + '/' + str(supp_test) + '/' + str(main_test)\n",
    "    return table1\n",
    "\n",
    "table1_gemma = get_table1_newest('value_dims_rsd_gemma/23_stat.csv', 'value_dims_rsd_gemmatest/23_stat.csv')\n",
    "table1_gemma.to_csv('table1_gemma_newest.csv')\n",
    "table1_llama = get_table1_newest('value_dims_rsd_llama/23_stat.csv', 'value_dims_rsd_llamatest/23_stat.csv')\n",
    "table1_llama.to_csv('table1_llama_newest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_valuebench_features_csv_gemma_compare_positve = os.path.join('useful_data',\"ans_gemma_test_compare_positive_s1.csv\")\n",
    "answer_valuebench_features_csv_gemma_compare_negative = os.path.join('useful_data',\"ans_gemma_test_compare_negative_s1.csv\")\n",
    "answer_valuebench_features_csv_llama_compare_positve = os.path.join('useful_data',\"ans_llama_test_compare_positive.csv\")\n",
    "answer_valuebench_features_csv_llama_compare_negative = os.path.join('useful_data',\"ans_llama_test_compare_negative.csv\")\n",
    "data_csv_gemma_compare_positive = pd.read_csv(answer_valuebench_features_csv_gemma_compare_positve)\n",
    "data_csv_gemma_compare_negative = pd.read_csv(answer_valuebench_features_csv_gemma_compare_negative)\n",
    "data_csv_llama_compare_positive = pd.read_csv(answer_valuebench_features_csv_llama_compare_positve)\n",
    "data_csv_llama_compare_negative = pd.read_csv(answer_valuebench_features_csv_llama_compare_negative)\n",
    "\n",
    "# for each value, check the status of all roles in data_csv_gemma_test without sae steer\n",
    "value_steers_gemma_negative = {'Aesthetic': 10605, 'Resilience':4752, 'Social Cynicism': 4752, 'Positive coping':10096, 'Religious':1341, 'Breadth of Interest': 14049, 'Understanding':1975,'Theoretical':1975, 'Uncertainty Avoidance':4752,  'Social':1975, } #'\n",
    "value_steers_gemma_positive = {'Positive coping':1975, 'Theoretical': 10096, 'Uncertainty Avoidance': 1312, 'Aesthetic': 1312, 'Breadth of Interest':10096, 'Religious':1312, 'Social':14049, 'Understanding':4752,  'Resilience': 1341, 'Social Cynicism': 14049}\n",
    "value_steers_llama_negative = {'Aesthetic': 58305, 'Resilience':7754, 'Social Cynicism': 47207, 'Positive coping':7754, 'Religious':47207, 'Breadth of Interest': 7754, 'Understanding':7754,'Theoretical':7754, 'Uncertainty Avoidance':12477,  'Social':7754, } #'\n",
    "value_steers_llama_positive = {'Positive coping':9332, 'Theoretical': 9332, 'Uncertainty Avoidance': 7754, 'Aesthetic': 49202, 'Breadth of Interest': 58305, 'Religious':9332, 'Social': 54606, 'Understanding':47207,  'Resilience': 9332, 'Social Cynicism': 7754}#Uncertainty Avoidance':62769\n",
    "\n",
    "\n",
    "#check data_csv_gemma_test, pay attention to the values in value_steers_gemma_negative, check the difference between the not doing steering and doing steering using the saes in value_steers_gemma_negative\n",
    "def steer_compare(sae_dict, data_csv,  data_csv_compare):\n",
    "    from collections import Counter\n",
    "    changed_roles_sae = Counter()\n",
    "    changed_roles_compare = Counter()\n",
    "    for value_name, steer in sae_dict.items():\n",
    "        data_csv_steering_none = data_csv[np.isnan(data_csv['steer_dim'])][['player_name', value_name]]\n",
    "        data_csv_steering_bestsae = data_csv[data_csv['steer_dim'] == steer][['player_name', value_name]]\n",
    "        data_csv_steering_compare = data_csv_compare[['player_name', value_name]]\n",
    "        data_csv_steering_compare = data_csv_steering_compare[data_csv_steering_compare['player_name'].isin(data_csv_steering_none['player_name'])]\n",
    "        assert data_csv_steering_none['player_name'].tolist() == data_csv_steering_bestsae['player_name'].tolist() == data_csv_steering_compare['player_name'].tolist()\n",
    "        \n",
    "\n",
    "        \n",
    "        diff1 = data_csv_steering_bestsae[value_name].to_numpy() - data_csv_steering_none[value_name].to_numpy()\n",
    "        diff2 = data_csv_steering_compare[value_name].to_numpy() - data_csv_steering_none[value_name].to_numpy()\n",
    "        #map diff1 to -1, 0, 1\n",
    "        changed_roles_sae['positive'] += np.sum(np.sign(diff1) == 1)\n",
    "        changed_roles_sae['negative'] += np.sum(np.sign(diff1) == -1)\n",
    "        changed_roles_sae['zero'] += np.sum(np.sign(diff1) == 0)\n",
    "        changed_roles_compare['positive'] += np.sum(np.sign(diff2) == 1)\n",
    "        changed_roles_compare['negative'] += np.sum(np.sign(diff2) == -1)\n",
    "        changed_roles_compare['zero'] += np.sum(np.sign(diff2) == 0)\n",
    "        print(value_name, np.sum(np.sign(diff1)))\n",
    "        print(value_name, np.sum(np.sign(diff2)))\n",
    "        print('++++++++++')\n",
    "        # changed_roles_sae.append(sign * np.sum(np.sign(diff1)))\n",
    "        # changed_roles_compare.append(sign * np.sum(np.sign(diff2)))\n",
    "        \n",
    "    return changed_roles_sae, changed_roles_compare\n",
    "change_roles_sae_gemma_postive, changed_roles_compare_gemma_positive = steer_compare(value_steers_gemma_positive, data_csv_gemma_test, data_csv_gemma_compare_positive)\n",
    "change_roles_sae_gemma_negative, changed_roles_compare_gemma_negative = steer_compare(value_steers_gemma_negative, data_csv_gemma_test, data_csv_gemma_compare_negative)\n",
    "change_roles_sae_llama_postive, changed_roles_compare_llama_positive = steer_compare(value_steers_llama_positive, data_csv_llama_test, data_csv_llama_compare_positive)\n",
    "change_roles_sae_llama_negative, changed_roles_compare_llama_negative = steer_compare(value_steers_llama_negative, data_csv_llama_test, data_csv_llama_compare_negative)\n",
    "\n",
    "#draw a 1*2 table of pie charts, each pie chart shows the percentage of positive, negative, and zero changes in the roles\n",
    "#the columns are the SAE and the compare\n",
    "#begin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def draw_pie_chart(count_1, count_0, count_neg1, steer_method):\n",
    "    labels = ['Positively Changed Roles', 'Unchanged Roles', 'Negatively Changed Roles']\n",
    "    sizes = [count_1, count_0, count_neg1]\n",
    "    colors = [blue_color, '#ffffff', yellow_color]\n",
    "    \n",
    "    \n",
    "    explode = (0, 0, 0)  # explode the 1st slice (Positive)\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    wedges, texts, autotexts = plt.pie(sizes, explode=explode, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140, labeldistance=0.85, textprops={'fontsize': 30})\n",
    "    #plt.legend(wedges, labels, title=\"Role Changes\", loc=\"center\", bbox_to_anchor=(0.5, 0, 0.5, 1))\n",
    "\n",
    "    #plt.title('Value Changing Direction Results for ' + steer_method)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "    # centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "    # fig = plt.gcf()\n",
    "    # fig.gca().add_artist(centre_circle)\n",
    "\n",
    "draw_pie_chart(change_roles_sae_gemma_postive['positive'], change_roles_sae_gemma_postive['zero'], change_roles_sae_gemma_postive['negative'], 'Positive SAE Steering on Gemma-2B-IT')\n",
    "draw_pie_chart(changed_roles_compare_gemma_positive['positive'], changed_roles_compare_gemma_positive['zero'], changed_roles_compare_gemma_positive['negative'], 'Positive Hard Prompt on Gemma-2B-IT')\n",
    "draw_pie_chart(change_roles_sae_gemma_negative['positive'], change_roles_sae_gemma_negative['zero'], change_roles_sae_gemma_negative['negative'], 'Negative SAE Steering on Gemma-2B-IT')\n",
    "draw_pie_chart(changed_roles_compare_gemma_negative['positive'], changed_roles_compare_gemma_negative['zero'], changed_roles_compare_gemma_negative['negative'], 'Negative Hard Prompt on Gemma-2B-IT')\n",
    "draw_pie_chart(change_roles_sae_llama_postive['positive'], change_roles_sae_llama_postive['zero'], change_roles_sae_llama_postive['negative'], 'Positive SAE Steering on LLAMA3-8B-IT')\n",
    "draw_pie_chart(changed_roles_compare_llama_positive['positive'], changed_roles_compare_llama_positive['zero'], changed_roles_compare_llama_positive['negative'], 'Positive Hard Prompt on LLAMA3-8B-IT')\n",
    "draw_pie_chart(change_roles_sae_llama_negative['positive'], change_roles_sae_llama_negative['zero'], change_roles_sae_llama_negative['negative'], 'Negative SAE Steering on LLAMA3-8B-IT')\n",
    "draw_pie_chart(changed_roles_compare_llama_negative['positive'], changed_roles_compare_llama_negative['zero'], changed_roles_compare_llama_negative['negative'], 'Negative Hard Prompt on LLAMA3-8B-IT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_steer_of_value(data_test_csv, value):\n",
    "    data_test_csv = data_test_csv[data_test_csv['steer_dim'] == value]\n",
    "    return \n",
    "get_best_steer_of_value(data_csv_gemma_test, 'Aesthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_table1_rotate_tutu(table1, table1_name, modelname, exclude_rows=None, exclude_columns=None):\n",
    "    if exclude_rows is None:\n",
    "        exclude_rows = []\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "\n",
    "    #value_dims = [str(vd) for vd in table1.index if vd not in exclude_rows]\n",
    "    value_dims = [str(vd) for vd in table1.columns if vd not in exclude_columns]\n",
    "    #steering_features = [int(sf) for sf in table1.columns if sf not in exclude_columns]\n",
    "    steering_features = [int(sf) for sf in table1.index if sf not in exclude_rows]\n",
    "\n",
    "    table_filtered = table1.loc[steering_features, value_dims]\n",
    "\n",
    "    # 初始化 LaTeX 代码\n",
    "    latex_code = '''\n",
    "\\\\newcommand{\\\\cellbar}[5]{%\n",
    "    \\\\raisebox{\\\\height}{%\n",
    "        \\\\begin{tikzpicture}[baseline=(current bounding box.center)]\n",
    "            \\\\draw[draw=black] (0,0) rectangle (1cm,0.4cm);\n",
    "            \\\\path[fill=CustomBlue, opacity=#5] (0,0) rectangle (#1cm,0.4cm);\n",
    "            \\\\path[fill=white] (#1cm,0) rectangle ({#1cm + #3cm},0.4cm);\n",
    "            \\\\path[fill=CustomYellow, opacity=#5] ({#1cm + #3cm},0) rectangle (1cm,0.4cm);\n",
    "            \\\\node[anchor=center, font=\\\\scriptsize] at (0.5cm,0.2cm) {#2};\n",
    "        \\\\end{tikzpicture}%\n",
    "    }%\n",
    "}\n",
    "\\\\begin{table*}\n",
    "\\\\centering\n",
    "\\\\caption{Value steering using SAE features for the {''' + modelname + '''} model.}\n",
    "\\\\label{table: sae-steering-''' + modelname + '''}\n",
    "\\\\resizebox{\\\\textwidth}{!}{%\n",
    "\\\\begin{tabular}{>{\\\\centering\\\\arraybackslash}m{1.5cm} *{''' + str(len(value_dims)) + '''}{>{\\\\centering\\\\arraybackslash}m{1cm}}>{\\\\centering\\\\arraybackslash}m{1cm}}\n",
    "\\\\toprule\n",
    "'''\n",
    "\n",
    "\n",
    "    # 生成表头\n",
    "    header_row = ['\\\\diagbox[width=1.8cm, height=2.7cm]{\\\\textbf{SAE}\\\\\\\\\\\\textbf{Feature}}{\\\\rotatebox{90}{\\\\textbf{Value}}}'] + ['\\\\rotatebox{90}{\\\\textbf{' + vd + '}}' for vd in value_dims] + ['\\\\rotatebox{90}{\\\\textbf{Mean Similarity}}']\n",
    "    latex_code += ' & '.join(header_row) + ' \\\\\\\\\\n\\\\midrule\\n'\n",
    "    latex_code += '\\\\multicolumn{' + str(len(value_dims)+2) + '}{c}{\\\\textbf{\\\\small{' + modelname + '}}}' + ' \\\\\\\\\\n\\\\midrule\\n'\n",
    "\n",
    "    # 定义单元格内容的生成函数\n",
    "    def generate_cell_content(red_ratio, green_ratio, transparency_ratio, similarity):\n",
    "        \"\"\"\n",
    "        生成单元格的 LaTeX 代码，使用 \\\\cellbar{red_length}{similarity}{transparency_length}{green_length}{opacity}\n",
    "        \"\"\"\n",
    "        # 确保比例在 0 到 1 之间，并归一化\n",
    "        total = red_ratio + green_ratio + transparency_ratio\n",
    "        if total > 0:\n",
    "            red_ratio /= total\n",
    "            green_ratio /= total\n",
    "            transparency_ratio /= total\n",
    "        else:\n",
    "            red_ratio = green_ratio = transparency_ratio = 0.0\n",
    "\n",
    "        red_length = red_ratio\n",
    "        transparency_length = transparency_ratio\n",
    "        green_length = green_ratio\n",
    "\n",
    "        opacity = 1.0 - transparency_ratio  # 透明度与透明部分成反比\n",
    "\n",
    "        return f'\\\\cellbar{{{red_length:.2f}}}{{{similarity:.2f}}}{{{transparency_length:.2f}}}{{{green_length:.2f}}}{{{opacity:.2f}}}'\n",
    "\n",
    "    # 生成表格内容\n",
    "    lowest_ratios = {vd: [] for vd in value_dims} \n",
    "    for sf in steering_features:\n",
    "        cosines = []\n",
    "        row = ['\\\\textbf{' + str(sf) + '}']\n",
    "        for vd in value_dims:\n",
    "            value = table_filtered.loc[sf, vd]\n",
    "            if isinstance(value, str):\n",
    "                # 处理字符串值，计算相似度\n",
    "                value_list = list(map(int, value.split('/')))\n",
    "                traindata = value_list[:3]\n",
    "                testdata = value_list[3:]\n",
    "                # 计算相似度\n",
    "                traindata_p = np.array(traindata) / np.sum(traindata)\n",
    "                testdata_p = np.array(testdata) / np.sum(testdata)\n",
    "                similarity = np.dot(traindata_p, testdata_p) / (np.linalg.norm(traindata_p) * np.linalg.norm(testdata_p))\n",
    "                cosines.append(similarity)\n",
    "                lowest_ratios[vd].append(min(traindata_p))\n",
    "\n",
    "                # 提取红色、绿色和透明度的值\n",
    "                red_value = traindata[0]\n",
    "                green_value = traindata[1]\n",
    "                transparency_value = traindata[2]\n",
    "                # 计算比例\n",
    "                red_ratio = red_value\n",
    "                green_ratio = green_value\n",
    "                transparency_ratio = transparency_value\n",
    "\n",
    "                # 生成单元格内容\n",
    "                cell_content = generate_cell_content(red_ratio, green_ratio, transparency_ratio, similarity)\n",
    "                row.append(cell_content)\n",
    "            else:\n",
    "                row.append('')\n",
    "\n",
    "        avg_similarity = np.mean(cosines) if cosines else 0\n",
    "        row.append(f'{avg_similarity:.2f}')\n",
    "        latex_code += ' & '.join(row) + ' \\\\\\\\\\n'\n",
    "    latex_code += '\\\\midrule\\nNoise Ratio:& ' + '&'.join([f'{np.mean(lowest_ratios[vd]):.2f}' for vd in value_dims]) + ' \\\\\\\\\\n'\n",
    "    latex_code += '\\\\bottomrule\\n\\\\end{tabular}\\n}\\n\\\\end{table*}\\n'\n",
    "\n",
    "    # 保存 LaTeX 代码\n",
    "    with open(f'{table1_name}.tex', 'w') as f:\n",
    "        f.write(latex_code)\n",
    "  \n",
    "\n",
    "get_latex_table1_rotate_tutu(table1_gemma, 'table1_gemma_rotate', 'Gemma-2B-IT', exclude_rows_gemma, exclude_columns_gemma)\n",
    "get_latex_table1_rotate_tutu(table1_llama, 'table1_llama_rotate', 'Llama3-8B-IT', exclude_rows_llama, exclude_columns_llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T2P\n",
    "CAUSAL_METHOD = 'pc'\n",
    "NOISE_AUGUMENT_SINGLE_SAE = None #10\n",
    "NOISE_VAR = 0.00001\n",
    "PC_ALPHA = 0.05 #0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_d_columns_deprecated(answer_valuebench_features_csv):\n",
    "    data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
    "    digits = [str(d) for d in range(10)]\n",
    "    d_columns = [d for d in data_csv.columns if d[0] in digits]\n",
    "    d_data = data_csv[d_columns]\n",
    "    stds = d_data.std()\n",
    "    avgs = d_data.mean()\n",
    "    std_avg = stds/avgs\n",
    "    #d_columns_valid = [d for d in d_columns if avgs[d] > 1]\n",
    "    d_columns_valid = d_columns\n",
    "    return d_columns_valid\n",
    "\n",
    "def get_value_dims_from_csv(data_csv):\n",
    "    return [v for v in data_csv.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "\n",
    "def causal_inference(data, ci_dimensions, pdy_name, method, noise_augument=None, prior_source_set=None):\n",
    "    if noise_augument:\n",
    "        data = np.tile(data, (noise_augument, 1))\n",
    "        noise = np.random.normal(0, NOISE_VAR, data.shape)\n",
    "        data = data + noise\n",
    "\n",
    "    if method == 'pc':\n",
    "        g = pc(data, PC_ALPHA, node_names=ci_dimensions)\n",
    "        \n",
    "        if prior_source_set:\n",
    "            bk = BackgroundKnowledge()\n",
    "            nodes = g.G.get_nodes()\n",
    "            for node1 in nodes:\n",
    "                for node2 in nodes:\n",
    "                    if node1.name in prior_source_set and node2.name in prior_source_set and node1.name != node2.name:\n",
    "                        bk = bk.add_forbidden_by_node(node1, node2)\n",
    "            g = pc(data, PC_ALPHA, node_names=ci_dimensions, background_knowledge=bk)\n",
    "            \n",
    "        graph = g.G\n",
    "\n",
    "        edges = []\n",
    "        for n1 in range(len(graph.nodes)):\n",
    "            assert graph.nodes[n1].name == ci_dimensions[n1]\n",
    "            for n2 in range(n1+1, len(graph.nodes)):\n",
    "                # if n1 == n2:\n",
    "                #     continue\n",
    "                if graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == 1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'single-arrow'])\n",
    "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == -1:\n",
    "                    edges.append([graph.nodes[n2].name, graph.nodes[n1].name, 1, 'single-arrow']) \n",
    "                elif graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == -1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'no-arrow'])\n",
    "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == 1:\n",
    "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'double-arrow'])\n",
    "                else:\n",
    "                    if not (graph.graph[n1][n2] == 0 and graph.graph[n2][n1] == 0):\n",
    "                        raise ValueError('Invalid edge')\n",
    "    else:\n",
    "        raise ValueError('Invalid method')\n",
    "    \n",
    "    columns_concerned_vis = [label.replace(':','-') for label in ci_dimensions]\n",
    "    pdy = GraphUtils.to_pydot(graph, labels=columns_concerned_vis)\n",
    "    pdy.write_png(pdy_name)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def deal_with_csv(data_csv, graph_path, v_showongraph='ALL', row_num='ALL', method=CAUSAL_METHOD, dummy_steered_dim=False): \n",
    "    v_inference = get_value_dims_from_csv(data_csv)\n",
    "    if os.path.exists(graph_path):\n",
    "        shutil.rmtree(graph_path)\n",
    "    os.makedirs(graph_path, exist_ok=True)\n",
    "    \n",
    "    # data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
    "    # v_columns_all = [v for v in data_csv.columns if (v not in ['player_name', 'steer_dim', 'stds']) and (not v.endswith(':scstd'))]\n",
    "    # if v_inference == 'ALL':\n",
    "    #     v_columns_inference = v_columns_all\n",
    "    # else:\n",
    "    #     for v in v_inference:\n",
    "    #         if v not in v_columns_all:\n",
    "    #             raise ValueError('Invalid v_inference')\n",
    "    #     v_columns_inference = v_inference\n",
    "\n",
    "    v_columns_inference = v_inference\n",
    "\n",
    "    if v_showongraph == 'ALL':\n",
    "        v_columns_showgraph = v_columns_inference\n",
    "    else:\n",
    "        for v in v_showongraph:\n",
    "            if v not in v_columns_inference:\n",
    "                raise ValueError('Invalid v_showongraph')\n",
    "        v_columns_showgraph = v_showongraph\n",
    "\n",
    "    if dummy_steered_dim:\n",
    "        steer_dim_dummies = pd.get_dummies(data_csv['steer_dim'], prefix='steer_dim') * 1\n",
    "        data = pd.concat([data_csv, steer_dim_dummies], axis=1)\n",
    "        v_columns_inference_total = v_columns_inference + list(steer_dim_dummies.columns) \n",
    "        v_columns_showgraph_total = v_columns_showgraph + list(steer_dim_dummies.columns)\n",
    "    else:\n",
    "        data = data_csv\n",
    "        v_columns_inference_total = v_columns_inference\n",
    "        v_columns_showgraph_total = v_columns_showgraph\n",
    "    \n",
    "    data = data[v_columns_inference_total].to_numpy()    \n",
    "    \n",
    "    if type(row_num) == int:\n",
    "        rows = np.random.choice(data.shape[0], row_num, replace=False)\n",
    "        data = data[rows]\n",
    "    else:\n",
    "        assert row_num == 'ALL'\n",
    "\n",
    "    if dummy_steered_dim:\n",
    "        edges_total = causal_inference(data, v_columns_inference_total, graph_path + \"/total.png\", method, noise_augument=None, prior_source_set=list(steer_dim_dummies.columns))\n",
    "    else:\n",
    "        edges_total = causal_inference(data, v_columns_inference_total, graph_path + \"/total.png\", method, noise_augument=None)\n",
    "    \n",
    "    edges_sfs = []\n",
    "    steer_dims = data_csv['steer_dim'].unique()\n",
    "    for steer_dim in steer_dims:\n",
    "        print(steer_dim)\n",
    "        if np.isnan(steer_dim):\n",
    "            data = data_csv[data_csv['steer_dim'].isnull()][v_columns_inference].to_numpy()\n",
    "            edges_nosteer = causal_inference(data, v_columns_inference, graph_path + f'/{steer_dim}.png', method, noise_augument=NOISE_AUGUMENT_SINGLE_SAE)\n",
    "        else:\n",
    "            data = data_csv[data_csv['steer_dim'] == steer_dim][v_columns_inference].to_numpy()\n",
    "            sfedge = causal_inference(data, v_columns_inference, graph_path + f'/{steer_dim}.png', method, noise_augument=NOISE_AUGUMENT_SINGLE_SAE)\n",
    "            edges_sfs.append(sfedge)\n",
    "\n",
    "    return edges_total, edges_nosteer, edges_sfs\n",
    "\n",
    "#T2P\n",
    "edges_gemma_total, edges_gemma_nosteer, edges_gemma_sfs = deal_with_csv(data_csv_gemma_train, 'value_causal_graph_gemma')\n",
    "edges_llama_total, edges_llama_nosteer, edges_llama_sfs = deal_with_csv(data_csv_llama_train, 'value_causal_graph_llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "edges_standard_small = [\n",
    "    ['Positive coping', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Social', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Anxiety Disorder', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Economic', 'Organization', 1, 'single-arrow'],\n",
    "    ['Political', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Religious', 'Understanding', 1, 'single-arrow'],\n",
    "]\n",
    "\n",
    "edges_llama_gen = [\n",
    "    ['Understanding', 'Empathy', 1, 'single-arrow'],# Greater understanding leads to increased empathy.\n",
    "    ['Social Complexity', 'Social', 1, 'single-arrow'],# Increased social complexity leads to a greater emphasis on social values.\n",
    "    ['Anxiety Disorder', 'Uncertainty Avoidance', 1, 'single-arrow'],# Anxiety may increase the need to avoid uncertainty.\n",
    "    ['Social Cynicism', 'Social Complexity', 1, 'single-arrow'],# Cynicism might arise from perceiving social structures as complex and untrustworthy.\n",
    "    ['Achievement', 'Economic', 1, 'single-arrow'],# A focus on achievement often aligns with economic values.\n",
    "    ['Organization', 'Economic', 1, 'single-arrow'],# Organizational structures often prioritize economic efficiency.\n",
    "    ['Political', 'Social', 1, 'single-arrow'],# Political values often influence social norms and structures.\n",
    "    ['Religious', 'Theoretical', 1, 'single-arrow'],# Religious beliefs often involve theoretical or philosophical frameworks.\n",
    "    ['Breadth of Interest', 'Theoretical', 1, 'single-arrow'],# A broad range of interests can lead to a greater appreciation for theoretical concepts.\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],# Avoiding uncertainty can contribute to anxiety disorders.\n",
    "    ['Social', 'Empathy', 1, 'single-arrow'],# Social values emphasize the importance of empathy and understanding.\n",
    "    ['Aesthetic', 'Understanding', 1, 'single-arrow'],# Appreciation for aesthetics can foster a deeper understanding of the world.\n",
    "]\n",
    "\n",
    "edges_gemma_gen = [\n",
    "    ['Positive coping', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Social Complexity', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Economic', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Political', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Religious', 1, 'single-arrow'],\n",
    "    ['Positive coping', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Positive coping', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Understanding', 1, 'single-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Social Complexity', 1, 'single-arrow']\n",
    "]\n",
    "\n",
    "\n",
    "edges_valuebench = [\n",
    "    # ['Self-Enhancement', 'Achievement', 1, 'single-arrow'],\n",
    "    # ['Industriousness', 'Achievement', 1, 'single-arrow'],\n",
    "    # ['Analytical', 'Breath of Interest', 1, 'single-arrow'],\n",
    "    # ['Openness', 'Breath of Interest', 1, 'single-arrow'],\n",
    "    # ['Agreeableness', 'Empathy', 1, 'single-arrow'],\n",
    "    # ['Emotional', 'Empathy', 1, 'single-arrow'],\n",
    "    # ['Cooperativeness', 'Empathy', 1, 'single-arrow'],\n",
    "    # ['Conscientiousness', 'Organization', 1, 'single-arrow'],\n",
    "    # ['Dependable', 'Organization', 1, 'single-arrow'],\n",
    "    # ['Openness to Experience', 'Understanding', 1, 'single-arrow'],\n",
    "    # ['Agreeableness', 'Understanding', 1, 'single-arrow'],\n",
    "    ##########################################\n",
    "    ['Empathy', 'Understanding', 1, 'single-arrow'],\n",
    "]\n",
    "\n",
    "edges_random = [\n",
    "    ['Positive coping', 'Economic', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Achievement', 1, 'single-arrow'],\n",
    "    ['Resilience', 'Social Cynicism', 1, 'single-arrow'],\n",
    "    ['Social Complexity', 'Breadth of Interest', 1, 'single-arrow'],\n",
    "    ['Achievement', 'Uncertainty Avoidance', 1, 'single-arrow'],\n",
    "    ['Uncertainty Avoidance', 'Anxiety Disorder', 1, 'single-arrow'],\n",
    "    ['Aesthetic', 'Religious', 1, 'single-arrow'],\n",
    "    ['Anxiety Disorder', 'Understanding', 1, 'single-arrow'],\n",
    "    ['Breadth of Interest', 'Theoretical', 1, 'single-arrow'],\n",
    "    ['Economic', 'Political', 1, 'single-arrow'],\n",
    "    ['Organization', 'Social', 1, 'single-arrow'],\n",
    "    ['Political', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Religious', 'Empathy', 1, 'single-arrow'],\n",
    "    ['Theoretical', 'Positive coping', 1, 'single-arrow'],\n",
    "    ['Understanding', 'Social Complexity', 1, 'double-arrow'],\n",
    "    ['Social', 'Resilience', 1, 'single-arrow'],\n",
    "    ['Social Cynicism', 'Breadth of Interest', 1, 'double-arrow'],\n",
    "    ['Positive coping', 'Aesthetic', 1, 'single-arrow'],\n",
    "    ['Empathy', 'Theoretical', 1, 'double-arrow'],\n",
    "    ['Resilience', 'Organization', 1, 'single-arrow'],\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zero_double_arrow(edges):\n",
    "    double_arrow_edges = [edge for edge in edges if edge[3] == 'double-arrow']\n",
    "    zero_arrow_edges = [edge for edge in edges if edge[3] == 'no-arrow']\n",
    "    if double_arrow_edges:\n",
    "        raise ValueError('Double arrow:', double_arrow_edges)\n",
    "    if zero_arrow_edges:\n",
    "        raise ValueError('Zero arrow:', zero_arrow_edges)\n",
    "\n",
    "def dealwith_zero_double_duplicated_arrow(edges):\n",
    "    double_arrow_edges = [edge for edge in edges if edge[3] == 'double-arrow']\n",
    "    zero_arrow_edges = [edge for edge in edges if edge[3] == 'no-arrow']\n",
    "    print('Double arrow:', double_arrow_edges)\n",
    "    print('Zero arrow:', zero_arrow_edges)\n",
    "    print('Dealwith zero and double arrow edges')\n",
    "    print('----------------------')\n",
    "    \n",
    "    new_edges = []\n",
    "    for edge in edges:\n",
    "        if edge[3] == 'double-arrow' or edge[3] == 'no-arrow':\n",
    "            if [edge[0], edge[1], edge[2], 'single-arrow'] not in new_edges:\n",
    "                new_edges.append([edge[0], edge[1], edge[2], 'single-arrow'])\n",
    "            if [edge[1], edge[0], edge[2], 'single-arrow'] not in new_edges:\n",
    "                new_edges.append([edge[1], edge[0], edge[2], 'single-arrow'])\n",
    "        else:\n",
    "            if edge not in new_edges:\n",
    "                new_edges.append(edge)\n",
    "    return new_edges\n",
    "\n",
    "def check_dag(edges):\n",
    "    nxg = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        if edge[3] == 'single-arrow':\n",
    "            nxg.add_edge(edge[0], edge[1])\n",
    "    if not nx.is_directed_acyclic_graph(nxg):\n",
    "        cycles = list(nx.simple_cycles(nxg))\n",
    "        raise ValueError('Cycle:', cycles)\n",
    "\n",
    "##################################################\n",
    "\n",
    "def compute_reachability(edges, nodes):\n",
    "    # 构建邻接矩阵\n",
    "    n = len(nodes)\n",
    "    adj = [[False] * n for _ in range(n)]\n",
    "    for edge in edges:\n",
    "        u = nodes.index(edge[0])\n",
    "        v = nodes.index(edge[1])\n",
    "        adj[u][v] = True\n",
    "    \n",
    "    # 计算传递闭包\n",
    "    reach = [row[:] for row in adj]\n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                reach[i][j] = reach[i][j] or (reach[i][k] and reach[k][j])\n",
    "    \n",
    "    return reach\n",
    "\n",
    "#很笨的方法，不断的循环直至不能再减少\n",
    "def transitive_reduction(edges, nodes):\n",
    "    # 计算原始传递闭包\n",
    "    original_reach = compute_reachability(edges, nodes)\n",
    "    \n",
    "    # 复制原始边列表\n",
    "    reduced_edges = edges.copy()\n",
    "    \n",
    "    # 标志位，表示传递闭包是否发生变化\n",
    "    reach_changed = True\n",
    "    \n",
    "    # 循环遍历所有边，直到传递闭包不再变化，这个可以保证找到最小的传递闭包\n",
    "    while reach_changed:\n",
    "        reach_changed = False  # 假设传递闭包不会变化\n",
    "        for edge in edges:\n",
    "            if edge not in reduced_edges:\n",
    "                continue  # 如果边已经被删除，跳过\n",
    "            \n",
    "            u = nodes.index(edge[0])\n",
    "            v = nodes.index(edge[1])\n",
    "            \n",
    "            # 尝试删除边 (u, v)\n",
    "            reduced_edges.remove(edge)\n",
    "            \n",
    "            # 计算删除后的传递闭包\n",
    "            new_reach = compute_reachability(reduced_edges, nodes)\n",
    "            \n",
    "            # 检查传递闭包是否变化\n",
    "            if new_reach == original_reach:\n",
    "                # 传递闭包不变，保留删除操作\n",
    "                reach_changed = True  # 传递闭包可能在其他边删除时变化\n",
    "            else:\n",
    "                # 传递闭包变化，恢复该边\n",
    "                reduced_edges.append(edge)\n",
    "    \n",
    "    return reduced_edges\n",
    "\n",
    "def plot_graph(edges, title):\n",
    "    # 创建有向图\n",
    "    G = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    # 绘制图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(G, seed=42)  # 布局算法\n",
    "    nx.draw(G, pos, with_labels=True, node_size=2000, node_color=\"lightblue\", font_size=10, font_weight=\"bold\", arrows=True)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "def compare_graph_reduction(edges):\n",
    "    # 提取所有节点并固定顺序\n",
    "    nodes = sorted(set(edge[0] for edge in edges).union(set(edge[1] for edge in edges)))\n",
    "\n",
    "    # 可视化原始图\n",
    "    plot_graph(edges, \"Original Graph\")\n",
    "\n",
    "    # 执行传递归约\n",
    "    reduced_edges = transitive_reduction(edges, nodes)\n",
    "\n",
    "    # 可视化归约后的图\n",
    "    plot_graph(reduced_edges, \"Reduced Graph\")\n",
    "\n",
    "    print(\"Reduced Edges:\")\n",
    "    for edge in reduced_edges:\n",
    "        print(edge)\n",
    "    \n",
    "    print(len(edges), len(reduced_edges))\n",
    "\n",
    "def get_reduced_edges(edges):\n",
    "    nodes = sorted(set(edge[0] for edge in edges).union(set(edge[1] for edge in edges)))\n",
    "    reduced_edges = transitive_reduction(edges, nodes)\n",
    "    return reduced_edges\n",
    "###############################################\n",
    "#T2P\n",
    "edges_gemma_total = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_gemma_total))\n",
    "edges_gemma_nosteer =  get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_gemma_nosteer))\n",
    "edges_gemma_sfs = [get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_sf)) for edges_sf in edges_gemma_sfs]\n",
    "edges_llama_total = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_llama_total))\n",
    "edges_llama_nosteer = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_llama_nosteer))\n",
    "edges_llama_sfs = [get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_sf)) for edges_sf in edges_llama_sfs]\n",
    "edges_standard_small = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_standard_small))\n",
    "edges_random = get_reduced_edges(dealwith_zero_double_duplicated_arrow(edges_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subsequent_nodes(edges, node):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "\n",
    "    subsequent_nodes = set()\n",
    "    subsequent_nodes.add(node)\n",
    "    while True:\n",
    "        subsequent_nodes_len = len(subsequent_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[0] in subsequent_nodes:\n",
    "                subsequent_nodes.add(edge[1])\n",
    "        if len(subsequent_nodes) == subsequent_nodes_len:\n",
    "            break\n",
    "    subsequent_nodes.remove(node)\n",
    "    return subsequent_nodes\n",
    "\n",
    "def get_all_preceding_nodes(edges, node):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "\n",
    "    preceding_nodes = set()\n",
    "    preceding_nodes.add(node)\n",
    "    while True:\n",
    "        preceding_nodes_len = len(preceding_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[1] in preceding_nodes:\n",
    "                preceding_nodes.add(edge[0])\n",
    "        if len(preceding_nodes) == preceding_nodes_len:\n",
    "            break\n",
    "    preceding_nodes.remove(node)\n",
    "    return preceding_nodes\n",
    "\n",
    "def get_all_un_related_nodes(edges, node, all_nodes):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "   \n",
    "\n",
    "    #find all nodes that are in a same connected component with node\n",
    "    unrelated_nodes = set(all_nodes)\n",
    "    connected_nodes = set()\n",
    "    connected_nodes.add(node) \n",
    "    while True:\n",
    "        connected_nodes_len = len(connected_nodes)\n",
    "        for edge in edges:\n",
    "            if edge[0] in connected_nodes:\n",
    "                connected_nodes.add(edge[1])\n",
    "            if edge[1] in connected_nodes:\n",
    "                connected_nodes.add(edge[0])\n",
    "        if len(connected_nodes) == connected_nodes_len:\n",
    "            break\n",
    "    unrelated_nodes = unrelated_nodes - connected_nodes\n",
    "\n",
    "    #T2P\n",
    "    #return unrelated_nodes, connected_nodes - set([node])\n",
    "    return set(all_nodes) - get_all_subsequent_nodes(edges, node) - set([node]) - get_all_preceding_nodes(edges, node), connected_nodes - set([node])\n",
    "\n",
    "def get_all_non_subsequent_nodes(edges, node, all_nodes):\n",
    "    #T2P\n",
    "    check_zero_double_arrow(edges)\n",
    "    return set(all_nodes) - get_all_subsequent_nodes(edges, node) - set([node])\n",
    "\n",
    "\n",
    "\n",
    "# print(get_all_subsequent_nodes(edges_standard_small, 'Understanding'))\n",
    "# print(get_all_preceding_nodes(edges_standard_small, 'Understanding'))\n",
    "# v_inference = [v for v in data_csv_gemma_train.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "# print(get_all_un_related_nodes(edges_standard_small, 'Understanding', v_inference))\n",
    "\n",
    "print(get_all_subsequent_nodes(edges_gemma_total, 'Achievement'))\n",
    "print(get_all_preceding_nodes(edges_gemma_total, 'Achievement'))\n",
    "v_inference = [v for v in data_csv_gemma_train.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "print(get_all_un_related_nodes(edges_gemma_total, 'Achievement', v_inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_table2(data_scorechange, edges, edges_source, pd_result_table2, stat_method, expected, unexpected):\n",
    "    for column in data_scorechange.columns:\n",
    "        #print('=============')\n",
    "        #print(column)\n",
    "                \n",
    "        subsequent_columns_ideal = get_all_subsequent_nodes(edges, column)\n",
    "        unrelated_columns_ideal, related_columns_ideal = get_all_un_related_nodes(edges, column, set(data_scorechange.columns))\n",
    "        non_subsequent_columns_ideal = get_all_non_subsequent_nodes(edges, column, set(data_scorechange.columns))\n",
    "\n",
    "        if expected == 'subsequent':\n",
    "            expected_columns_ideal = subsequent_columns_ideal\n",
    "        elif expected == 'related':\n",
    "            expected_columns_ideal = related_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid expected')\n",
    "        \n",
    "        if unexpected == 'unrelated':\n",
    "            unexpected_columns_ideal = unrelated_columns_ideal\n",
    "        elif unexpected == 'non_subsequent':\n",
    "            unexpected_columns_ideal = non_subsequent_columns_ideal\n",
    "        else:\n",
    "            raise ValueError('Invalid unexpected')\n",
    "\n",
    "        if stat_method == 'change_score':\n",
    "            related_columns_real = data_scorechange[data_scorechange[column] != 0].abs().mean().sort_values()\n",
    "            #related_columns_real = data_scorechange[data_scorechange[column].abs() > 0.1].abs().mean().sort_values()\n",
    "            #related_columns_real = data_scorechange.abs().mean().sort_values()\n",
    "        elif stat_method == 'change_count':\n",
    "            related_columns_real = data_scorechange[data_scorechange[column] != 0]\n",
    "            related_columns_real = related_columns_real.astype(bool).mean(axis=0)#.sort_values()\n",
    "        else:\n",
    "            raise ValueError('Invalid stat_method')\n",
    "\n",
    "        def classify(vd):\n",
    "            if vd in expected_columns_ideal:\n",
    "                return 'expected'\n",
    "            elif vd in unexpected_columns_ideal:\n",
    "                return 'unexpected'\n",
    "            else:\n",
    "                return 'none'\n",
    "\n",
    "        expected_scabs = []\n",
    "        unexpected_scabs = []\n",
    "        for related_column in related_columns_real.index:\n",
    "            if related_column in expected_columns_ideal:\n",
    "                expected_scabs.append(related_columns_real[related_column])\n",
    "            elif related_column in unexpected_columns_ideal:\n",
    "                unexpected_scabs.append(related_columns_real[related_column])\n",
    "            else:\n",
    "                pass\n",
    "                #assert related_column == column\n",
    "            #print(related_column, related_columns_real[related_column], classify(related_column))\n",
    "        #print('~~~')\n",
    "        \n",
    "        #print('Expected:', np.mean([vdsc for vdsc in expected_scabs if not np.isnan(vdsc)]), len(expected_scabs))\n",
    "        #print('Unexpected:', np.mean([vdsc for vdsc in unexpected_scabs if not np.isnan(vdsc)]), len(unexpected_scabs))\n",
    "        #print('Expected:', np.mean(expected_scabs), len(expected_scabs))\n",
    "        #print('Unexpected:', np.mean(unexpected_scabs), len(unexpected_scabs))\n",
    "\n",
    "        mean_scorechange_related = 'mean_scorechange_' + edges_source + '_expected'\n",
    "        num_related = 'num_' + edges_source + '_expected'\n",
    "        mean_scorechange_unrelated = 'mean_scorechange_' + edges_source + '_unexpected'\n",
    "        num_unrelated = 'num_' + edges_source + '_unexpected'\n",
    "\n",
    "        pd_result_table2.loc[mean_scorechange_related, column] = np.mean(expected_scabs)\n",
    "        #pd_result_table2.loc[num_related, column] = len(expected_scabs)\n",
    "        pd_result_table2.loc[mean_scorechange_unrelated, column] = np.mean(unexpected_scabs)\n",
    "        #pd_result_table2.loc[num_unrelated, column] = len(unexpected_scabs)\n",
    "        #print('----------------------')\n",
    "\n",
    "def create_table2(data_csv, fixed_role, fixed_sf, change_base, modelname, edges_trained, edges_ref, stat_method, expected, unexpected, pd_result_table2):\n",
    "    assert modelname in ['gemma', 'llama']\n",
    "    data_source = modelname\n",
    "\n",
    "    roles = data_csv['player_name'].unique()\n",
    "    sfs = data_csv['steer_dim'].unique()\n",
    "    if not fixed_role:\n",
    "        assert fixed_sf in sfs or fixed_sf == 'plain'\n",
    "    else:\n",
    "        assert fixed_role in roles or fixed_role == 'plain'\n",
    "        assert not fixed_sf\n",
    "    \n",
    "    assert change_base in ['zero', 'first']  \n",
    "    assert stat_method in ['change_score', 'change_count']\n",
    "    assert expected in ['subsequent', 'related']\n",
    "    assert unexpected in ['unrelated', 'non_subsequent']\n",
    "\n",
    "    if fixed_sf:\n",
    "        if fixed_sf == 'plain':\n",
    "            data_nosteer = data_csv[data_csv['steer_dim'].isnull()]\n",
    "        else:\n",
    "            data_nosteer = data_csv[data_csv['steer_dim'] == fixed_sf]\n",
    "\n",
    "        if change_base == 'first':\n",
    "            data_nosteer = data_nosteer[data_nosteer['player_name'].notnull()] \n",
    "        else:\n",
    "            assert np.isnan(data_nosteer.iloc[0]['player_name'])\n",
    "\n",
    "        data_nosteer = data_nosteer[v_inference + ['player_name']]\n",
    "        data_nosteer = data_nosteer.set_index('player_name')\n",
    "\n",
    "    if fixed_role:\n",
    "        if fixed_role == 'plain':\n",
    "            data_nosteer = data_csv[data_csv['player_name'].isnull()]\n",
    "        else:\n",
    "            data_nosteer = data_csv[data_csv['player_name'] == fixed_role]\n",
    "        \n",
    "        if change_base == 'first':\n",
    "            data_nosteer = data_nosteer[data_nosteer['steer_dim'].notnull()]\n",
    "        else:\n",
    "            assert np.isnan(data_nosteer.iloc[0]['steer_dim'])\n",
    "\n",
    "        data_nosteer = data_nosteer[v_inference + ['steer_dim']]\n",
    "        data_nosteer = data_nosteer.set_index('steer_dim')\n",
    "    \n",
    "    data_nosteer = data_nosteer.astype(float)\n",
    "\n",
    "    data_scorechange = data_nosteer - data_nosteer.iloc[0]\n",
    "    assert v_inference == data_scorechange.columns.tolist()\n",
    "\n",
    "    #count the non-zero values per row and calculate the mean\n",
    "    print(\"AVERAGE CHANGED VALUE: \", data_scorechange.abs().astype(bool).sum(axis=0).mean())\n",
    "\n",
    "    \n",
    "    steer_func = '_prompt' if fixed_sf else '_sae'\n",
    "    write_table2(data_scorechange, edges_trained, steer_func + '_ours' , pd_result_table2, stat_method, expected, unexpected)\n",
    "    write_table2(data_scorechange, edges_ref, steer_func + '_refs', pd_result_table2, stat_method, expected, unexpected)\n",
    "\n",
    "\n",
    "v_inference_gemma = [v for v in data_csv_gemma_test.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "v_inference_llama = [v for v in data_csv_llama_test.columns if (v not in ['player_name', 'steer_dim', 'stds', 'scstds']) and (not v.endswith(':scstd'))]\n",
    "assert v_inference_gemma == v_inference_llama\n",
    "v_inference = v_inference_gemma\n",
    "pd_result_table2_gemma = pd.DataFrame(columns=v_inference)\n",
    "pd_result_table2_llama = pd.DataFrame(columns=v_inference)\n",
    "\n",
    "#T2P \n",
    "# create_table2(data_csv_gemma_train, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='gemma', edges_trained=edges_gemma_nosteer, edges_ref=edges_standard, stat_method='change_score', expected = 'subsequent', unexpected = 'non_subsequent',pd_result_table2=pd_result_table2)\n",
    "# create_table2(data_csv_llama_train, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='llama', edges_trained=edges_llama_nosteer, edges_ref=edges_standard, stat_method='change_score', expected = 'subsequent', unexpected = 'non_subsequent',pd_result_table2=pd_result_table2)\n",
    "\n",
    "unexpected = 'non_subsequent'\n",
    "edges_ref = edges_standard_small\n",
    "#edges_ref = edges_valuebench\n",
    "\n",
    "\n",
    "create_table2(data_csv_gemma_test, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='gemma', edges_trained=edges_gemma_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_gemma)\n",
    "create_table2(data_csv_llama_test, fixed_role=None, fixed_sf='plain', change_base=\"first\", modelname='llama', edges_trained=edges_llama_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_llama)\n",
    "\n",
    "for test_name in data_csv_gemma_test['player_name'].unique():\n",
    "    try: \n",
    "        np.isnan(test_name)\n",
    "        continue\n",
    "    except:\n",
    "        if test_name in data_csv_llama_test['player_name'].unique():\n",
    "            break\n",
    "# test_name_list = set.intersection(set(data_csv_gemma_test['player_name']), set(data_csv_llama_test['player_name']))\n",
    "# test_name = random.choice(list(test_name_list))\n",
    " \n",
    "test_name = 'Mary Holt'\n",
    "print(test_name)\n",
    "\n",
    "create_table2(data_csv_gemma_test, fixed_role=test_name, fixed_sf=None, change_base=\"zero\", modelname='gemma', edges_trained=edges_gemma_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_gemma)\n",
    "create_table2(data_csv_llama_test, fixed_role=test_name, fixed_sf=None, change_base=\"zero\", modelname='llama', edges_trained=edges_llama_total, edges_ref=edges_ref, stat_method='change_count', expected = 'subsequent', unexpected = unexpected, pd_result_table2=pd_result_table2_llama)\n",
    "\n",
    "pd_result_table2_gemma.to_csv('table2_gemma.csv')\n",
    "pd_result_table2_llama.to_csv('table2_llama.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the amount of questions per value\n",
    "from sae_lens.config import LOCAL_SAE_MODEL_PATH\n",
    "#df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation_train.csv'))\n",
    "df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation_test.csv'))\n",
    "grouped = df_valuebench.groupby('value')\n",
    "value_counts = grouped.size().reset_index(name='counts')\n",
    "#get the value dimensions in v_inference\n",
    "value_counts = value_counts[value_counts['value'].isin(v_inference)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_table2(group_name):\n",
    "    index_group = [\n",
    "        'mean_scorechange__prompt_ours_expected',\n",
    "        'mean_scorechange__prompt_ours_unexpected',\n",
    "        'mean_scorechange__prompt_refs_expected',\n",
    "        'mean_scorechange__prompt_refs_unexpected',\n",
    "        'mean_scorechange__sae_ours_expected',\n",
    "        'mean_scorechange__sae_ours_unexpected',\n",
    "        'mean_scorechange__sae_refs_expected',\n",
    "        'mean_scorechange__sae_refs_unexpected',\n",
    "        ]\n",
    "\n",
    "    #print the table2 in latex\n",
    "    #rows are for each values dimensions\n",
    "    #columns are in form num_related_ours(mean_scorechange_related_ours), num_unrelated_ours(mean_scorechange_unrelated_ours), num_related_standard(mean_scorechange_related_standard), num_unrelated_standard(mean_scorechange_unrelated_standard)\n",
    "    #the values are the number of related values, the mean of the score change of related values, the number of unrelated values, the mean of the score change of unrelated values\n",
    "    #the values are rounded to 3 decimal places\n",
    "    #the values are in the form number(mean)\n",
    "    #the values are in the form of number(mean)\n",
    "    pd_result_table2 = pd.read_csv('table2_' + group_name + '.csv', index_col=0)\n",
    "    latex_code = '\\\\begin{table*}[ht]\\n\\\\caption{The mean of the score change of related values, the number of related values, the mean of the score change of unrelated values, and the number of unrelated values.}\\n\\\\label{table: scorechange}\\n\\\\begin{center}\\n'\n",
    "    #latex_code += '\\\\begin{tabular}{c@{\\\\hspace{2pt}}' + 'c@{\\\\hspace{2pt}}' * (len(pd_result_table2.columns) - 1) + 'c' + '}\\n\\\\toprule\\n'\n",
    "    latex_code += '\\\\begin{tabular}{c@{\\\\hspace{2pt}}|' + 'c@{\\\\hspace{2pt}}' * 4 +'|' + 'c@{\\\\hspace{2pt}}' * 4 + '}\\n\\\\toprule\\n'\n",
    "    latex_code += 'Value & \\\\multicolumn{4}{c|}{\\\\bf \\\\small Prompt} & \\\\multicolumn{4}{c}{\\\\bf \\\\small SAE}\\\\\\\\\\n\\\\hline\\n'\n",
    "    latex_code += 'Dimensions & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Our causal graph} & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Random causal graph} & \\\\multicolumn{2}{c|}{\\\\bf \\\\tiny Our causal graph} & \\\\multicolumn{2}{c}{\\\\bf \\\\tiny Random causal graph}  \\\\\\\\\\n\\\\hline\\n'\n",
    "    latex_code += 'Score change & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c|}{\\\\bf \\\\tiny Unexpected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Expected} & \\\\multicolumn{1}{c}{\\\\bf \\\\tiny Unexpected}\\\\\\\\\\n\\\\hline\\n'\n",
    "    #each row in latex is a column in the dataframe\n",
    "\n",
    "    index_dict = {index: [] for index in index_group}\n",
    "\n",
    "    for column in pd_result_table2.columns:\n",
    "        latex_code += '\\\\small ' + column + ' & '\n",
    "        #for index in pd_result_table2.index:\n",
    "        for index in index_group:\n",
    "            if index.startswith('mean'):\n",
    "                latex_code += str(round(pd_result_table2.loc[index, column], 2)) + ' & '\n",
    "                index_dict[index].append(pd_result_table2.loc[index, column])\n",
    "\n",
    "        latex_code = latex_code[:-2] + ' \\\\\\\\\\n'\n",
    "    latex_code += '\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\end{table*}'\n",
    "    #print(latex_code)\n",
    "    #write the latex code to a file\n",
    "    with open('table2' + group_name + '.tex', 'w') as f:\n",
    "        f.write(latex_code)\n",
    "    return index_dict\n",
    "\n",
    "index_dict_gemma = get_latex_table2('gemma')\n",
    "index_dict_llama = get_latex_table2('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "\n",
    "def make_chart(index_dict, modelname):\n",
    "    data1 = [\n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_ours_expected']), \n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_refs_expected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_ours_expected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_refs_expected'])\n",
    "        ]\n",
    "    data2 = [\n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_ours_unexpected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__prompt_refs_unexpected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_ours_unexpected']),\n",
    "        np.nanmean(index_dict['mean_scorechange__sae_refs_unexpected'])\n",
    "        ]\n",
    "    #Also keep the standard deviation of each group\n",
    "    data1_std = [\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_ours_expected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_refs_expected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_ours_expected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_refs_expected'])\n",
    "        ]\n",
    "    data2_std = [\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_ours_unexpected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__prompt_refs_unexpected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_ours_unexpected']),\n",
    "        np.nanstd(index_dict['mean_scorechange__sae_refs_unexpected'])\n",
    "    ]\n",
    "    print(data1_std, data2_std)\n",
    "\n",
    "    labels_ = ['Our Graph','Reference Graph','Our Graph','Reference Graph']\n",
    "    \n",
    "    #用来为坐标的常规坐标 还是 对数坐标做准备\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.8))#用来控制图片的大小\n",
    "    #fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "    # 设置柱状图参数\n",
    "    width = 0.35 #柱状图每个柱子的宽度,同时也是调整每组柱子之间的间隙\n",
    "    x = np.arange(len(labels_)) #用来指定每个柱子位置参数\n",
    "    \n",
    "    # 绘制柱状图,正常坐标,别忘了errorbar\n",
    "    ax.errorbar(x-width/2, (data1), yerr=data1_std, fmt='o', color=(115/255, 186/255, 214/255), capsize=5)\n",
    "    ax.errorbar(x+width/2, (data2), yerr=data2_std, fmt='o', color=(38/255, 70/255, 83/255), capsize=5)\n",
    "\n",
    "\n",
    "    ax.bar(x-width/2, (data1), width=width, label='Expected',  color=(115/255, 186/255, 214/255)) #edgecolor='#2596be', facecolor='none', hatch='//',\n",
    "    ax.bar(x+width/2, (data2), width=width, label='Unexpected', color=(38/255, 70/255, 83/255)) #hatch='xx', \n",
    "    #change the color of the bars\n",
    "\n",
    "    # 给每个柱条添加数值标签\n",
    "    for a, b in zip(x-width/2, data1):\n",
    "        plt.text(a, b+0.01, '%.2f' % b, ha='center', va='bottom', fontsize=12)\n",
    "    for a, b in zip(x+width/2, data2):\n",
    "        plt.text(a, b+0.01, '%.2f' % b, ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    # 设置y轴范围, \n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "\n",
    "    # 设置图表标题和轴标签\n",
    "    plt.title(f'{modelname}')\n",
    "    #plt.xlabel('Order')\n",
    "    plt.ylabel('Frequency of changes in other value dimensions')\n",
    "    \n",
    "    #plt.xticks(x, fontsize=12, rotation=45,loc='inside')#设置标签的文字大小和旋转方向\n",
    "    plt.xticks(x, labels_)  #使得标签现实的是给定的文字标签\n",
    "\n",
    "\n",
    "    p1 = patches.Rectangle((.515, 0), width=.39, height=.10, alpha=.1, facecolor='yellow', transform=fig.transFigure)\n",
    "    p2 = patches.Rectangle((.125, 0), width=.39, height=.10, alpha=.1, facecolor='red', transform=fig.transFigure)\n",
    "    #can we add a label for each rectangle?\n",
    "\n",
    "    fig.add_artist(p1)\n",
    "    fig.add_artist(p2)\n",
    "\n",
    "    fig.text(0.125 + 0.39/2, .03, 'Prompt', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            fontsize=12, \n",
    "            color='black')\n",
    "    fig.text(0.515 + 0.39/2, .03, 'Sae', \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            fontsize=12, \n",
    "            color='black')\n",
    "\n",
    "    # 设置图例\n",
    "    plt.legend(loc='upper right', ncol=1)#设置图例的位置和列数\n",
    "    \n",
    "    #获取默认图片尺寸\n",
    "    figure = plt.gcf()\n",
    "    width = figure.bbox.width\n",
    "    height = figure.bbox.height\n",
    "    print(width,height)\n",
    "    \n",
    "    #plt.tight_layout(pad=10)\n",
    "\n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "make_chart(index_dict_gemma, 'Gemma-2B-IT')\n",
    "make_chart(index_dict_llama, 'Llama3-8B-IT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steer_dims = ['nan', 1312, 1341, 2221, 3183, 6619, 7502, 8387, 10096, 14049]\n",
    "\n",
    "nodes = {}\n",
    "for entity in v_inference:\n",
    "    nodes[entity] = os.path.join('valuebench','value_questions_' + entity + '.html'),\n",
    "# for feature in data_csv.['steer_dim'].unique()[1:]:\n",
    "#     nodes[feature] = 'https://www.neuronpedia.org/' + sae.cfg.model_name +'/' + str(sae.cfg.hook_layer) + '-res-jb/' + str(feature)\n",
    "\n",
    "print(len(edges_gemma_total), len(edges_llama_total), len(edges_ref))\n",
    "edges = {\n",
    "    'gemma': edges_gemma_total,\n",
    "    'llama': edges_llama_total,\n",
    "    'reference': edges_standard_small,\n",
    "    'gemma_gen': edges_gemma_gen,\n",
    "    'llama_gen': edges_llama_gen,\n",
    "    'valuebench': edges_valuebench,\n",
    "}\n",
    "\n",
    "json_object = {\n",
    "    'nodes': nodes,\n",
    "    'edges': edges\n",
    "    }\n",
    "\n",
    "json.dump(json_object, open('value_graph_data1.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We denote the set of subsequent nodes of $v$ in graph $G$ as $V_{suc}^G(v)$, the set of roles that $S_r[v]\\neq S_{r_0}[v]$ as $R_{diff}(v)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
