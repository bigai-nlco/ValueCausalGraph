{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNk7IylTv610"
      },
      "source": [
        "# Loading and Analysing Pre-Trained Sparse Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_DusoOvwV0M"
      },
      "source": [
        "## Imports & Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfDUxRx0wSRl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    from google.colab import output\n",
        "    COLAB = True\n",
        "    %pip install sae-lens transformer-lens\n",
        "except:\n",
        "    COLAB = False\n",
        "    from IPython import get_ipython # type: ignore\n",
        "    ipython = get_ipython(); assert ipython is not None\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "# Standard imports\n",
        "import os\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import math\n",
        "import gc\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from faker import Faker\n",
        "\n",
        "# Imports for displaying vis in Colab / notebook\n",
        "import webbrowser\n",
        "import http.server\n",
        "import socketserver\n",
        "import threading\n",
        "PORT = 8000\n",
        "\n",
        "import torch\n",
        "torch.set_grad_enabled(False);\n",
        "from openai import AzureOpenAI\n",
        "from datasets import load_dataset  \n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "import transformer_lens\n",
        "from transformer_lens import utils\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformer_lens.utils import tokenize_and_concatenate\n",
        "\n",
        "from sae_lens import SAE\n",
        "from sae_lens.config import DTYPE_MAP, LOCAL_SAE_MODEL_PATH\n",
        "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
        "\n",
        "from sae_vis.data_config_classes import SaeVisConfig\n",
        "from sae_vis.data_storing_fns import SaeVisData\n",
        "\n",
        "from causallearn.search.ConstraintBased.FCI import fci\n",
        "from causallearn.search.ConstraintBased.PC import pc\n",
        "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
        "from causallearn.utils.cit import kci\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aGgWkbav610"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQSD7trbv610",
        "outputId": "222a40c4-75d4-46e2-ed3f-991841144926"
      },
      "outputs": [],
      "source": [
        "# For the most part I'll try to import functions and classes near where they are used\n",
        "# to make it clear where they come from.\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPUq_bdW8mcp"
      },
      "outputs": [],
      "source": [
        "def display_vis_inline(filename: str, height: int = 850):\n",
        "    '''\n",
        "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
        "    vis has a unique port without having to define a port within the function.\n",
        "    '''\n",
        "    if not(COLAB):\n",
        "        webbrowser.open(filename);\n",
        "\n",
        "    else:\n",
        "        global PORT\n",
        "\n",
        "        def serve(directory):\n",
        "            os.chdir(directory)\n",
        "\n",
        "            # Create a handler for serving files\n",
        "            handler = http.server.SimpleHTTPRequestHandler\n",
        "\n",
        "            # Create a socket server with the handler\n",
        "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
        "                print(f\"Serving files from {directory} on port {PORT}\")\n",
        "                httpd.serve_forever()\n",
        "\n",
        "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
        "        thread.start()\n",
        "\n",
        "        output.serve_kernel_port_as_iframe(PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True)\n",
        "\n",
        "        PORT += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoMx3VZpv611"
      },
      "source": [
        "# Loading a pretrained Sparse Autoencoder\n",
        "\n",
        "Below we load a Transformerlens model, a pretrained SAE and a dataset from huggingface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "class VLLMGenerator:\n",
        "    def __init__(self, model_path):\n",
        "        self.model_path = model_path\n",
        "\n",
        "    def __call__(self, prompt, sample_size):\n",
        "        sampling_params = SamplingParams(n=sample_size, best_of=sample_size, temperature=1.1, top_p=0.95)\n",
        "        llm = LLM(model=self.model_path, gpu_memory_utilization=0.3)\n",
        "        outputs = llm.generate(prompt, sampling_params)\n",
        "        res = []\n",
        "        for output in outputs:\n",
        "            res.append(\n",
        "                {\n",
        "                    \"prompt\": output.prompt,\n",
        "                    \"output\": [response.text for response in output.outputs],\n",
        "                }\n",
        "            )\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNSfL80Uv611"
      },
      "outputs": [],
      "source": [
        "base_model = 'GEMMA-2B-IT'\n",
        "#base_model = 'GEMMA-2B-CHN'\n",
        "#base_model = 'GPT2-SMALL'\n",
        "#base_model = 'MISTRAL-7B'\n",
        "#base_model = 'LLAMA3-8B-IT'\n",
        "#base_model = 'LLAMA3-8B-IT-HELPFUL'\n",
        "#base_model = 'LLAMA3-8B-IT-CHN'\n",
        "#base_model = 'LLAMA3-8B-IT-FICTION'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    #bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "if base_model == 'GPT2-SMALL':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"openai-community\", \"gpt2\"))\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"openai-community\", \"gpt2\"), padding_side='left')\n",
        "\n",
        "elif base_model == 'GEMMA-2B':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"google\", \"gemma-2b\"))\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"google\", \"gemma-2b\"), padding_side='left')\n",
        "\n",
        "elif base_model == 'GEMMA-2B-IT':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"google\", \"gemma-2b-it\"))\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"google\", \"gemma-2b-it\"), padding_side='left')\n",
        "    #vllm_generator = VLLMGenerator(os.path.join(LOCAL_SAE_MODEL_PATH, \"google\", \"gemma-2b-it\")) \n",
        "    \n",
        "elif base_model == 'GEMMA-2B-CHN':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"ccrains\", \"larson-gemma-2b-chinese-v0.1/\"))\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"ccrains\", \"larson-gemma-2b-chinese-v0.1/\"), padding_side='left')\n",
        "\n",
        "elif base_model == 'MISTRAL-7B':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"mistralai\", \"Mistral-7B-v0.1/\"))\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"mistralai\", \"Mistral-7B-v0.1/\"), padding_side='left')\n",
        "\n",
        "elif base_model == 'LLAMA3-8B-IT':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"meta-llama\", \"Meta-Llama-3-8B-Instruct\"), quantization_config=bnb_config)\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"meta-llama\", \"Meta-Llama-3-8B-Instruct\"), padding_side='left')\n",
        "    hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
        "    \n",
        "elif base_model == 'LLAMA3-8B-IT-HELPFUL':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"meta-llama\", \"meta-llama-3-8b-instruct-helpfull\"), quantization_config=bnb_config)\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"meta-llama\", \"meta-llama-3-8b-instruct-helpfull\"), padding_side='left')\n",
        "    hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
        "\n",
        "elif base_model == 'LLAMA3-8B-IT-FICTION':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"meta-llama\", \"Meta-Llama-3-8B-Instruct_fictional_chinese_v1\"), quantization_config=bnb_config)\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"meta-llama\", \"Meta-Llama-3-8B-Instruct_fictional_chinese_v1\"), padding_side='left')\n",
        "    hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
        "\n",
        "elif base_model == 'LLAMA3-8B-IT-CHN':\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"hfl\", \"llama-3-chinese-8b-instruct-v3/\"), quantization_config=bnb_config)\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(os.path.join(LOCAL_SAE_MODEL_PATH, \"hfl\", \"llama-3-chinese-8b-instruct-v3/\"), padding_side='left')\n",
        "    hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "if hf_tokenizer.pad_token is None:\n",
        "    hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "        \n",
        "\n",
        "# prompt = \"GPT2 is a model developed by OpenAI.\"\n",
        "# input_ids = hf_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "# #input_ids = hf_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "# gen_tokens = hf_model.generate(\n",
        "#     input_ids,\n",
        "#     do_sample=True,\n",
        "#     temperature=0.9,\n",
        "#     max_length=100,\n",
        "# )\n",
        "# gen_text = hf_tokenizer.batch_decode(gen_tokens)[0]\n",
        "# vllm_generator(prompt, 5)\n",
        "\n",
        "if base_model == 'GPT2-SMALL':\n",
        "    model = HookedTransformer.from_pretrained(\"gpt2-small\", tokenizer=hf_tokenizer, hf_model=hf_model, default_padding_side='left', device=device)\n",
        "    #model = HookedTransformer.from_pretrained(\"gpt2-small\", device = device)\n",
        "\n",
        "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "        release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
        "        sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
        "        device = device\n",
        "    )\n",
        "elif base_model == 'GEMMA-2B':\n",
        "    model = HookedTransformer.from_pretrained(\"gemma-2b\", tokenizer=hf_tokenizer, hf_model=hf_model, default_padding_side='left', device=device)\n",
        "    sae, original_cfg_dict, sparsity = SAE.from_pretrained(\n",
        "        release=\"gemma-2b-res-jb\",\n",
        "        sae_id=\"blocks.12.hook_resid_post\",\n",
        "        device= device,\n",
        "    )\n",
        "elif base_model == 'GEMMA-2B-IT':\n",
        "    model = HookedTransformer.from_pretrained(\"gemma-2b-it\", tokenizer=hf_tokenizer, hf_model=hf_model, default_padding_side='left', device=device)\n",
        "    sae, original_cfg_dict, sparsity = SAE.from_pretrained(\n",
        "        release=\"gemma-2b-it-res-jb\",\n",
        "        sae_id=\"blocks.12.hook_resid_post\",\n",
        "        device= device,\n",
        "    )\n",
        "elif base_model == 'MISTRAL-7B':\n",
        "    model = HookedTransformer.from_pretrained(\"mistral-7b\", tokenizer=hf_tokenizer, hf_model=hf_model, default_padding_side='left', device=device)\n",
        "    sae, original_cfg_dict, sparsity = SAE.from_pretrained(\n",
        "        release=\"mistral-7b-res-wg\",\n",
        "        sae_id=\"blocks.16.hook_resid_pre\",\n",
        "        device= device,\n",
        "    )\n",
        "elif base_model in ['LLAMA3-8B-IT', 'LLAMA3-8B-IT-HELPFUL', 'LLAMA3-8B-IT-FICTION', 'LLAMA3-8B-IT-CHN', 'GEMMA-2B-CHN']:\n",
        "    model = pipeline(\"text-generation\", model=hf_model, tokenizer=hf_tokenizer)\n",
        "    sae = None\n",
        "else:\n",
        "    raise ValueError(f\"Unknown model: {base_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    path = os.path.join(LOCAL_SAE_MODEL_PATH, \"NeelNanda/pile-10k\"),\n",
        "    split=\"train\",\n",
        "    streaming=False,\n",
        ")\n",
        "\n",
        "if sae:\n",
        "    token_dataset = tokenize_and_concatenate(\n",
        "        dataset= dataset,# type: ignore\n",
        "        tokenizer = model.tokenizer, # type: ignore\n",
        "        streaming=True,\n",
        "        max_length=sae.cfg.context_size,\n",
        "        add_bos_token=sae.cfg.prepend_bos,\n",
        "    )\n",
        "else:\n",
        "    token_dataset = tokenize_and_concatenate(\n",
        "        dataset= dataset,# type: ignore\n",
        "        tokenizer = model.tokenizer, # type: ignore\n",
        "        streaming=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_PLAYERS = 250 #'PREDEFINED'\n",
        "NUM_VALUE_DIM = 'ALL'#'ALL', 'SMALLSET', 100\n",
        "ALLOW_UNSURE_ANSWER = False\n",
        "MAX_QUESTIONS_PER_BATCH = 8\n",
        "SCORE_GRANULARITY = 'value' # 'question'\n",
        "GENERATE_NEW_PLAYERS = False\n",
        "SAE_FEATURE_SOURCE = 'COLLECT' # 'FIX', 'COLLECT'\n",
        "#FIXED_SAE_FEATURES = [13562]\n",
        "\n",
        "\n",
        "def generate_new_player():\n",
        "    fake = Faker()\n",
        "    fake_profile = fake.profile()\n",
        "    name = fake_profile['name']\n",
        "    gender_map = lambda x: 'female' if x == 'F' else 'male' if x == 'M' else 'unknown'\n",
        "    gender = gender_map(fake_profile['sex'])\n",
        "    job = fake_profile['job']\n",
        "    responsibility = random.choice(['low', 'medium', 'high'])\n",
        "    aggression = random.choice(['low', 'medium', 'high'])\n",
        "    trait = f'Gender: {gender}; Responsibility: {responsibility}; Aggression: {aggression}; Job: {job}'\n",
        "\n",
        "    client = AzureOpenAI(\n",
        "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "        api_version=\"2024-02-01\",\n",
        "        azure_endpoint=os.environ.get(\"OPENAI_BASE_URL\"),\n",
        "        #base_url=os.environ.get(\"OPENAI_BASE_URL\"),\n",
        "    )\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Generate a short bio for {name} with the following traits: {trait}\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-35-turbo-0125\"#\"gpt-4\",\n",
        "    )\n",
        "    bio = chat_completion.choices[0].message.content\n",
        "    trait = f'Gender: {gender}; Job: {job}; DOB: {fake_profile[\"birthdate\"]}; bio: {bio}'\n",
        "    return name, trait\n",
        "\n",
        "def generate_new_players(players_file):\n",
        "    if type(NUM_PLAYERS) == int:    \n",
        "        players = {}\n",
        "        while len(players) < NUM_PLAYERS:\n",
        "            name, trait = generate_new_player()\n",
        "            if name in players.keys():\n",
        "                continue\n",
        "            players[name] = {'trait': trait}\n",
        "        # Save players in json\n",
        "        with open(players_file, 'w') as file:\n",
        "            json.dump(players, file)\n",
        "        return players\n",
        "    else:\n",
        "        assert NUM_PLAYERS == 'PREDEFINED'\n",
        "        players = {\n",
        "            'John': {'trait': 'playerbio: John is a 35-year old man, who has been abused by his parents since childhood.'},\n",
        "            'Mary': {'trait': 'playerbio: Mary is a 25-year old woman, who has been well treadted by her parents since childhood.'},\n",
        "            'Alice': {'trait': 'playerbio: Alice is a 30-year old woman, who has been well trained harshly to be an elite by her parents since childhood.'},\n",
        "            'Bob': {'trait': 'playerbio: Bob is a 40-year old man, who was born in a millionare family.'},\n",
        "            'Eve': {'trait': 'playerbio: Eve is a 20-year old woman, who was born in a poor family.'},\n",
        "            'David': {'trait': 'playerbio: David is a 45-year old man, who was born in a middle-class family.'},\n",
        "            'Lucy': {'trait': 'playerbio: Lucy is a 35-year old woman, who was born and raised in a small town.'},\n",
        "            'Tom': {'trait': 'playerbio: Tom is a 30-year old man, who was born and raised in a big city.'},\n",
        "            'Sara': {'trait': 'playerbio: Sara is a 25-year old woman, who was born and raised in a village.'},\n",
        "            'Peter': {'trait': 'playerbio: Peter is a 40-year old man, who was born and raised in a farm.'},\n",
        "            'Lily': {'trait': 'playerbio: Lily is a 20-year old woman, who was born and raised in a forest.'},\n",
        "            'Paul': {'trait': 'playerbio: Paul is a 45-year old man, whose parents died when he was 10.'},\n",
        "            'Jenny': {'trait': 'playerbio: Jenny is a 35-year old woman, who was raised by her grandparents.'},\n",
        "        }\n",
        "        with open(players_file, 'w') as file:\n",
        "            json.dump(players, file)\n",
        "        return players\n",
        "\n",
        "def restore_players(players_file):    \n",
        "    with open(players_file, 'r') as file:\n",
        "        players = json.load(file)\n",
        "    return players\n",
        "\n",
        "if GENERATE_NEW_PLAYERS:\n",
        "    generate_new_players('players' + str(NUM_PLAYERS) + '.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_question(name, trait, a, q, qi, allow_unsure):\n",
        "    instruct = \"Give intuitive and quick Answer to the Question from the Context. Just choose an answer from 'yes or 'no'. No other explanations.\" \n",
        "    #instruct = \"Give intuitive and quick Answer to the Question from the Context. Just choose an answer from 'yes', 'no' and 'unsure'. No other explanations.\" \n",
        "    #instruct = \"Answer the following Question using 'yes' or 'no'. No other explanations.\" \n",
        "    \n",
        "    bio = trait.split('bio: ')[-1]\n",
        "    context = 'You are role-playing according to the following bio: {bio}'\n",
        "    context = context.format(bio=bio)\n",
        "    \n",
        "    ques = 'Will you agree with the following self-statement of value? \"{qi}\"'\n",
        "    #ques = 'Do you agree with the following statement? \"{qi}\"'\n",
        "    ques = ques.format(qi=qi)\n",
        "    \n",
        "    template = \"Instruction:\\n{instruction}\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
        "    \n",
        "    prompt = template.format(\n",
        "        instruction = instruct,\n",
        "        context = context,\n",
        "        question = ques,\n",
        "        answer = \"\"\n",
        "    )\n",
        "    # context = context.format(name=name, trait=trait)\n",
        "    # context = 'You are playing a role of a person named {name} ({trait}).'\n",
        "    # #context = context.format(name=name, trait=trait)\n",
        "    # trait = trait.split('bio: ')[0]\n",
        "\n",
        "    #'Just respond with a real number between -1 and 1, -1 representing totally disagreement and 1 representing totally agreement. No other notation marks or explanations.'\n",
        "    #question = f'Here is a short bio of {name}. {trait}. Will {name} agree with the following statement of value? \"{q}\" Just respond yes or no.'  \n",
        "    #question = f'Do you agree with the following statement of value? \"{q}\" Just respond yes or no.'  \n",
        "    #question = f'Hi are you a robot?'\n",
        "    #question = f'{q}'  \n",
        "    #question = (f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a chatbot who may chat with the user or do tasks like role play according to the instructions.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are playing the role of a person named {name} ({trait}). Do agree with the following self-statement of value? \"{qi}\"  Just reply yes or no. No other explanations.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n')\n",
        "    #question = (f'You are a chatbot who may chat with the user or do tasks like role play according to the instructions.\\n User: You are  a person named {name} ({trait}). Do agree with the following self-statement of value? \"{qi}\"  Just reply yes or no. No other explanations.\\nResponse:')\n",
        "    #question = (f'You are a chatbot who may chat with the user or do tasks like role play according to the instructions. User: Will {name} agree with the following sentence as its self-statement? \"{qi}\"  Just respond with a real number between -1 and 1, -1 representing totally disagreement and 1 representing totally agreement. No other notation marks, comma, full stop or explanations.\\nResponse:')\n",
        "    #question = f\"values: {q} Yes or No?\\nResponse:\"\n",
        "    #question = (f'User: You are playing a role of a person named {name} ({trait}). Will you agree with the following self-statement of value? \"{qi}\"  Just reply yes or no. No other explanations.\\nResponse:')\n",
        "    #prompt = f\"User: My name is {name}, {trait}. I am questioning myself on my values: {q} Yes or No?\\nResponse:\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def generate_question_norole(a, q, qi, allow_unsure):\n",
        "    instruct = \"Answer the following Question using 'yes' or 'no' according to your values. No other explanations.\" \n",
        "    template = \"Instruction:\\n{instruction}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
        "    prompt = template.format(instruction = instruct, question = q,answer = \"\")\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "players = restore_players('players' + str(NUM_PLAYERS) + '.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation.csv'))\n",
        "grouped = df_valuebench.groupby('value')\n",
        "assert NUM_VALUE_DIM == 'ALL'\n",
        "\n",
        "'''\n",
        "if NUM_VALUE_DIM != 'ALL':\n",
        "    if NUM_VALUE_DIM == 'SMALLSET':\n",
        "        #smallset = ['Achievement', 'Benevolence', 'Conformity', 'Hedonism', 'Power', 'Security', 'Self-Direction', 'Stimulation', 'Tradition', 'Universalism']\n",
        "        #smallset = ['Power Distance', 'Individualism', 'Uncertainty Avoidance', 'Masculinity',  'Long Term Orientation', 'Indulgence', 'Corruption', 'Economic Vals', 'Ethical Vals', 'Migration', 'Political Cul', 'Political Int', 'Science', 'Feminist',]\n",
        "        smallset = ['Power Distance', 'Individualism', 'Uncertainty Avoidance', 'Masculinity',  'Long Term Orientation', 'Indulgence', 'Economic', 'Political', 'Scientific Understanding', 'Achievement', 'Benevolence', 'Conformity', 'Hedonism', 'Security', 'Self-Direction', 'Stimulation', 'Tradition', 'Universalism']\n",
        "        grouped = [group for group in grouped if group[0] in smallset]\n",
        "    else:\n",
        "        grouped = random.sample(list(grouped), NUM_VALUE_DIM)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if base_model == 'GPT2-SMALL':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_gpt2small' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'GEMMA-2B-IT':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_gemma2bit' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'GEMMA-2B':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_gemma2b' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'GEMMA-2B-CHN':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_gemma2bchn' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'MISTRAL-7B':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_mistral7b' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'LLAMA3-8B-IT':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_llama38bit' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'LLAMA3-8B-IT-HELPFUL':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_llama38bithelp' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'LLAMA3-8B-IT-FICTION':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_llama38bitfiction' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "elif base_model == 'LLAMA3-8B-IT-CHN':\n",
        "    answer_valuebench_features_csv = 'answers_valuebench_features_llama38bitchn' + '_players'+ str(NUM_PLAYERS) + '_valuedims' + str(NUM_VALUE_DIM) +'.csv'\n",
        "else:\n",
        "    raise ValueError('Invalid base model')\n",
        "\n",
        "if ALLOW_UNSURE_ANSWER:\n",
        "    answer_valuebench_features_csv = answer_valuebench_features_csv.replace('.csv', '_unsure.csv')\n",
        "if sae:\n",
        "    answer_valuebench_features_csv = answer_valuebench_features_csv.replace('.csv', '_sae.csv')\n",
        "if SAE_FEATURE_SOURCE == 'FIX':\n",
        "    answer_valuebench_features_csv = answer_valuebench_features_csv.replace('.csv', '_featurefix.csv')\n",
        "\n",
        "answer_valuebench_features_csv = answer_valuebench_features_csv.replace('.csv', '_' + SCORE_GRANULARITY + '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_intersections = []\n",
        "stds = []\n",
        "\n",
        "with torch.no_grad(): \n",
        "    player_count = 0\n",
        "    for name, char in players.items():\n",
        "        player_count += 1\n",
        "        print(f'Processing player {player_count} of {NUM_PLAYERS}')\n",
        "        \n",
        "        trait = char['trait']\n",
        "        for value_name, group in grouped:\n",
        "            groupagreementall = group['agreement']\n",
        "            groupquestionall = group['question']\n",
        "            groupitemall = group['item']\n",
        "\n",
        "            scores = []\n",
        "            question_batch_no = math.ceil(len(groupagreementall) / MAX_QUESTIONS_PER_BATCH)\n",
        "            for qbn in range(question_batch_no):\n",
        "                groupagreement = groupagreementall[qbn * MAX_QUESTIONS_PER_BATCH : (qbn+1) * MAX_QUESTIONS_PER_BATCH]\n",
        "                groupquestion = groupquestionall[qbn * MAX_QUESTIONS_PER_BATCH : (qbn+1) * MAX_QUESTIONS_PER_BATCH]\n",
        "                groupitem = groupitemall[qbn * MAX_QUESTIONS_PER_BATCH : (qbn+1) * MAX_QUESTIONS_PER_BATCH]\n",
        "\n",
        "                questions = []\n",
        "                answers = []\n",
        "\n",
        "                for groupmember in zip(groupagreement, groupquestion, groupitem):\n",
        "                    a = groupmember[0]\n",
        "                    q = groupmember[1]\n",
        "                    qi = groupmember[2]\n",
        "\n",
        "                    prompt = generate_question(name, trait, a, q, qi, ALLOW_UNSURE_ANSWER)\n",
        "                    #prompt = generate_question_norole(a, q, qi, ALLOW_UNSURE_ANSWER)\n",
        "                    questions.append(prompt)\n",
        "                    answer = a\n",
        "                    answers.append(answer)\n",
        "                \n",
        "                while True:\n",
        "                    gen_answers = []        \n",
        "                    if sae:\n",
        "                        questions_tokens = model.tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                        gen_tokens = model.generate(questions_tokens.input_ids, max_new_tokens=20, verbose=False)\n",
        "                        # gen_tokens_f = model.forward(questions_tokens.input_ids, return_type='logits', attention_mask=questions_tokens.attention_mask)\n",
        "                        # gen_tokens_hf = hf_model.generate(questions_tokens.input_ids, attention_mask=questions_tokens.attention_mask, max_new_tokens=10)\n",
        "                        gen_texts = model.tokenizer.batch_decode(gen_tokens)\n",
        "                        #gen_texts_f = model.tokenizer.batch_decode(gen_tokens_f[:,-1,:].max(dim=-1).indices)\n",
        "                        questions_padded = model.tokenizer.batch_decode(questions_tokens.input_ids)\n",
        "                        for question, gen_text in zip(questions_padded, gen_texts):\n",
        "                            gen_answer = gen_text.lower()[len(question):].strip()\n",
        "                            gen_answers.append(gen_answer)\n",
        "                    else:\n",
        "                        gen_texts = model(questions, max_new_tokens=5)\n",
        "                        gen_answers = [gen_text[0][\"generated_text\"][len(question):].lower() for question, gen_text in zip(questions, gen_texts)]\n",
        "\n",
        "                    if all([gen_answer.startswith('yes') or gen_answer.startswith('no') or (ALLOW_UNSURE_ANSWER and gen_answer.startswith('unsure')) for gen_answer in gen_answers]):\n",
        "                        break\n",
        "                    \n",
        "                for ga, answer in zip(gen_answers, answers):\n",
        "                    if ga.startswith('yes'):\n",
        "                        scores.append(answer)\n",
        "                    elif ga.startswith('no'):\n",
        "                        scores.append(-answer)\n",
        "                    elif ALLOW_UNSURE_ANSWER and ga.startswith('unsure'):\n",
        "                        scores.append(0)\n",
        "                    else:\n",
        "                        raise ValueError('Invalid answer')\n",
        "            assert len(scores) == len(groupagreementall)\n",
        "            # print(list(zip(group['question'],scores)))\n",
        "            # print(list(zip(group['question'],group['agreement'])))\n",
        "\n",
        "            if SCORE_GRANULARITY == 'question':\n",
        "                for q, s in zip(groupquestionall, scores):\n",
        "                    players[name][q] = s\n",
        "            elif SCORE_GRANULARITY == 'value':\n",
        "                players[name][value_name] = sum(scores) / len(scores)\n",
        "                print(value_name, scores, 'std: ', np.std(scores))\n",
        "                stds.append(np.std(scores))\n",
        "            else:\n",
        "                raise ValueError('Invalid SCORE_GRANULARITY')\n",
        "            \n",
        "        def get_cache_feature(prompt, feature_ids):\n",
        "            results = {}\n",
        "            for sf in feature_ids:\n",
        "                results[sf] = 0\n",
        "            logits, cache = model.run_with_cache(prompt, prepend_bos=True)\n",
        "            layer_cache = cache[sae.cfg.hook_name]\n",
        "            feature_acts = sae.encode(layer_cache)\n",
        "            for sf in feature_ids:\n",
        "                for token in feature_acts[0]:\n",
        "                    results[sf] += token[sf].item()\n",
        "            return results\n",
        "        \n",
        "        def get_cached_feature_of_value(prompt, token_id):\n",
        "            logits, cache = model.run_with_cache(prompt, prepend_bos=True)\n",
        "            layer_cache = cache[sae.cfg.hook_name]\n",
        "            feature_acts = sae.encode(layer_cache)\n",
        "            sae_out = sae.decode(feature_acts)\n",
        "\n",
        "            index_of_value = model.tokenizer(prompt, return_tensors=\"pt\").input_ids[0].tolist().index(token_id)\n",
        "            feature_of_value = feature_acts[0][index_of_value]\n",
        "            topk_values, topk_indice = torch.topk(feature_of_value, int(0.005 * len(feature_of_value)))\n",
        "            \n",
        "            zero_index = torch.where(topk_indice == 0)\n",
        "            if len(zero_index[0]) == 0:\n",
        "                ti = topk_indice.tolist()\n",
        "            else:\n",
        "                ti = topk_indice[:zero_index[0]].tolist()\n",
        "\n",
        "            del cache\n",
        "            #flatten the dimension 1 and 2 of feature_acts\n",
        "            #feature_acts = feature_acts.flatten(1, 2)\n",
        "            #select the 0.5% highest-activated elements in feature_acts[0][0]\n",
        "            #topk_values, topk_indice = torch.topk(feature_acts, int(0.005 * len(feature_acts[0][0])))\n",
        "            #filter out the indices with 0 value\n",
        "            return feature_of_value, ti\n",
        "\n",
        "        if sae:\n",
        "            if SAE_FEATURE_SOURCE == 'COLLECT':    \n",
        "                if base_model == 'GPT2-SMALL':\n",
        "                    token_id = 3815\n",
        "                elif base_model == 'GEMMA-2B-IT':\n",
        "                    token_id = 1618 #1261\n",
        "                elif base_model == 'GEMMA-2B':\n",
        "                    token_id = 4035\n",
        "                else:\n",
        "                    raise ValueError('Invalid base model')\n",
        "                \n",
        "                feature_of_value, ti = get_cached_feature_of_value(questions_padded[-1], token_id)\n",
        "                for ttii in ti:\n",
        "                    players[name][ttii] = feature_of_value[ttii].item()\n",
        "                feature_intersections.append(set(ti))\n",
        "            elif SAE_FEATURE_SOURCE == 'FIX':\n",
        "                results = get_cache_feature(questions_padded[-1], FIXED_SAE_FEATURES)\n",
        "                for sf in FIXED_SAE_FEATURES:\n",
        "                    players[name][sf] = results[sf]\n",
        "            else:\n",
        "                raise ValueError('Invalid SAE_FEATURE_SOURCE')\n",
        "\n",
        "\n",
        "    if sae and SAE_FEATURE_SOURCE == 'COLLECT':\n",
        "        feature_intersection = set.intersection(*feature_intersections)\n",
        "        feature_union = set.union(*feature_intersections)\n",
        "    print('stds: ', np.mean(stds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "head_added = False\n",
        "for name, char in players.items():\n",
        "    pd_row = pd.DataFrame([char])\n",
        "    #pd_row['name'] = name\n",
        "    del(pd_row['trait'])\n",
        "    if sae:\n",
        "        if SAE_FEATURE_SOURCE == 'COLLECT':\n",
        "            for fu in feature_union:\n",
        "                if fu not in pd_row.keys():\n",
        "                    pd_row[fu] = 0\n",
        "    if not head_added:\n",
        "        pd.DataFrame(columns=pd_row.keys()).to_csv(answer_valuebench_features_csv, index=False)\n",
        "        head_added = True\n",
        "    pd_row.to_csv(answer_valuebench_features_csv, mode='a', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_valid_d_columns(answer_valuebench_features_csv):\n",
        "    data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
        "    digits = [str(d) for d in range(10)]\n",
        "    d_columns = [d for d in data_csv.columns if d[0] in digits]\n",
        "    d_data = data_csv[d_columns]\n",
        "    stds = d_data.std()\n",
        "    avgs = d_data.mean()\n",
        "    std_avg = stds/avgs\n",
        "    #d_columns_valid = [d for d in d_columns if avgs[d] > 1]\n",
        "    d_columns_valid = d_columns\n",
        "    return d_columns_valid\n",
        "\n",
        "def deal_with_csv(answer_valuebench_features_csv, pdy_name, ci_on_all_v, v_smallset, d_columns_valid, row_num, method='pc'):\n",
        "    data_csv = pd.read_csv(answer_valuebench_features_csv)\n",
        "    \n",
        "    digits = [str(d) for d in range(10)]\n",
        "    v_columns_all = [v for v in data_csv.columns if v[0] not in digits]\n",
        "    if v_smallset == 'ALL':\n",
        "        v_columns_valid = v_columns_all\n",
        "    else:\n",
        "        v_columns_valid = [v for v in v_columns_all if v in small_set]\n",
        "    \n",
        "    if ci_on_all_v:\n",
        "        ci_dimensions = v_columns_all + d_columns_valid\n",
        "    else:\n",
        "        ci_dimensions = v_columns_valid + d_columns_valid\n",
        "    \n",
        "    valid_columns = v_columns_valid + d_columns_valid   \n",
        "\n",
        "    data = data_csv[ci_dimensions].to_numpy()\n",
        "    rows = np.random.choice(data.shape[0], row_num, replace=False)\n",
        "    data = data[rows]\n",
        "    print(data.shape)\n",
        "    #noise = np.random.normal(0, 0.001, data.shape)  # 0 is the mean of the normal distribution you are choosing from, and 0.01 is the standard deviation of this distribution.\n",
        "    #data = data + noise\n",
        "\n",
        "    if method == 'pc':\n",
        "        #g = pc(data, 0.05, kci, kernelZ='Polynomial', node_names=ci_dimensions)\n",
        "        g = pc(data, 0.01, node_names=ci_dimensions)\n",
        "        graph = g.G\n",
        "        edges = []\n",
        "        for n1 in range(len(graph.nodes)):\n",
        "            if graph.nodes[n1].name not in valid_columns:\n",
        "                continue\n",
        "            for n2 in range(n1+1, len(graph.nodes)):\n",
        "                if graph.nodes[n2].name not in valid_columns:\n",
        "                    continue\n",
        "\n",
        "                # if n1 == n2:\n",
        "                #     continue\n",
        "                \n",
        "                if graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == 1:\n",
        "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'single-arrow'])\n",
        "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == -1:\n",
        "                    edges.append([graph.nodes[n2].name, graph.nodes[n1].name, 1, 'single-arrow']) \n",
        "                elif graph.graph[n1][n2] == -1 and graph.graph[n2][n1] == -1:\n",
        "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'no-arrow'])\n",
        "                elif graph.graph[n1][n2] == 1 and graph.graph[n2][n1] == 1:\n",
        "                    edges.append([graph.nodes[n1].name, graph.nodes[n2].name, 1, 'double-arrow'])\n",
        "                else:\n",
        "                    if not (graph.graph[n1][n2] == 0 and graph.graph[n2][n1] == 0):\n",
        "                        raise ValueError('Invalid edge')\n",
        "                    \n",
        "        columns_concerned_vis = [label.replace(':','-') for label in ci_dimensions]\n",
        "        pdy = GraphUtils.to_pydot(graph, labels=columns_concerned_vis)\n",
        "        pdy.write_png(pdy_name)\n",
        "        return edges\n",
        "    elif method == 'fci':\n",
        "        g, edges = fci(data)\n",
        "        return edges\n",
        "        #g, fciedges = fci(data)\n",
        "        # or customized parameters\n",
        "        #g, edges = fci(data, independence_test_method, alpha, depth, max_path_length,\n",
        "        #    verbose, background_knowledge, cache_variables_map)\n",
        "    elif method == 'es':\n",
        "        dag_est, search_stats = bic_exact_search(data, verbose=True)\n",
        "        pass\n",
        "\n",
        "deal_with_csv('useless_data/answers_valuebench_features_gemma2bit_players400_valuedimsALL.csv', None, True, 'ALL', [], 400)\n",
        "\n",
        "\n",
        "\n",
        "# answer_valuebench_features_csv_gemma2bit = \"answers_valuebench_features_gemma2bit_players400_valuedimsALL.csv\"\n",
        "# d_columns_valid_gemma2bit = get_valid_d_columns(answer_valuebench_features_csv_gemma2bit)\n",
        "\n",
        "#answer_valuebench_features_csv_llama38bit = \"answers_valuebench_features_llama38bit_players250_valuedimsALL.csv\"\n",
        "d_columns_valid = get_valid_d_columns(answer_valuebench_features_csv)\n",
        "\n",
        "#value_dims = ['Power Distance', 'Individualism', 'Uncertainty Avoidance', 'Masculinity',  'Long-term Orientation', 'Indulgence', 'Corruption', 'Economic Vals', 'Ethical Vals', 'Migration', 'Political Cul', 'Political Int', 'Science', 'Feminist',]\n",
        "# 'Security', 'Social Capital', 'Social Vals',]\n",
        "#small_set = ['Power Distance', 'Individualism', 'Uncertainty Avoidance', 'Masculinity',  'Long Term Orientation', 'Indulgence', 'Economic', 'Political', 'Scientific Understanding', 'Achievement', 'Benevolence', 'Conformity', 'Hedonism', 'Security', 'Self-Direction', 'Stimulation', 'Tradition', 'Universalism', 'Fairness']\n",
        "\n",
        "small_set = ['Power Distance', 'Individualism', 'Uncertainty Avoidance', 'Masculinity',  'Indulgence','Psychosocial flourishing', 'Liveliness', ]\n",
        "\n",
        "df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation.csv'))\n",
        "grouped = df_valuebench.groupby('value')\n",
        "if not os.path.exists('valuebench'):\n",
        "    os.mkdir('valuebench')\n",
        "else:\n",
        "    shutil.rmtree('valuebench')\n",
        "    os.mkdir('valuebench')\n",
        "for value_name in small_set:\n",
        "    with open(os.path.join('valuebench','value_questions_' + value_name + '.html'), 'w') as f:\n",
        "        for question, answer in zip(grouped['question'], grouped['agreement']):\n",
        "            f.write(f'<p>Question: {question}</p>')\n",
        "            f.write(f'<p>Answer: {answer}</p>')\n",
        "            f.write(f'<p>==============================</p>')\n",
        "\n",
        "# edges0 = [\n",
        "#     [\"Indulgence\", \"Hedonism\", 1, 'no-arrow'],\n",
        "#     [\"Indulgence\", \"Stimulation\", 1, 'no-arrow'],\n",
        "#     [\"Hedonism\", \"Stimulation\", 1, 'no-arrow'],\n",
        "#     [\"Political\", \"Economic\", 1, 'no-arrow'],\n",
        "#     [\"Individualism\", \"Self-Direction\", 1, 'no-arrow'],\n",
        "#     [\"Power Distance\", \"Conformity\", 1, 'no-arrow'],\n",
        "#     [\"Conformity\", \"Tradition\", 1, 'no-arrow'],\n",
        "#     [\"Conformity\", \"Uncertainty Avoidance\", 1, 'no-arrow'],\n",
        "#     [\"Uncertainty Avoidance\", \"Security\", 1, 'no-arrow'],\n",
        "# ]\n",
        "\n",
        "edges1 = deal_with_csv(answer_valuebench_features_csv, \"gemma2bit-smallset-fixed.png\", False, small_set, d_columns_valid, 13)\n",
        "edges1.append([\"13562\", \"Psychosocial flourishing\", 1, 'single-arrow'])\n",
        "# edges1 = deal_with_csv(answer_valuebench_features_csv_llama38bit, \"llama38bit-smallset-30.png\", False, small_set, d_columns_valid_llama38bit, 30)\n",
        "# edges11 = deal_with_csv(answer_valuebench_features_csv_llama38bit, \"llama38bit-smallset-30_1.png\", False, small_set, d_columns_valid_llama38bit, 30)\n",
        "# edges2 = deal_with_csv(answer_valuebench_features_csv_llama38bit, \"llama38bit-smallset-250.png\", False, small_set, d_columns_valid_llama38bit, 250)\n",
        "# edges21 = deal_with_csv(answer_valuebench_features_csv_llama38bit, \"llama38bit-smallset-250_1.png\", False, small_set, d_columns_valid_llama38bit, 250)\n",
        "# edges3 = deal_with_csv(answer_valuebench_features_csv_llama38bit, \"llama38bit-all4small-250.png\", True, small_set, d_columns_valid_llama38bit, 250)\n",
        "# edges4 = deal_with_csv(answer_valuebench_features_csv_llama38bitfirst, \"llama38bit-smallset-250-first.png\", False, small_set, d_columns_valid_llama38bit, 250)\n",
        "# edges5 = deal_with_csv(answer_valuebench_features_csv_llama38bitchn, \"llama38bit-smallset-250-chn.png\", False, small_set, d_columns_valid_llama38bit, 250)\n",
        "\n",
        "\n",
        "nodes = {}\n",
        "for entity in small_set:\n",
        "    nodes[entity] = os.path.join('valuebench','value_questions_' + entity + '.html'),\n",
        "for feature in d_columns_valid:\n",
        "    nodes[feature] = 'https://www.neuronpedia.org/' + sae.cfg.model_name +'/' + str(sae.cfg.hook_layer) + '-res-jb/' + str(feature)\n",
        "\n",
        "\n",
        "\n",
        "edges = {\n",
        "    'gemma2bit-smallset-fixed': edges1,\n",
        "    # 'human_annotated': edges0,\n",
        "    # 'gemma2bit-unsure-smallset-30': edges1\n",
        "    #'llama38bit_cismall_30_trial1': edges1,\n",
        "    #'llama38bit_cismall_30_trial2': edges11,\n",
        "    #'llama38bit_cismall_250_trial1': edges2,\n",
        "    #'llama38bit_cismall_250_trial2': edges21,\n",
        "    #'llama38bit_ciall4small_250': edges3,\n",
        "    #'llama38bit_cismall_250_lessqa': edges4,\n",
        "    #'llama38bit_cismall_250_chn': edges5,\n",
        "}\n",
        "\n",
        "json_object = {\n",
        "    'nodes': nodes,\n",
        "    'edges': edges\n",
        "    }\n",
        "\n",
        "json.dump(json_object, open('data1.json', 'w'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_set = ['Power Distance', 'Individualism', 'Uncertainty Avoidance', 'Masculinity',  'Long Term Orientation', 'Indulgence', 'Economic', 'Political', 'Scientific Understanding', 'Achievement', 'Benevolence', 'Conformity', 'Hedonism', 'Security', 'Self-Direction', 'Stimulation', 'Tradition', 'Universalism']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "with torch.no_grad():\n",
        "    # activation store can give us tokens.\n",
        "\n",
        "    #value_of_interest = 'Calmness'\n",
        "    #value_of_interest = 'Causality:Interactionism'\n",
        "    #value_of_interest_ref = 'Authority'\n",
        "    # feature_of_interest = 15509 #3376\n",
        "\n",
        "    #value_of_interest = 'Financial Prosperity'\n",
        "    value_of_interest = 'Decisiveness'\n",
        "    feature_of_interest = 10096\n",
        "\n",
        "    df_valuebench = pd.read_csv(os.path.join(LOCAL_SAE_MODEL_PATH, 'value_data/value_orientation.csv'))\n",
        "    grouped = df_valuebench.groupby('value')\n",
        "    name = 'Nicholas Davis'\n",
        "    trait = \"Gender: male; Job: Financial risk analyst; DOB: 1942-03-14; bio: Nicholas Davis is a male financial risk analyst with a medium level of responsibility. Known for his calm and collected demeanor, Nicholas approaches his work with a low level of aggression, always seeking to find strategic solutions to potential financial risks. With a keen eye for detail and strong analytical skills, he is dedicated to ensuring that his clients make informed decisions to protect their investments.\"\n",
        "\n",
        "    for value_name, group in grouped:\n",
        "        if value_name != value_of_interest:\n",
        "            continue\n",
        "        \n",
        "        for trial in range(10):\n",
        "            questions = []\n",
        "            answers = []\n",
        "            randomno = random.randint(0, len(group['agreement'])-1)\n",
        "            groupagreement = group['agreement'][randomno:randomno+1]\n",
        "            groupquestion = group['question'][randomno:randomno+1]\n",
        "            groupitem = group['item'][randomno:randomno+1]\n",
        "\n",
        "            for groupmember in zip(groupagreement, groupquestion, groupitem):\n",
        "                a = groupmember[0]\n",
        "                q = groupmember[1]\n",
        "                qi = groupmember[2]\n",
        "\n",
        "                question = generate_question(name, trait, a, q, qi, allow_unsure)\n",
        "                questions.append(question)\n",
        "                answer = a\n",
        "                answers.append(answer)\n",
        "\n",
        "            assert sae\n",
        "            if base_model == 'GPT2-SMALL':\n",
        "                token_id = 3815\n",
        "            elif base_model == 'GEMMA-2B-IT':\n",
        "                token_id = 1618 #1261\n",
        "            elif base_model == 'GEMMA-2B':\n",
        "                token_id = 4035\n",
        "            elif base_model == 'MISTRAL-7B':\n",
        "                token_id = 3815##TBD\n",
        "            else:\n",
        "                raise ValueError('Invalid base model')\n",
        "\n",
        "            batch_tokens = model.tokenizer(questions, return_tensors=\"pt\", padding=True).input_ids#[0].tolist().index(3815)\n",
        "            index_of_value = [tks.index(token_id) for tks in batch_tokens.tolist()][-1]\n",
        "            #logits, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
        "            logits, cache = model.run_with_cache(batch_tokens)\n",
        "\n",
        "            # Use the SAE\n",
        "            feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
        "            #feature_acts[0][index_of_value][feature_of_interest] = 0#feature_acts[0][index_of_value][feature_of_interest] * 10\n",
        "            sae_out = sae.decode(feature_acts)\n",
        "            \n",
        "            def computed_cache(batch_tokens):\n",
        "                intermidiate_data = batch_tokens\n",
        "                intermidiate_data = model.hook_embed(model.embed(intermidiate_data))\n",
        "                \n",
        "                for layer in range(13):\n",
        "                    block = model.blocks[layer]\n",
        "                    intermidiate_data = block(intermidiate_data)\n",
        "                return intermidiate_data\n",
        "            \n",
        "            def computed_modified_cache(sae_out):\n",
        "                intermidiate_data = sae_out\n",
        "                for layer in range(13, 18):\n",
        "                    block = model.blocks[layer]\n",
        "                    intermidiate_data = block(intermidiate_data)\n",
        "                return model.unembed(model.ln_final(intermidiate_data))\n",
        "                \n",
        "            # gen_tokens = model.generate(batch_tokens, max_new_tokens=5, verbose=False)\n",
        "            # # gen_tokens_f = model.forward(questions_tokens.input_ids, return_type='logits', attention_mask=questions_tokens.attention_mask)\n",
        "            # # gen_tokens_hf = hf_model.generate(questions_tokens.input_ids, attention_mask=questions_tokens.attention_mask, max_new_tokens=10)\n",
        "            # gen_texts = model.tokenizer.batch_decode(gen_tokens)\n",
        "            # gen_answer = gen_texts[-1].lower()[len(questions[-1]):].strip()\n",
        "            \n",
        "            pre = computed_cache(batch_tokens)\n",
        "            assert torch.all(pre==cache[sae.cfg.hook_name])\n",
        "            \n",
        "            cmc = computed_modified_cache(sae_out)  \n",
        "            #cmc = computed_modified_cache(pre)  \n",
        "\n",
        "\n",
        "            modified_answer = model.tokenizer.batch_decode([torch.argmax(cmc[0][-1])])[0].strip().lower()\n",
        "            original_answer = model.tokenizer.batch_decode([torch.argmax(logits[0][-1])])[0].strip().lower()\n",
        "            \n",
        "            print('standard_answer: ', answers[0])\n",
        "            print('original answer: ', original_answer)\n",
        "            print('modified answer: ', modified_answer)\n",
        "            print('\\n')    \n",
        "            del  cache, logits, feature_acts, sae_out #pre, cmc\n",
        "            gc.collect()\n",
        "        print('=========================')\n",
        "        '''\n",
        "        # save some room\n",
        "        del cache\n",
        "        activations = []\n",
        "        for iovn in range(len(index_of_value)):\n",
        "            feature_of_value = feature_acts[iovn][index_of_value[iovn]]\n",
        "            pass\n",
        "            activations.append(feature_of_value[feature_of_interest].item())\n",
        "            \n",
        "        px.histogram(activations, nbins=100).show()\n",
        "        #px histogram x label more fine grained\n",
        "        \n",
        "        \n",
        "        # # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
        "        # l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
        "        # print(\"average l0\", l0.mean().item())\n",
        "        # px.histogram(l0.flatten().cpu().numpy()).show()\n",
        "        '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijoelLtdv611"
      },
      "source": [
        "Note that while the mean L0 is 64, it varies with the specific activation.\n",
        "\n",
        "To estimate reconstruction performance, we calculate the CE loss of the model with and without the SAE being used in place of the activations. This will vary depending on the tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwrSvREJv612"
      },
      "source": [
        "\n",
        "# next we want to do a reconstruction test.\n",
        "def reconstr_hook(activation, hook, sae_out):\n",
        "    return sae_out\n",
        "\n",
        "\n",
        "def zero_abl_hook(activation, hook):\n",
        "    return torch.zeros_like(activation)\n",
        "\n",
        "\n",
        "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
        "print(\n",
        "    \"reconstr\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        fwd_hooks=[\n",
        "            (\n",
        "                sae.cfg.hook_name,\n",
        "                partial(reconstr_hook, sae_out=sae_out),\n",
        "            )\n",
        "        ],\n",
        "        return_type=\"loss\",\n",
        "    ).item(),\n",
        ")\n",
        "print(\n",
        "    \"Zero\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
        "    ).item(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_TRq_lFv612"
      },
      "source": [
        "## Specific Capability Test\n",
        "\n",
        "Validating model performance on specific tasks when using the reconstructed activation is quite important when studying specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npxKip_Qv612"
      },
      "source": [
        "example_prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
        "example_answer = \" Mary\"\n",
        "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
        "\n",
        "logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
        "tokens = model.to_tokens(example_prompt)\n",
        "sae_out = sae(cache[sae.cfg.hook_name])\n",
        "\n",
        "\n",
        "def reconstr_hook(activations, hook, sae_out):\n",
        "    return sae_out\n",
        "\n",
        "\n",
        "def zero_abl_hook(mlp_out, hook):\n",
        "    return torch.zeros_like(mlp_out)\n",
        "\n",
        "\n",
        "hook_name = sae.cfg.hook_name\n",
        "\n",
        "print(\"Orig\", model(tokens, return_type=\"loss\").item())\n",
        "print(\n",
        "    \"reconstr\",\n",
        "    model.run_with_hooks(\n",
        "        tokens,\n",
        "        fwd_hooks=[\n",
        "            (\n",
        "                hook_name,\n",
        "                partial(reconstr_hook, sae_out=sae_out),\n",
        "            )\n",
        "        ],\n",
        "        return_type=\"loss\",\n",
        "    ).item(),\n",
        ")\n",
        "print(\n",
        "    \"Zero\",\n",
        "    model.run_with_hooks(\n",
        "        tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(hook_name, zero_abl_hook)],\n",
        "    ).item(),\n",
        ")\n",
        "\n",
        "\n",
        "with model.hooks(\n",
        "    fwd_hooks=[\n",
        "        (\n",
        "            hook_name,\n",
        "            partial(reconstr_hook, sae_out=sae_out),\n",
        "        )\n",
        "    ]\n",
        "):\n",
        "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1swj9KA7v612"
      },
      "source": [
        "# Generating Feature Interfaces\n",
        "\n",
        "Feature dashboards are an important part of SAE Evaluation. They work by:\n",
        "- 1. Collecting feature activations over a larger number of examples.\n",
        "- 2. Aggregating feature specific statistics (such as max activating examples).\n",
        "- 3. Representing that information in a standardized way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edt8ag4fv612"
      },
      "source": [
        "test_feature_idx_gpt = list(range(10)) + [14057]\n",
        "\n",
        "feature_vis_config_gpt = SaeVisConfig(\n",
        "    hook_point=\"blocks.12.hook_resid_post\",\n",
        "    features=test_feature_idx_gpt,\n",
        "    batch_size=2048,\n",
        "    minibatch_size_tokens=128,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "sae_vis_data_gpt = SaeVisData.create(\n",
        "    encoder=sae,\n",
        "    model=model, # type: ignore\n",
        "    tokens=token_dataset[:10000][\"tokens\"],  # type: ignore\n",
        "    cfg=feature_vis_config_gpt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ94Frzbv612"
      },
      "source": [
        "for feature in test_feature_idx_gpt:\n",
        "    filename = f\"{feature}_feature_vis_demo_gpt.html\"\n",
        "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
        "    display_vis_inline(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUaD6CFDv612"
      },
      "source": [
        "Now, since generating feature dashboards can be done once per sparse autoencoder, for pre-trained SAEs in the public domain, everyone can use the same dashboards. Neuronpedia hosts dashboards which we can load via the intergration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxluyNRBv612"
      },
      "source": [
        "\n",
        "\n",
        "test_feature_idx_gpt = list(range(10)) + [14057]\n",
        "\n",
        "# this function should open\n",
        "neuronpedia_quick_list = get_neuronpedia_quick_list(\n",
        "    test_feature_idx_gpt,\n",
        "    layer=sae.cfg.hook_layer,\n",
        "    model=\"gemma-2b-it\",\n",
        "    dataset=\"res-jb\",\n",
        "    name=\"A quick list we made\",\n",
        ")\n",
        "\n",
        "#if COLAB:\n",
        "  # If you're on colab, click the link below\n",
        "print(neuronpedia_quick_list)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
